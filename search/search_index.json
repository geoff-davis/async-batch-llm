{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"async-batch-llm","text":"<p>Process thousands of LLM requests in parallel with automatic retries, rate limiting, and flexible error handling.</p> <p>Works with any LLM provider (OpenAI, Anthropic, Google, LangChain, or custom) through a simple strategy pattern. Built on asyncio for efficient I/O-bound processing.</p> <p> </p>"},{"location":"#why-async-batch-llm","title":"Why async-batch-llm?","text":"<ul> <li>\u2705 Universal - Works with any LLM provider through a simple strategy interface</li> <li>\u2705 Reliable - Built-in retry logic, timeout handling, and coordinated rate limiting</li> <li>\u2705 Fast - Parallel async processing with configurable concurrency</li> <li>\u2705 Observable - Token tracking, metrics collection, and event hooks</li> <li>\u2705 Cost-Effective - Shared caching strategies can dramatically reduce repeated prompt costs</li> <li>\u2705 Type-Safe - Full generic type support with Pydantic validation</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Basic installation\npip install async-batch-llm\n\n# With PydanticAI support (recommended for structured output)\npip install 'async-batch-llm[pydantic-ai]'\n\n# With Google Gemini support\npip install 'async-batch-llm[gemini]'\n\n# With everything\npip install 'async-batch-llm[all]'\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import asyncio\nfrom async_batch_llm import (\n    ParallelBatchProcessor,\n    LLMWorkItem,\n    ProcessorConfig,\n    PydanticAIStrategy,\n)\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass Summary(BaseModel):\n    title: str\n    key_points: list[str]\n\nasync def main():\n    # Create agent and wrap in strategy\n    agent = Agent(\"gemini-2.5-flash\", result_type=Summary)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    # Configure processor\n    config = ProcessorConfig(max_workers=5, timeout_per_item=30.0)\n\n    # Process items with automatic resource cleanup\n    async with ParallelBatchProcessor[str, Summary, None](config=config) as processor:\n        # Add work items\n        for doc in [\"Document 1 text...\", \"Document 2 text...\"]:\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=f\"doc_{hash(doc)}\",\n                    strategy=strategy,\n                    prompt=f\"Summarize: {doc}\",\n                )\n            )\n\n        # Process all in parallel\n        result = await processor.process_all()\n\n    print(f\"Succeeded: {result.succeeded}/{result.total_items}\")\n    print(f\"Tokens used: {result.total_input_tokens + result.total_output_tokens}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started Guide - Learn the basics</li> <li>Examples - See more examples</li> <li>API Reference - Full API documentation</li> <li>Contributing - Help improve async-batch-llm</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - See LICENSE for details.</p>"},{"location":"API/","title":"async-batch-llm API Reference","text":"<p>Complete API documentation for async-batch-llm v0.4.0.</p>"},{"location":"API/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Core Classes</li> <li>LLMWorkItem</li> <li>WorkItemResult</li> <li>BatchResult</li> <li>ParallelBatchProcessor</li> <li>LLM Strategies</li> <li>LLMCallStrategy (Abstract)</li> <li>PydanticAIStrategy</li> <li>GeminiStrategy</li> <li>GeminiCachedStrategy</li> <li>Configuration</li> <li>ProcessorConfig</li> <li>RetryConfig</li> <li>RateLimitConfig</li> <li>Error Handling</li> <li>ErrorClassifier</li> <li>ErrorInfo</li> <li>RateLimitStrategy</li> <li>Middleware &amp; Observers</li> <li>Middleware</li> <li>ProcessorObserver</li> <li>MetricsObserver</li> <li>Core Types</li> <li>TokenUsage</li> <li>FrameworkTimeoutError</li> <li>TokenTrackingError</li> <li>Type Aliases</li> <li>PostProcessorFunc</li> <li>ProgressCallbackFunc</li> </ul>"},{"location":"API/#core-classes","title":"Core Classes","text":""},{"location":"API/#llmworkitem","title":"LLMWorkItem","text":"<p>Represents a single work item to be processed by an LLM strategy.</p> <pre><code>@dataclass\nclass LLMWorkItem(Generic[TInput, TOutput, TContext]):\n    item_id: str\n    strategy: LLMCallStrategy[TOutput]\n    prompt: str = \"\"\n    context: TContext | None = None\n</code></pre> <p>Type Parameters:</p> <ul> <li><code>TInput</code>: Input data type (unused in v0.1, kept for backward compatibility)</li> <li><code>TOutput</code>: Expected output type from the LLM</li> <li><code>TContext</code>: Optional context data type passed through to results</li> </ul> <p>Fields:</p> <ul> <li><code>item_id</code> (str): Unique identifier for this work item. Must be non-empty.</li> <li><code>strategy</code> (LLMCallStrategy[TOutput]): Strategy that encapsulates how to make the LLM call</li> <li><code>prompt</code> (str, optional): The prompt/input to pass to the LLM. Default: \"\"</li> <li><code>context</code> (TContext | None, optional): Optional context data passed through to results/post-processor</li> </ul> <p>Example:</p> <pre><code>from async_batch_llm import LLMWorkItem, PydanticAIStrategy\nfrom pydantic_ai import Agent\n\nagent = Agent(\"openai:gpt-4\", result_type=MyOutput)\nstrategy = PydanticAIStrategy(agent=agent)\n\nwork_item = LLMWorkItem(\n    item_id=\"task_1\",\n    strategy=strategy,\n    prompt=\"Analyze this text...\",\n    context={\"user_id\": 123}\n)\n</code></pre> <p>Validation:</p> <ul> <li>Raises <code>ValueError</code> if <code>item_id</code> is empty or whitespace-only</li> <li>Raises <code>ValueError</code> if <code>item_id</code> is not a string</li> </ul>"},{"location":"API/#workitemresult","title":"WorkItemResult","text":"<p>Result of processing a single work item.</p> <pre><code>@dataclass\nclass WorkItemResult(Generic[TOutput, TContext]):\n    item_id: str\n    success: bool\n    output: TOutput | None = None\n    error: str | None = None\n    context: TContext | None = None\n    token_usage: TokenUsage = field(default_factory=dict)  # type: ignore[assignment]\n    gemini_safety_ratings: dict[str, str] | None = None\n</code></pre> <p>Fields:</p> <ul> <li><code>item_id</code> (str): ID of the work item</li> <li><code>success</code> (bool): Whether processing succeeded</li> <li><code>output</code> (TOutput | None): LLM output if successful, None if failed</li> <li><code>error</code> (str | None): Error message if failed, None if successful</li> <li><code>context</code> (TContext | None): Context data from the work item</li> <li><code>token_usage</code> (TokenUsage): Token usage statistics with optional fields:</li> <li><code>input_tokens</code> (int): Number of tokens in the input/prompt</li> <li><code>output_tokens</code> (int): Number of tokens in the output/completion</li> <li><code>total_tokens</code> (int): Total tokens used (input + output)</li> <li><code>cached_input_tokens</code> (int): Number of input tokens served from cache (Gemini context caching)</li> <li><code>gemini_safety_ratings</code> (dict[str, str] | None): Gemini API safety ratings if available</li> </ul> <p>Example:</p> <pre><code>result = await processor.process_all()\nfor item_result in result.results:\n    if item_result.success:\n        print(f\"\u2713 {item_result.item_id}: {item_result.output}\")\n        print(f\"  Tokens: {item_result.token_usage}\")\n    else:\n        print(f\"\u2717 {item_result.item_id}: {item_result.error}\")\n</code></pre>"},{"location":"API/#batchresult","title":"BatchResult","text":"<p>Result of processing a batch of work items.</p> <pre><code>@dataclass\nclass BatchResult(Generic[TOutput, TContext]):\n    results: list[WorkItemResult[TOutput, TContext]]\n    total_items: int = 0\n    succeeded: int = 0\n    failed: int = 0\n    total_input_tokens: int = 0\n    total_output_tokens: int = 0\n    total_cached_tokens: int = 0\n</code></pre> <p>Fields:</p> <ul> <li><code>results</code> (list[WorkItemResult]): List of individual work item results</li> <li><code>total_items</code> (int): Total number of items processed</li> <li><code>succeeded</code> (int): Number of successful items</li> <li><code>failed</code> (int): Number of failed items</li> <li><code>total_input_tokens</code> (int): Sum of input tokens across all items</li> <li><code>total_output_tokens</code> (int): Sum of output tokens across all items</li> <li><code>total_cached_tokens</code> (int): Sum of cached input tokens from Gemini context caching</li> </ul> <p>Note: Summary statistics are calculated automatically in <code>__post_init__</code>.</p> <p>Example:</p> <pre><code>result = await processor.process_all()\n\nprint(f\"Processed {result.total_items} items\")\nprint(f\"Success: {result.succeeded}, Failed: {result.failed}\")\nprint(f\"Total tokens: {result.total_input_tokens + result.total_output_tokens}\")\n\n# Access individual results\nfor item_result in result.results:\n    if item_result.success:\n        process_output(item_result.output)\n</code></pre>"},{"location":"API/#parallelbatchprocessor","title":"ParallelBatchProcessor","text":"<p>Main processor that executes work items in parallel.</p> <pre><code>class ParallelBatchProcessor(\n    BatchProcessor[TInput, TOutput, TContext],\n    Generic[TInput, TOutput, TContext]\n):\n    def __init__(\n        self,\n        config: ProcessorConfig,\n        post_processor: PostProcessorFunc[TOutput, TContext] | None = None,\n        progress_callback: ProgressCallbackFunc | None = None,\n        error_classifier: ErrorClassifier | None = None,\n        rate_limit_strategy: RateLimitStrategy | None = None,\n        middlewares: list[Middleware] | None = None,\n        observers: list[ProcessorObserver] | None = None,\n    )\n</code></pre> <p>Parameters:</p> <ul> <li><code>config</code> (ProcessorConfig): Configuration for the processor</li> <li><code>post_processor</code> (PostProcessorFunc | None): Optional async function called after each item</li> <li><code>progress_callback</code> (ProgressCallbackFunc | None): Optional callback for progress updates</li> <li><code>error_classifier</code> (ErrorClassifier | None): Custom error classifier. Default: <code>DefaultErrorClassifier()</code></li> <li><code>rate_limit_strategy</code> (RateLimitStrategy | None): Custom rate limit handling. Default: <code>ExponentialBackoffStrategy()</code></li> <li><code>middlewares</code> (list[Middleware] | None): List of middleware for pre/post processing</li> <li><code>observers</code> (list[ProcessorObserver] | None): List of observers for monitoring events</li> </ul> <p>Post-processing: The optional <code>post_processor</code> runs inline on the worker as soon as an item finishes. It should hand off any heavy operations (long DB writes, expensive analytics, etc.) to another system; if the function takes too long the worker sits idle until the 75\u202fs timeout triggers, reducing overall throughput.</p> <p>Methods:</p>"},{"location":"API/#async-def-add_workwork_item-llmworkitem-none","title":"<code>async def add_work(work_item: LLMWorkItem) -&gt; None</code>","text":"<p>Add a work item to the processing queue.</p> <pre><code>await processor.add_work(work_item)\n</code></pre> <p>Note: If <code>max_queue_size</code> is set and queue is full, this will block until space is available.</p>"},{"location":"API/#async-def-process_all-batchresult","title":"<code>async def process_all() -&gt; BatchResult</code>","text":"<p>Process all work items in the queue.</p> <pre><code>result = await processor.process_all()\n</code></pre> <p>Returns: <code>BatchResult</code> containing all results and statistics</p> <p>Behavior:</p> <ol> <li>Starts worker tasks (up to <code>max_workers</code>)</li> <li>Workers process items from queue with retry logic</li> <li>Waits for all work to complete</li> <li>Returns aggregated results</li> </ol>"},{"location":"API/#async-def-cleanup-none","title":"<code>async def cleanup() -&gt; None</code>","text":"<p>Clean up resources (cancel pending workers, clear queue).</p> <pre><code>await processor.cleanup()\n</code></pre> <p>Note: Automatically called when using async context manager.</p>"},{"location":"API/#context-manager-support","title":"Context Manager Support","text":"<pre><code>async with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(item)\n    result = await processor.process_all()\n# Automatic cleanup\n</code></pre> <p>Example:</p> <pre><code>from async_batch_llm import ParallelBatchProcessor, ProcessorConfig, LLMWorkItem\n\nconfig = ProcessorConfig(max_workers=5, timeout_per_item=60.0)\n\nasync with ParallelBatchProcessor(config=config) as processor:\n    for i in range(100):\n        work_item = LLMWorkItem(\n            item_id=f\"item_{i}\",\n            strategy=my_strategy,\n            prompt=f\"Task {i}\"\n        )\n        await processor.add_work(work_item)\n\n    result = await processor.process_all()\n    print(f\"Completed: {result.succeeded}/{result.total_items}\")\n</code></pre>"},{"location":"API/#llm-strategies","title":"LLM Strategies","text":""},{"location":"API/#llmcallstrategy","title":"LLMCallStrategy","text":"<p>Abstract base class for LLM call strategies.</p> <pre><code>class LLMCallStrategy(ABC, Generic[TOutput]):\n    async def prepare(self) -&gt; None: ...\n\n    @abstractmethod\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,\n    ) -&gt; tuple[TOutput, TokenUsage]: ...\n\n    async def on_error(\n        self,\n        exception: Exception,\n        attempt: int,\n        state: RetryState | None = None,\n    ) -&gt; None: ...\n\n    async def cleanup(self) -&gt; None: ...\n\n    async def dry_run(self, prompt: str) -&gt; tuple[TOutput, TokenUsage]: ...\n</code></pre> <p>Lifecycle:</p> <ol> <li><code>prepare()</code> - Called once before any retry attempts</li> <li>For each attempt (including retries):</li> <li><code>execute()</code> is called (or <code>dry_run()</code> if <code>config.dry_run=True</code>)</li> <li>If <code>execute()</code> raises an exception, <code>on_error()</code> is called before retry logic</li> <li><code>cleanup()</code> - Called once after all attempts complete</li> </ol> <p>Methods:</p>"},{"location":"API/#async-def-prepare-none","title":"<code>async def prepare() -&gt; None</code>","text":"<p>Initialize resources before making LLM calls (e.g., create caches, initialize clients).</p> <p>Default: No-op</p>"},{"location":"API/#async-def-executeprompt-str-attempt-int-timeout-float-state-retrystate-none-none-tupletoutput-tokenusage","title":"<code>async def execute(prompt: str, attempt: int, timeout: float, state: RetryState | None = None) -&gt; tuple[TOutput, TokenUsage]</code>","text":"<p>Execute an LLM call.</p> <p>Parameters:</p> <ul> <li><code>prompt</code> (str): The prompt to send to the LLM</li> <li><code>attempt</code> (int): Which retry attempt this is (1, 2, 3, ...)</li> <li><code>timeout</code> (float): Maximum time to wait for response (seconds)</li> <li>Note: Timeout enforcement is handled by the framework wrapping this call in <code>asyncio.wait_for()</code></li> <li><code>state</code> (RetryState | None): Mutable per-work-item state provided by the framework   so strategies can track partial progress across retries</li> </ul> <p>Returns: Tuple of <code>(output, token_usage)</code></p> <ul> <li><code>output</code> (TOutput): The LLM response</li> <li><code>token_usage</code> (TokenUsage): Token usage dict with optional keys: <code>input_tokens</code>,   <code>output_tokens</code>, <code>total_tokens</code>, <code>cached_input_tokens</code></li> </ul> <p>Raises: Any exception to trigger retry (if retryable) or failure</p>"},{"location":"API/#async-def-dry_runprompt-str-tupletoutput-tokenusage","title":"<code>async def dry_run(prompt: str) -&gt; tuple[TOutput, TokenUsage]</code>","text":"<p>Return mock output for dry-run mode (testing without API calls).</p> <p>Called when <code>ProcessorConfig(dry_run=True)</code> is set. Override this method to provide realistic mock data for testing.</p> <p>Parameters:</p> <ul> <li><code>prompt</code> (str): The prompt that would have been sent to the LLM</li> </ul> <p>Returns: Tuple of <code>(mock_output, mock_token_usage)</code></p> <p>Default behavior:</p> <ul> <li>Returns string <code>\"[DRY-RUN] Mock output for prompt: {prompt[:50]}...\"</code> as output</li> <li>Returns mock token usage: 100 input, 50 output, 150 total tokens</li> </ul> <p>Example override:</p> <pre><code>class MyStrategy(LLMCallStrategy[Output]):\n    async def dry_run(self, prompt: str) -&gt; tuple[Output, TokenUsage]:\n        # Return realistic mock data\n        mock_output = Output(result=\"Test result\")\n        mock_tokens: TokenUsage = {\n            \"input_tokens\": len(prompt.split()),\n            \"output_tokens\": 50,\n            \"total_tokens\": len(prompt.split()) + 50,\n        }\n        return mock_output, mock_tokens\n</code></pre>"},{"location":"API/#async-def-on_errorexception-exception-attempt-int-none","title":"<code>async def on_error(exception: Exception, attempt: int) -&gt; None</code>","text":"<p>Handle errors that occur during execute().</p> <p>Called by the framework when <code>execute()</code> raises an exception, before deciding whether to retry. This allows strategies to:</p> <ul> <li>Inspect the error type to adjust retry behavior</li> <li>Store error information for use in the next attempt</li> <li>Modify prompts based on validation errors</li> <li>Track error patterns across attempts</li> <li>Make intelligent decisions (e.g., escalate to smarter model only on validation errors)</li> </ul> <p>Parameters:</p> <ul> <li><code>exception</code> (Exception): The exception that was raised during <code>execute()</code></li> <li><code>attempt</code> (int): Which attempt number failed (1, 2, 3, ...)</li> </ul> <p>Default: No-op</p> <p>Use Cases:</p> <ol> <li>Smart Model Escalation - Only escalate to expensive models on validation errors, not    network errors:</li> </ol> <pre><code>class SmartModelEscalationStrategy(LLMCallStrategy[Output]):\n    def __init__(self):\n        self.validation_failures = 0\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        if isinstance(exception, ValidationError):\n            self.validation_failures += 1\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Only escalate model on validation errors\n        model_index = min(self.validation_failures, len(MODELS) - 1)\n        model = MODELS[model_index]\n        # Make call with appropriate model...\n</code></pre> <ol> <li>Smart Retry with Partial Parsing - Build better retry prompts based on what failed:</li> </ol> <pre><code>class SmartRetryStrategy(LLMCallStrategy[Output]):\n    def __init__(self):\n        self.last_error = None\n        self.last_response = None\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        if isinstance(exception, ValidationError):\n            self.last_error = exception\n            # last_response set in execute() before raising\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        if attempt &gt; 1 and self.last_error:\n            # Build smart retry prompt with partial parsing feedback\n            prompt = self._create_retry_prompt_with_partial_data(prompt)\n        # Make call with improved prompt...\n</code></pre> <ol> <li>Error Type Tracking - Distinguish between different error types:</li> </ol> <pre><code>class ErrorTrackingStrategy(LLMCallStrategy[Output]):\n    def __init__(self):\n        self.validation_errors = 0\n        self.network_errors = 0\n        self.rate_limit_errors = 0\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        if isinstance(exception, ValidationError):\n            self.validation_errors += 1\n        elif isinstance(exception, ConnectionError):\n            self.network_errors += 1\n        elif \"429\" in str(exception):\n            self.rate_limit_errors += 1\n</code></pre> <p>Important Notes:</p> <ul> <li>Exceptions in <code>on_error()</code> are caught and logged by the framework - they won't crash processing</li> <li><code>on_error()</code> is only called when <code>execute()</code> raises an exception, not on success</li> <li>The error is still propagated to the framework's retry logic after <code>on_error()</code> returns</li> <li>For stateful strategies, each work item should use a separate strategy instance</li> </ul> <p>See Also:</p> <ul> <li>examples/example_smart_model_escalation.py - Complete   smart model escalation example</li> <li>examples/example_gemini_smart_retry.py - Smart retry with   partial parsing</li> </ul>"},{"location":"API/#async-def-cleanup-none_1","title":"<code>async def cleanup() -&gt; None</code>","text":"<p>Clean up resources after all attempts complete (e.g., delete caches, close clients).</p> <p>Default: No-op</p> <p>Custom Strategy Example:</p> <pre><code>from async_batch_llm import LLMCallStrategy, TokenUsage\n\nclass MyCustomStrategy(LLMCallStrategy[str]):\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[str, TokenUsage]:\n        # Your custom LLM API call\n        response = await my_llm_api.generate(prompt)\n\n        tokens: TokenUsage = {\n            \"input_tokens\": response.input_tokens,\n            \"output_tokens\": response.output_tokens,\n            \"total_tokens\": response.total_tokens,\n        }\n\n        return response.text, tokens\n</code></pre>"},{"location":"API/#pydanticaistrategy","title":"PydanticAIStrategy","text":"<p>Strategy for using PydanticAI agents.</p> <pre><code>class PydanticAIStrategy(LLMCallStrategy[TOutput]):\n    def __init__(self, agent: Agent[None, TOutput])\n</code></pre> <p>Parameters:</p> <ul> <li><code>agent</code> (Agent[None, TOutput]): Configured PydanticAI agent</li> </ul> <p>Requires: <code>pip install 'async-batch-llm[pydantic-ai]'</code></p> <p>Example:</p> <pre><code>from async_batch_llm import PydanticAIStrategy, LLMWorkItem\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass BookSummary(BaseModel):\n    title: str\n    summary: str\n\nagent = Agent(\"openai:gpt-4\", result_type=BookSummary)\nstrategy = PydanticAIStrategy(agent=agent)\n\nwork_item = LLMWorkItem(\n    item_id=\"book_1\",\n    strategy=strategy,\n    prompt=\"Summarize: The Great Gatsby...\"\n)\n</code></pre>"},{"location":"API/#geministrategy","title":"GeminiStrategy","text":"<p>Strategy for calling Google Gemini API directly (without caching).</p> <pre><code>class GeminiStrategy(LLMCallStrategy[TOutput]):\n    def __init__(\n        self,\n        model: str,\n        client: genai.Client,\n        response_parser: Callable[[Any], TOutput],\n        config: GenerateContentConfig | None = None,\n        include_metadata: bool = False,\n    )\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (str): Model name (e.g., \"gemini-2.5-flash\")</li> <li><code>client</code> (genai.Client): Initialized Gemini client</li> <li><code>response_parser</code> (Callable): Function to parse response into TOutput</li> <li><code>config</code> (GenerateContentConfig | None): Optional generation config (temperature, etc.)</li> <li><code>include_metadata</code> (bool): If True, <code>execute()</code> returns <code>GeminiResponse[TOutput]</code>   so you can access safety ratings, finish reasons, and raw responses</li> </ul> <p>Requires: <code>pip install 'async-batch-llm[gemini]'</code></p> <p>API key: Set <code>GOOGLE_API_KEY</code> (preferred) or the legacy <code>GEMINI_API_KEY</code> environment variable before running this example.</p> <p>Example:</p> <pre><code>from async_batch_llm import GeminiStrategy, LLMWorkItem\nfrom google import genai\n\nclient = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"))\n\ndef parse_response(response) -&gt; str:\n    return response.text\n\nstrategy = GeminiStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n)\n\nwork_item = LLMWorkItem(\n    item_id=\"task_1\",\n    strategy=strategy,\n    prompt=\"Explain quantum computing\"\n)\n</code></pre>"},{"location":"API/#geminicachedstrategy","title":"GeminiCachedStrategy","text":"<p>Strategy for calling Google Gemini API with context caching.</p> <pre><code>class GeminiCachedStrategy(LLMCallStrategy[TOutput]):\n    def __init__(\n        self,\n        model: str,\n        client: genai.Client,\n        response_parser: Callable[[Any], TOutput],\n        cached_content: list[Content],\n        cache_ttl_seconds: int = 3600,\n        cache_refresh_threshold: float = 0.1,\n        cache_renewal_buffer_seconds: int = 300,\n        auto_renew: bool = True,\n        include_metadata: bool = False,\n        cache_tags: dict[str, str] | None = None,\n        config: GenerateContentConfig | None = None,\n    )\n</code></pre> <p>Parameters:</p> <ul> <li><code>model</code> (str): Model name</li> <li><code>client</code> (genai.Client): Initialized Gemini client</li> <li><code>response_parser</code> (Callable): Function to parse response</li> <li><code>cached_content</code> (list[Content]): Content to cache (system instructions, documents)</li> <li><code>cache_ttl_seconds</code> (int): Cache TTL in seconds. Default: 3600 (1 hour)</li> <li><code>cache_refresh_threshold</code> (float): Legacy refresh fraction parameter (kept for compatibility)</li> <li><code>cache_renewal_buffer_seconds</code> (int): Renew caches this many seconds before expiry (default 300)</li> <li><code>auto_renew</code> (bool): Automatically renew caches when they near expiry. Default: True</li> <li><code>include_metadata</code> (bool): Wrap outputs in <code>GeminiResponse[TOutput]</code> when True</li> <li><code>cache_tags</code> (dict[str, str] | None): Optional metadata for cache matching/versioning</li> <li><code>config</code> (GenerateContentConfig | None): Optional generation config</li> </ul> <p>Lifecycle:</p> <ul> <li><code>prepare()</code>: Finds or creates the Gemini cache</li> <li><code>execute()</code>: Uses the cache and auto-renews when enabled</li> <li><code>cleanup()</code>: Runs once when the processor exits; by default caches are left alive so   future batches can reuse them (call <code>delete_cache()</code> to remove immediately)</li> </ul> <p>Requires: <code>pip install 'async-batch-llm[gemini]'</code></p> <p>API key: Same as above \u2013 <code>GOOGLE_API_KEY</code> is preferred, <code>GEMINI_API_KEY</code> also works.</p> <p>Example:</p> <pre><code>from async_batch_llm import GeminiCachedStrategy, LLMWorkItem\nfrom google import genai\nfrom google.genai.types import Content\n\nclient = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"))\n\n# Large document to cache\ncached_content = [\n    Content(role=\"system\", parts=[{\"text\": \"You are a helpful assistant.\"}]),\n    Content(role=\"user\", parts=[{\"text\": large_document}]),\n]\n\nstrategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=lambda r: r.text,\n    cached_content=cached_content,\n    cache_ttl_seconds=3600,\n)\n\n# Reuse strategy across multiple work items to benefit from caching\nfor i in range(100):\n    work_item = LLMWorkItem(\n        item_id=f\"task_{i}\",\n        strategy=strategy,  # Same strategy, shared cache\n        prompt=f\"Question {i} about the document\"\n    )\n    await processor.add_work(work_item)\n</code></pre>"},{"location":"API/#configuration","title":"Configuration","text":""},{"location":"API/#processorconfig","title":"ProcessorConfig","text":"<p>Complete configuration for batch processor.</p> <pre><code>@dataclass\nclass ProcessorConfig:\n    max_workers: int = 5\n    timeout_per_item: float = 120.0\n    retry: RetryConfig = field(default_factory=RetryConfig)\n    rate_limit: RateLimitConfig = field(default_factory=RateLimitConfig)\n    progress_interval: int = 10\n    progress_callback_timeout: float | None = 5.0\n    enable_detailed_logging: bool = False\n    max_queue_size: int = 0\n    max_requests_per_minute: float | None = None\n    dry_run: bool = False\n</code></pre> <p>Fields:</p> <ul> <li><code>max_workers</code> (int): Maximum number of concurrent workers. Default: 5</li> <li><code>timeout_per_item</code> (float): Timeout per item in seconds (includes retries). Default: 120.0</li> <li><code>retry</code> (RetryConfig): Retry configuration</li> <li><code>rate_limit</code> (RateLimitConfig): Rate limit handling configuration</li> <li><code>progress_interval</code> (int): Log progress every N items. Default: 10</li> <li><code>progress_callback_timeout</code> (float | None): Max seconds to wait for progress callback. Default: 5.0.   Set to <code>None</code> for no timeout.</li> <li><code>enable_detailed_logging</code> (bool): Enable detailed debug logging. Default: False</li> <li><code>max_queue_size</code> (int): Max queue size (0 = unlimited). Default: 0</li> <li><code>max_requests_per_minute</code> (float | None): Optional proactive rate limiter that throttles   requests before hitting provider limits</li> <li><code>dry_run</code> (bool): Skip actual API calls, use mock data from <code>strategy.dry_run()</code>. Default: False</li> </ul> <p>Example:</p> <pre><code>from async_batch_llm import ProcessorConfig, RetryConfig\n\nconfig = ProcessorConfig(\n    max_workers=10,\n    timeout_per_item=60.0,\n    retry=RetryConfig(max_attempts=5, initial_wait=2.0),\n    progress_interval=20,\n    max_queue_size=1000,\n)\n</code></pre>"},{"location":"API/#retryconfig","title":"RetryConfig","text":"<p>Configuration for retry behavior.</p> <pre><code>@dataclass\nclass RetryConfig:\n    max_attempts: int = 3\n    initial_wait: float = 1.0\n    max_wait: float = 60.0\n    exponential_base: float = 2.0\n    jitter: bool = True\n</code></pre> <p>Fields:</p> <ul> <li><code>max_attempts</code> (int): Maximum retry attempts. Default: 3</li> <li><code>initial_wait</code> (float): Initial wait time in seconds. Default: 1.0</li> <li><code>max_wait</code> (float): Maximum wait time in seconds. Default: 60.0</li> <li><code>exponential_base</code> (float): Exponential backoff base. Default: 2.0</li> <li><code>jitter</code> (bool): Add random jitter to wait times. Default: True</li> </ul> <p>Validation:</p> <ul> <li><code>max_attempts</code> must be &gt;= 1</li> <li><code>initial_wait</code> must be &gt; 0</li> <li><code>max_wait</code> must be &gt;= initial_wait</li> <li><code>exponential_base</code> must be &gt;= 1</li> </ul> <p>Example:</p> <pre><code>retry_config = RetryConfig(\n    max_attempts=5,\n    initial_wait=2.0,\n    max_wait=120.0,\n    exponential_base=2.0,\n    jitter=True,\n)\n</code></pre>"},{"location":"API/#ratelimitconfig","title":"RateLimitConfig","text":"<p>Configuration for rate limit handling.</p> <pre><code>@dataclass\nclass RateLimitConfig:\n    cooldown_seconds: float = 300.0\n    slow_start_items: int = 50\n    slow_start_initial_delay: float = 2.0\n    slow_start_final_delay: float = 0.1\n    backoff_multiplier: float = 1.5\n</code></pre> <p>Fields:</p> <ul> <li><code>cooldown_seconds</code> (float): Cooldown after rate limit. Default: 300.0 (5 minutes)</li> <li><code>slow_start_items</code> (int): Number of items for slow start. Default: 50</li> <li><code>slow_start_initial_delay</code> (float): Initial delay in slow start. Default: 2.0</li> <li><code>slow_start_final_delay</code> (float): Final delay in slow start. Default: 0.1</li> <li><code>backoff_multiplier</code> (float): Increase cooldown on repeated rate limits. Default: 1.5</li> </ul> <p>Validation:</p> <ul> <li><code>cooldown_seconds</code> must be &gt;= 0</li> <li><code>slow_start_items</code> must be &gt;= 0</li> <li><code>slow_start_initial_delay</code> must be &gt;= slow_start_final_delay</li> <li><code>backoff_multiplier</code> must be &gt;= 1.0</li> </ul>"},{"location":"API/#error-handling","title":"Error Handling","text":""},{"location":"API/#errorclassifier","title":"ErrorClassifier","text":"<p>Interface for classifying errors as retryable or not.</p> <pre><code>class ErrorClassifier(ABC):\n    @abstractmethod\n    def classify(self, exception: Exception) -&gt; ErrorInfo: ...\n</code></pre> <p>Built-in Implementations:</p> <ul> <li><code>DefaultErrorClassifier</code>: Provider-agnostic classification based on exception types</li> <li><code>GeminiErrorClassifier</code>: Specialized for Google Gemini API errors</li> </ul> <p>Custom Example:</p> <pre><code>from async_batch_llm import ErrorClassifier, ErrorInfo\n\nclass MyErrorClassifier(ErrorClassifier):\n    def classify(self, exception: Exception) -&gt; ErrorInfo:\n        error_str = str(exception).lower()\n\n        if \"rate limit\" in error_str:\n            return ErrorInfo(\n                is_retryable=True,\n                is_rate_limit=True,\n                category=\"rate_limit\",\n            )\n        elif \"timeout\" in error_str:\n            return ErrorInfo(is_retryable=True, category=\"timeout\")\n        else:\n            return ErrorInfo(is_retryable=False, category=\"unknown\")\n</code></pre>"},{"location":"API/#errorinfo","title":"ErrorInfo","text":"<p>Information about a classified error.</p> <pre><code>@dataclass\nclass ErrorInfo:\n    is_retryable: bool\n    is_rate_limit: bool\n    is_timeout: bool\n    error_category: str\n    suggested_wait: float | None = None\n</code></pre> <p>Fields:</p> <ul> <li><code>is_retryable</code> (bool): Whether the error should trigger a retry</li> <li><code>is_rate_limit</code> (bool): Whether this is a rate limit error (429, resource_exhausted, etc.)</li> <li><code>is_timeout</code> (bool): Whether this is a timeout error (framework or API timeout)</li> <li><code>error_category</code> (str): Error category for logging/metrics. Common values:</li> <li><code>\"framework_timeout\"</code> - Framework timeout (exceeded <code>timeout_per_item</code>)</li> <li><code>\"api_timeout\"</code> - API-level timeout</li> <li><code>\"rate_limit\"</code> - Rate limit error</li> <li><code>\"validation_error\"</code> - Pydantic validation error</li> <li><code>\"client_error\"</code> - 4xx client error</li> <li><code>\"server_error\"</code> - 5xx server error</li> <li><code>\"connection_error\"</code> - Network connection error</li> <li><code>\"unknown\"</code> - Unclassified error</li> <li><code>suggested_wait</code> (float | None): Suggested wait time before retry (seconds). Used for rate limits.</li> </ul> <p>Example:</p> <pre><code>from async_batch_llm import ErrorInfo\n\n# Rate limit error\nrate_limit_info = ErrorInfo(\n    is_retryable=False,  # Don't retry via exponential backoff\n    is_rate_limit=True,  # Trigger rate limit cooldown\n    is_timeout=False,\n    error_category=\"rate_limit\",\n    suggested_wait=300.0,  # 5 minute cooldown\n)\n\n# Framework timeout (retryable, might succeed if faster)\ntimeout_info = ErrorInfo(\n    is_retryable=True,\n    is_rate_limit=False,\n    is_timeout=True,\n    error_category=\"framework_timeout\",\n)\n</code></pre>"},{"location":"API/#ratelimitstrategy","title":"RateLimitStrategy","text":"<p>Interface for custom rate limit handling strategies.</p> <pre><code>class RateLimitStrategy(ABC):\n    @abstractmethod\n    async def handle_rate_limit(\n        self, error_info: ErrorInfo, attempt: int\n    ) -&gt; float: ...\n</code></pre> <p>Built-in Implementations:</p> <ul> <li><code>ExponentialBackoffStrategy</code>: Exponential backoff with configurable parameters</li> <li><code>FixedDelayStrategy</code>: Fixed delay between retries</li> </ul>"},{"location":"API/#middleware-observers","title":"Middleware &amp; Observers","text":""},{"location":"API/#middleware","title":"Middleware","text":"<p>Interface for middleware that can modify work items before/after processing.</p> <pre><code>class Middleware(ABC):\n    async def before_process(\n        self, work_item: LLMWorkItem\n    ) -&gt; LLMWorkItem | None: ...\n\n    async def after_process(\n        self, work_item: LLMWorkItem, result: WorkItemResult\n    ) -&gt; WorkItemResult: ...\n\n    async def on_error(\n        self, work_item: LLMWorkItem, error: Exception\n    ) -&gt; None: ...\n</code></pre> <p>Methods:</p> <ul> <li><code>before_process()</code>: Modify work item before processing. Return <code>None</code> to skip.</li> <li><code>after_process()</code>: Modify result after processing</li> <li><code>on_error()</code>: Handle errors (doesn't stop processing)</li> </ul> <p>Example:</p> <pre><code>from async_batch_llm.middleware import BaseMiddleware\n\nclass LoggingMiddleware(BaseMiddleware):\n    async def before_process(self, work_item):\n        print(f\"Processing {work_item.item_id}\")\n        return work_item\n\n    async def after_process(self, work_item, result):\n        print(f\"Completed {work_item.item_id}: {result.success}\")\n        return result\n</code></pre>"},{"location":"API/#processorobserver","title":"ProcessorObserver","text":"<p>Interface for observers that monitor processing events.</p> <pre><code>class ProcessorObserver(ABC):\n    @abstractmethod\n    async def on_event(\n        self, event: ProcessingEvent, data: dict[str, Any]\n    ) -&gt; None: ...\n</code></pre> <p>Events:</p> <ul> <li><code>BATCH_STARTED</code>: <code>{total, max_workers, start_time}</code></li> <li><code>BATCH_COMPLETED</code>: <code>{processed, succeeded, failed, total, total_tokens, cached_input_tokens, duration}</code></li> <li><code>WORKER_STARTED</code> / <code>WORKER_STOPPED</code>: <code>{worker_id}</code></li> <li><code>ITEM_STARTED</code>: <code>{item_id, worker_id}</code></li> <li><code>ITEM_COMPLETED</code>: <code>{item_id, duration, tokens}</code></li> <li><code>ITEM_FAILED</code>: <code>{item_id, error_type}</code></li> <li><code>RATE_LIMIT_HIT</code>: <code>{item_id, worker_id}</code></li> <li><code>COOLDOWN_STARTED</code>: <code>{worker_id, duration, consecutive}</code></li> <li><code>COOLDOWN_ENDED</code>: <code>{duration, error?}</code></li> </ul> <p>Cleanup note:</p> <ul> <li>Preferred: wrap <code>ParallelBatchProcessor</code> in <code>async with</code> so strategy cleanup runs automatically.</li> <li>If you do not use a context manager, call <code>await processor.shutdown()</code> after <code>process_all()</code> to flush   observers, stop workers, and run strategy cleanups.</li> </ul>"},{"location":"API/#metricsobserver","title":"MetricsObserver","text":"<p>Built-in observer for collecting metrics.</p> <pre><code>class MetricsObserver(BaseObserver):\n    async def get_metrics(self) -&gt; dict[str, Any]: ...\n    async def export_json(self) -&gt; str: ...\n    async def export_prometheus(self) -&gt; str: ...\n    async def export_dict(self) -&gt; dict[str, Any]: ...\n</code></pre> <p>Methods:</p> <ul> <li><code>get_metrics()</code>: Get current metrics as dict</li> <li><code>export_json()</code>: Export metrics as JSON string</li> <li><code>export_prometheus()</code>: Export in Prometheus text format</li> <li><code>export_dict()</code>: Export as dictionary</li> </ul> <p>Example:</p> <pre><code>from async_batch_llm import MetricsObserver\n\nmetrics = MetricsObserver()\nprocessor = ParallelBatchProcessor(config=config, observers=[metrics])\n\nawait processor.process_all()\n\n# Get metrics\nmetrics_data = await metrics.get_metrics()\nprint(f\"Items processed: {metrics_data['items_processed']}\")\nprint(f\"Success rate: {metrics_data['success_rate']:.1%}\")\n\n# Export for monitoring\nprometheus_text = await metrics.export_prometheus()\n</code></pre>"},{"location":"API/#core-types","title":"Core Types","text":""},{"location":"API/#tokenusage","title":"TokenUsage","text":"<p>TypedDict for token usage statistics from LLM API calls.</p> <pre><code>class TokenUsage(TypedDict, total=False):\n    input_tokens: int\n    output_tokens: int\n    total_tokens: int\n    cached_input_tokens: int\n</code></pre> <p>Fields (all optional):</p> <ul> <li><code>input_tokens</code> (int): Number of tokens in the input/prompt</li> <li><code>output_tokens</code> (int): Number of tokens in the output/completion</li> <li><code>total_tokens</code> (int): Total tokens used (input + output)</li> <li><code>cached_input_tokens</code> (int): Number of input tokens served from cache (Gemini context caching)</li> </ul> <p>Notes:</p> <ul> <li>All fields are optional to accommodate different provider APIs</li> <li>Different providers may return different subsets of these fields</li> <li>Use <code>.get()</code> method for safe access: <code>tokens.get(\"input_tokens\", 0)</code></li> </ul> <p>Example:</p> <pre><code>from async_batch_llm import TokenUsage\n\ntokens: TokenUsage = {\n    \"input_tokens\": 150,\n    \"output_tokens\": 75,\n    \"total_tokens\": 225,\n}\n\n# Safe access\ninput_tokens = tokens.get(\"input_tokens\", 0)\n\n# Gemini with caching\ngemini_tokens: TokenUsage = {\n    \"input_tokens\": 50,  # New tokens only\n    \"output_tokens\": 75,\n    \"total_tokens\": 125,\n    \"cached_input_tokens\": 1000,  # Tokens served from cache\n}\n</code></pre>"},{"location":"API/#retrystate","title":"RetryState","text":"<p>Mutable per-work-item state that persists across retries. The framework creates a <code>RetryState</code> instance for each <code>LLMWorkItem</code> and passes it to both <code>strategy.execute(...)</code> and <code>strategy.on_error(...)</code> via the <code>state</code> parameter.</p> <pre><code>from dataclasses import dataclass, field\n\n@dataclass\nclass RetryState:\n    data: dict[str, Any] = field(default_factory=dict)\n\n    def get(self, key: str, default: Any = None) -&gt; Any: ...\n    def set(self, key: str, value: Any) -&gt; None: ...\n    def delete(self, key: str, raise_if_missing: bool = False) -&gt; None: ...\n    def clear(self) -&gt; None: ...\n</code></pre> <p>Typical uses:</p> <ul> <li>Track validation failures to escalate models only when schema validation fails</li> <li>Store partial results so retries request only the missing fields</li> <li>Record which advanced retry prompt should be used next</li> </ul> <p>Example:</p> <pre><code>async def execute(\n    self, prompt: str, attempt: int, timeout: float, state: RetryState | None = None\n):\n    state = state or RetryState()\n    missing = state.get(\"missing_fields\", [\"name\", \"email\"])\n    response = await self.client.generate(prompt, focus=missing)\n    result = parse(response)\n\n    missing = [f for f in ALL_FIELDS if f not in result]\n    if missing:\n        state.set(\"missing_fields\", missing)\n        raise ValidationError(\"Still missing fields\", result)\n    state.delete(\"missing_fields\", raise_if_missing=False)\n    return result, extract_tokens(response)\n</code></pre> <p>Because the same <code>RetryState</code> instance is reused across attempts, each retry can build on the previous attempt\u2019s context without relying on global variables.</p>"},{"location":"API/#geminiresponse","title":"GeminiResponse","text":"<p>Container object returned when <code>include_metadata=True</code> on <code>GeminiStrategy</code> or <code>GeminiCachedStrategy</code>. It lets you access Gemini-specific metadata while still receiving the parsed output.</p> <pre><code>@dataclass\nclass GeminiResponse(Generic[TOutput]):\n    output: TOutput\n    safety_ratings: dict[str, str] | None\n    finish_reason: str | None\n    token_usage: dict[str, int]\n    raw_response: Any  # google-genai response object\n</code></pre> <p>Usage:</p> <pre><code>result = await processor.process_all()\nfirst = result.results[0]\nif isinstance(first.output, GeminiResponse):\n    parsed = first.output.output\n    ratings = first.output.safety_ratings\n    if ratings and ratings.get(\"HARM_CATEGORY_HATE_SPEECH\") == \"HIGH\":\n        log_flagged_content(parsed)\n</code></pre> <p>If you do not opt into metadata (<code>include_metadata=False</code>), the strategies return the plain <code>TOutput</code> to avoid type checking overhead.</p>"},{"location":"API/#frameworktimeouterror","title":"FrameworkTimeoutError","text":"<p>Exception raised when framework-level timeout is exceeded.</p> <pre><code>class FrameworkTimeoutError(TimeoutError):\n    \"\"\"\n    Timeout enforced by the async-batch-llm framework (asyncio.wait_for).\n\n    This distinguishes framework-level timeouts from API-level timeouts.\n    Framework timeouts indicate the configured timeout_per_item was exceeded,\n    whereas API timeouts indicate the LLM provider returned a timeout error.\n    \"\"\"\n</code></pre> <p>Purpose:</p> <p>Differentiates between:</p> <ul> <li>Framework timeout: <code>asyncio.wait_for()</code> timed out (exceeded <code>timeout_per_item</code>)</li> <li>API timeout: LLM provider returned timeout error (network issue, slow response)</li> </ul> <p>Error Classification:</p> <ul> <li><code>is_retryable</code>: <code>True</code> (might succeed if LLM is faster on retry)</li> <li><code>is_timeout</code>: <code>True</code></li> <li><code>error_category</code>: <code>\"framework_timeout\"</code></li> </ul> <p>When to increase <code>timeout_per_item</code>:</p> <p>If you see frequent <code>FrameworkTimeoutError</code>, it indicates:</p> <ol> <li>LLM calls are taking longer than configured timeout</li> <li>Retry delays don't fit within timeout window</li> <li>Solution: Increase <code>timeout_per_item</code> or reduce retry configuration</li> </ol> <p>Example:</p> <pre><code>from async_batch_llm import FrameworkTimeoutError\n\ntry:\n    result = await processor.process_all()\nexcept FrameworkTimeoutError as e:\n    print(f\"Framework timeout: {e}\")\n    print(\"Consider increasing timeout_per_item in config\")\n\n# Or check in results\nfor item_result in result.results:\n    if not item_result.success and \"FrameworkTimeoutError\" in item_result.error:\n        print(f\"{item_result.item_id} exceeded timeout\")\n</code></pre>"},{"location":"API/#tokentrackingerror","title":"TokenTrackingError","text":"<p>Exception wrapper that preserves token usage from failed LLM calls.</p> <pre><code>class TokenTrackingError(Exception):\n    \"\"\"\n    Wrapper exception that preserves token usage from failed LLM calls.\n\n    When an LLM call fails (e.g., validation error), we still want to track\n    the tokens that were consumed. This wrapper attaches token usage to\n    exceptions that don't natively support it.\n    \"\"\"\n\n    def __init__(self, message: str, *, token_usage: dict[str, int] | None = None):\n        super().__init__(message)\n        self._failed_token_usage = token_usage or {}\n</code></pre> <p>Purpose:</p> <p>When an LLM call succeeds in getting a response but fails during parsing/validation, the tokens were still consumed and should be tracked for accurate cost accounting. This wrapper preserves that token usage even when the original exception doesn't have a <code>__dict__</code> (like built-in exceptions).</p> <p>Usage:</p> <p>Strategies use this internally to wrap exceptions that don't support attribute assignment:</p> <pre><code>try:\n    output = parse_response(response)\nexcept Exception as e:\n    if not hasattr(e, \"__dict__\"):\n        wrapped = TokenTrackingError(str(e), token_usage=tokens)\n        wrapped.__cause__ = e\n        raise wrapped from e\n    else:\n        e.__dict__[\"_failed_token_usage\"] = tokens\n        raise\n</code></pre> <p>Catching TokenTrackingError:</p> <pre><code>from async_batch_llm import TokenTrackingError\n\nfor item_result in result.results:\n    if not item_result.success:\n        # Token usage is preserved even for failed items\n        print(f\"Failed: {item_result.item_id}\")\n        print(f\"Tokens consumed: {item_result.token_usage}\")\n</code></pre>"},{"location":"API/#type-aliases","title":"Type Aliases","text":""},{"location":"API/#postprocessorfunc","title":"PostProcessorFunc","text":"<p>Callback function called after each successful item.</p> <pre><code>PostProcessorFunc = Callable[\n    [WorkItemResult[TOutput, TContext]],\n    Awaitable[None] | None\n]\n</code></pre> <p>Example:</p> <pre><code>async def save_result(result: WorkItemResult):\n    if result.success:\n        await database.save(result.item_id, result.output)\n\nprocessor = ParallelBatchProcessor(\n    config=config,\n    post_processor=save_result\n)\n</code></pre>"},{"location":"API/#progresscallbackfunc","title":"ProgressCallbackFunc","text":"<p>Callback function for progress updates.</p> <pre><code>ProgressCallbackFunc = Callable[\n    [int, int, str],  # (completed, total, current_item_id)\n    Awaitable[None] | None\n]\n</code></pre> <p>Example:</p> <pre><code>async def on_progress(completed: int, total: int, current_item: str):\n    print(f\"Progress: {completed}/{total} - {current_item}\")\n\nprocessor = ParallelBatchProcessor(\n    config=config,\n    progress_callback=on_progress\n)\n</code></pre>"},{"location":"API/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom async_batch_llm import (\n    ParallelBatchProcessor,\n    ProcessorConfig,\n    LLMWorkItem,\n    PydanticAIStrategy,\n    MetricsObserver,\n)\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass Summary(BaseModel):\n    title: str\n    summary: str\n\nasync def main():\n    # Configure processor\n    config = ProcessorConfig(\n        max_workers=10,\n        timeout_per_item=60.0,\n        max_queue_size=100,\n    )\n\n    # Create strategy\n    agent = Agent(\"openai:gpt-4\", result_type=Summary)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    # Add metrics\n    metrics = MetricsObserver()\n\n    # Create processor with context manager\n    async with ParallelBatchProcessor(\n        config=config,\n        observers=[metrics]\n    ) as processor:\n        # Add work items\n        for i in range(50):\n            work_item = LLMWorkItem(\n                item_id=f\"doc_{i}\",\n                strategy=strategy,\n                prompt=f\"Summarize document {i}...\",\n            )\n            await processor.add_work(work_item)\n\n        # Process all\n        result = await processor.process_all()\n\n        # Report results\n        print(f\"Completed: {result.succeeded}/{result.total_items}\")\n        print(f\"Tokens used: {result.total_input_tokens + result.total_output_tokens}\")\n\n        # Get metrics\n        metrics_data = await metrics.get_metrics()\n        print(f\"Average processing time: {metrics_data['avg_processing_time']:.2f}s\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"API/#see-also","title":"See Also","text":"<ul> <li>README.md - Getting started guide</li> <li>MIGRATION_V0_1.md - Migration guide from v0.0.x (strategy pattern)</li> <li>MIGRATION_V0_4.md - Migration guide to v0.4.0 (context managers)</li> <li>GEMINI_INTEGRATION.md - Detailed Gemini integration guide</li> <li>CHANGELOG.md - Version history</li> </ul>"},{"location":"GEMINI_INTEGRATION/","title":"Gemini API Integration Guide","text":"<p>Complete guide for using async-batch-llm with Google's Gemini API.</p>"},{"location":"GEMINI_INTEGRATION/#installation","title":"Installation","text":"<pre><code># Install async-batch-llm with Gemini support\npip install 'async-batch-llm[gemini]'\n# or\nuv add 'async-batch-llm[gemini]'\n</code></pre> <p>This installs:</p> <ul> <li><code>async-batch-llm</code> - Core batch processing framework</li> <li><code>google-genai</code> - Official Google Gemini SDK</li> <li><code>pydantic</code> - For response validation</li> </ul>"},{"location":"GEMINI_INTEGRATION/#setup","title":"Setup","text":""},{"location":"GEMINI_INTEGRATION/#1-get-api-key","title":"1. Get API Key","text":"<p>Get a free API key from Google AI Studio: https://aistudio.google.com/apikey</p>"},{"location":"GEMINI_INTEGRATION/#2-set-environment-variable","title":"2. Set Environment Variable","text":"<pre><code>export GOOGLE_API_KEY=your_api_key_here\n</code></pre> <p>Or in Python:</p> <pre><code>import os\nos.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"\n</code></pre> <p>Tip: <code>google-genai</code> still honors the older <code>GEMINI_API_KEY</code> variable for backward compatibility, but <code>GOOGLE_API_KEY</code> takes precedence when both are set. Set at least one.</p>"},{"location":"GEMINI_INTEGRATION/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code>from google import genai\n\nclient = genai.Client()\nresponse = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=\"Say hello!\"\n)\nprint(response.text)\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#usage-with-async-batch-llm","title":"Usage with async-batch-llm","text":"<p>async-batch-llm provides two built-in Gemini strategies:</p>"},{"location":"GEMINI_INTEGRATION/#1-geministrategy-simple-api-calls","title":"1. GeminiStrategy (Simple API Calls)","text":"<p>For direct Gemini API calls without caching:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom async_batch_llm.llm_strategies import GeminiStrategy\nfrom google import genai\nfrom pydantic import BaseModel\n\nclass SummaryOutput(BaseModel):\n    \"\"\"Structured output for summarization.\"\"\"\n    summary: str\n    key_points: list[str]\n\n# Create client\nclient = genai.Client(api_key=\"your-api-key\")\n\n# Create response parser\ndef parse_response(response) -&gt; SummaryOutput:\n    \"\"\"Parse Gemini response into your output model.\"\"\"\n    return SummaryOutput.model_validate_json(response.text)\n\n# Create strategy\nstrategy = GeminiStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n    config=genai.types.GenerateContentConfig(\n        temperature=0.7,\n        response_mime_type=\"application/json\",\n        response_schema=SummaryOutput,\n    ),\n)\n\n# Configure processor\nconfig = ProcessorConfig(max_workers=5, timeout_per_item=30.0)\n\n# Process items\nasync with ParallelBatchProcessor[str, SummaryOutput, None](config=config) as processor:\n    texts = [\"Text 1...\", \"Text 2...\", \"Text 3...\"]\n\n    for i, text in enumerate(texts):\n        await processor.add_work(\n            LLMWorkItem(\n                item_id=f\"text_{i}\",\n                strategy=strategy,\n                prompt=f\"Summarize: {text}\",\n            )\n        )\n\n    result = await processor.process_all()\n\n# Use results\nfor item in result.results:\n    if item.success:\n        print(f\"{item.item_id}: {item.output.summary}\")\n        print(f\"  Tokens: {item.token_usage['total_tokens']}\")\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#2-geminicachedstrategy-with-context-caching","title":"2. GeminiCachedStrategy (With Context Caching)","text":"<p>Perfect for RAG applications with large shared context:</p> <pre><code>from async_batch_llm.llm_strategies import GeminiCachedStrategy\nfrom google import genai\n\nclient = genai.Client(api_key=\"your-api-key\")\n\n# Define large context to cache (e.g., retrieved documents)\ncached_content = [\n    genai.types.Content(\n        role=\"user\",\n        parts=[\n            genai.types.Part(text=\"Large document or knowledge base to cache...\")\n        ]\n    )\n]\n\ndef parse_response(response) -&gt; str:\n    return response.text\n\n# Create cached strategy\nstrategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n    cached_content=cached_content,\n    cache_ttl_seconds=3600,             # Cache for 1 hour\n    cache_renewal_buffer_seconds=300,   # Renew 5 min before expiry (v0.2+)\n    auto_renew=True,\n)\n\n# Use in processor\nconfig = ProcessorConfig(max_workers=3, timeout_per_item=30.0)\n\nasync with ParallelBatchProcessor[str, str, None](config=config) as processor:\n    questions = [\n        \"What is the main topic?\",\n        \"What are the key findings?\",\n        \"What are the conclusions?\",\n    ]\n\n    for i, question in enumerate(questions):\n        await processor.add_work(\n            LLMWorkItem(\n                item_id=f\"question_{i}\",\n                strategy=strategy,\n                prompt=question,\n            )\n        )\n\n    result = await processor.process_all()\n\n# Cache lifecycle:\n# - Created on first use\n# - Renewed automatically when nearing expiry (auto_renew)\n# - Left active after processing (call delete_cache() to force removal)\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#advanced-features","title":"Advanced Features","text":""},{"location":"GEMINI_INTEGRATION/#progressive-temperature-on-retries","title":"Progressive Temperature on Retries","text":"<p>Create a custom strategy that adjusts temperature based on attempt:</p> <pre><code>from async_batch_llm.llm_strategies import LLMCallStrategy\n\nclass ProgressiveTempGeminiStrategy(LLMCallStrategy[SummaryOutput]):\n    \"\"\"Gemini strategy with progressive temperature.\"\"\"\n\n    def __init__(self, client: genai.Client, temps=[0.0, 0.5, 1.0]):\n        self.client = client\n        self.temps = temps\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[SummaryOutput, dict[str, int]]:\n        # Use higher temperature for retries\n        temp = self.temps[min(attempt - 1, len(self.temps) - 1)]\n\n        config = genai.types.GenerateContentConfig(\n            temperature=temp,\n            response_mime_type=\"application/json\",\n            response_schema=SummaryOutput,\n        )\n\n        response = await self.client.aio.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=prompt,\n            config=config,\n        )\n\n        output = SummaryOutput.model_validate_json(response.text)\n\n        usage = response.usage_metadata\n        tokens = {\n            \"input_tokens\": usage.prompt_token_count or 0,\n            \"output_tokens\": usage.candidates_token_count or 0,\n            \"total_tokens\": usage.total_token_count or 0,\n        }\n\n        return output, tokens\n\n# Use it\nstrategy = ProgressiveTempGeminiStrategy(client=client, temps=[0.0, 0.5, 1.0])\n</code></pre> <p>Why progressive temperature?</p> <ul> <li>Attempt 1 (temp=0.0): Deterministic, most likely to succeed</li> <li>Attempt 2 (temp=0.5): More creative if first attempt had validation errors</li> <li>Attempt 3 (temp=1.0): Maximum creativity as last resort</li> </ul>"},{"location":"GEMINI_INTEGRATION/#model-selection","title":"Model Selection","text":"<pre><code># Fast, experimental (free tier)\nmodel=\"gemini-2.0-flash-exp\"\n\n# Production-ready, fast\nmodel=\"gemini-2.5-flash-lite\"\n\n# Most capable, slower\nmodel=\"gemini-2.5-flash\"\n\n# With extended thinking\nmodel=\"gemini-2.5-pro\"\n</code></pre> <p>See: https://ai.google.dev/gemini-api/docs/models/gemini</p>"},{"location":"GEMINI_INTEGRATION/#generation-config-options","title":"Generation Config Options","text":"<pre><code>from google.genai.types import GenerateContentConfig\n\nconfig = GenerateContentConfig(\n    # Temperature: 0.0 (deterministic) to 1.0 (creative)\n    temperature=0.7,\n\n    # Nucleus sampling: Consider tokens with cumulative probability top_p\n    top_p=0.95,\n\n    # Top-k sampling: Consider only top k tokens\n    top_k=40,\n\n    # Maximum tokens in response\n    max_output_tokens=2048,\n\n    # Stop sequences\n    stop_sequences=[\"END\", \"STOP\"],\n\n    # Structured output\n    response_mime_type=\"application/json\",\n    response_schema=YourPydanticModel,\n\n    # System instruction\n    system_instruction=\"You are a helpful assistant...\",\n\n    # Safety settings\n    safety_settings=[\n        {\n            \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n        }\n    ],\n)\n</code></pre> <p>See: https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters</p>"},{"location":"GEMINI_INTEGRATION/#error-handling","title":"Error Handling","text":"<p>async-batch-llm includes <code>GeminiErrorClassifier</code> for Gemini-specific errors:</p> <pre><code>from async_batch_llm.classifiers import GeminiErrorClassifier\n\nprocessor = ParallelBatchProcessor(\n    config=config,\n    error_classifier=GeminiErrorClassifier(),  # Handles 429, 500, etc.\n)\n</code></pre> <p>The classifier automatically:</p> <ul> <li>Detects rate limit errors (429) as non-retryable</li> <li>Marks server errors (500) as retryable</li> <li>Detects timeout errors</li> <li>Handles validation errors</li> </ul>"},{"location":"GEMINI_INTEGRATION/#rate-limit-handling","title":"Rate Limit Handling","text":"<p>Gemini has rate limits. Configure automatic handling:</p> <pre><code>from async_batch_llm.core import RateLimitConfig\n\nconfig = ProcessorConfig(\n    max_workers=10,\n    timeout_per_item=30.0,\n    rate_limit=RateLimitConfig(\n        cooldown_seconds=60.0,  # Wait 60s after rate limit\n        slow_start_items=50,  # Gradually resume over 50 items\n        slow_start_initial_delay=2.0,  # 2s between items initially\n        slow_start_final_delay=0.1,  # 0.1s between items finally\n    )\n)\n</code></pre> <p>When rate limit (429) is detected:</p> <ol> <li>All workers pause</li> <li>Wait for cooldown period</li> <li>Resume with slow-start (gradual ramp-up)</li> <li>Automatically retry failed items</li> </ol>"},{"location":"GEMINI_INTEGRATION/#multimodal-inputs","title":"Multimodal Inputs","text":"<p>Process images with text:</p> <pre><code>from google.genai.types import Part, Content\nfrom async_batch_llm.llm_strategies import LLMCallStrategy\n\nclass GeminiVisionStrategy(LLMCallStrategy[str]):\n    \"\"\"Strategy for Gemini vision tasks.\"\"\"\n\n    def __init__(self, client: genai.Client, image_path: str):\n        self.client = client\n        self.image_path = image_path\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[str, dict[str, int]]:\n        # Read image\n        with open(self.image_path, \"rb\") as f:\n            image_bytes = f.read()\n\n        # Create multimodal content\n        contents = [\n            Content(\n                parts=[\n                    Part.from_bytes(data=image_bytes, mime_type=\"image/jpeg\"),\n                    Part.from_text(text=prompt)\n                ]\n            )\n        ]\n\n        response = await self.client.aio.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=contents,\n        )\n\n        usage = response.usage_metadata\n        tokens = {\n            \"input_tokens\": usage.prompt_token_count or 0,\n            \"output_tokens\": usage.candidates_token_count or 0,\n            \"total_tokens\": usage.total_token_count or 0,\n        }\n\n        return response.text, tokens\n\n# Use it\nstrategy = GeminiVisionStrategy(client=client, image_path=\"photo.jpg\")\n</code></pre> <p>See: https://ai.google.dev/gemini-api/docs/vision</p>"},{"location":"GEMINI_INTEGRATION/#token-usage-tracking","title":"Token Usage Tracking","text":"<pre><code># After processing\nresult = await processor.process_all()\n\nprint(f\"Total input tokens: {result.total_input_tokens}\")\nprint(f\"Total output tokens: {result.total_output_tokens}\")\nprint(f\"Total tokens: {result.total_input_tokens + result.total_output_tokens}\")\n\n# Per-item usage\nfor item in result.results:\n    if item.success:\n        tokens = item.token_usage\n        cost = tokens[\"input_tokens\"] * 0.00001 + tokens[\"output_tokens\"] * 0.00003\n        print(f\"{item.item_id}: ${cost:.6f}\")\n</code></pre> <p>Pricing: https://ai.google.dev/pricing</p>"},{"location":"GEMINI_INTEGRATION/#complete-example","title":"Complete Example","text":"<p>See <code>examples/example_gemini_direct.py</code> for a complete working example.</p> <p>Run it:</p> <pre><code>export GOOGLE_API_KEY=your_key_here\nuv run python examples/example_gemini_direct.py\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#comparison-pydanticai-vs-direct-strategies","title":"Comparison: PydanticAI vs Direct Strategies","text":""},{"location":"GEMINI_INTEGRATION/#with-pydanticai","title":"With PydanticAI","text":"<pre><code>from async_batch_llm import PydanticAIStrategy\nfrom pydantic_ai import Agent\n\nagent = Agent('gemini-2.5-flash', result_type=SummaryOutput)\nstrategy = PydanticAIStrategy(agent=agent)\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,\n    prompt=\"Summarize this text...\"\n)\n</code></pre> <p>Pros: Simpler API, less code Cons: Less control over API parameters, extra dependency</p>"},{"location":"GEMINI_INTEGRATION/#direct-gemini-strategy","title":"Direct Gemini Strategy","text":"<pre><code>from async_batch_llm.llm_strategies import GeminiStrategy\n\nstrategy = GeminiStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n    config=config,  # Full control\n)\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,\n    prompt=\"Summarize this text...\"\n)\n</code></pre> <p>Pros: Full control, no PydanticAI dependency, custom configurations Cons: More code, manual parsing</p>"},{"location":"GEMINI_INTEGRATION/#best-practices","title":"Best Practices","text":"<ol> <li>Use structured output: Set <code>response_schema</code> for reliable parsing</li> <li>Implement progressive temperature: Start low (0.0), increase on retries</li> <li>Set reasonable timeouts: 30s for simple, 120s for complex queries</li> <li>Handle errors gracefully: Use <code>GeminiErrorClassifier</code> for Gemini errors</li> <li>Monitor token usage: Track costs using <code>item.token_usage</code></li> <li>Respect rate limits: Configure <code>rate_limit</code> settings appropriately</li> <li>Choose right model: Use flash for speed, pro for quality</li> <li>Use caching for RAG: <code>GeminiCachedStrategy</code> saves money on repeated context</li> </ol>"},{"location":"GEMINI_INTEGRATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"GEMINI_INTEGRATION/#api-key-not-found","title":"API Key Not Found","text":"<pre><code>Error: GOOGLE_API_KEY environment variable not set\n</code></pre> <p>Fix: Export your API key before running:</p> <pre><code>export GOOGLE_API_KEY=your_key_here\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#rate-limit-errors-429","title":"Rate Limit Errors (429)","text":"<pre><code>google.genai.errors.ClientError: 429 Resource exhausted\n</code></pre> <p>Fix: Reduce <code>max_workers</code> or configure rate limiting:</p> <pre><code>config = ProcessorConfig(\n    max_workers=3,  # Lower concurrency\n    rate_limit=RateLimitConfig(cooldown_seconds=60.0)\n)\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#validation-errors","title":"Validation Errors","text":"<pre><code>pydantic.ValidationError: response doesn't match schema\n</code></pre> <p>Fix:</p> <ol> <li>Check your Pydantic model matches expected output</li> <li>Use progressive temperature strategy (increases temp on retries)</li> <li>Add examples in your prompt</li> </ol>"},{"location":"GEMINI_INTEGRATION/#timeout-errors","title":"Timeout Errors","text":"<pre><code>asyncio.TimeoutError\n</code></pre> <p>Fix: Increase timeout:</p> <pre><code>config = ProcessorConfig(timeout_per_item=60.0)  # 60 seconds\n</code></pre>"},{"location":"GEMINI_INTEGRATION/#advanced-smart-retry-with-on_error","title":"Advanced: Smart Retry with on_error","text":"<p>Use the <code>on_error</code> callback to handle Gemini-specific errors intelligently:</p>"},{"location":"GEMINI_INTEGRATION/#smart-model-escalation-for-gemini","title":"Smart Model Escalation for Gemini","text":"<p>Only escalate to expensive Gemini models on validation errors, not network/rate limit errors:</p> <pre><code>from async_batch_llm.llm_strategies import LLMCallStrategy\nfrom async_batch_llm import TokenUsage\nfrom pydantic import ValidationError\nfrom google import genai\n\nclass SmartGeminiStrategy(LLMCallStrategy[PersonData]):\n    \"\"\"Smart model escalation for Gemini API.\"\"\"\n\n    MODELS = [\n        \"gemini-2.5-flash-lite\",  # Cheapest, fastest\n        \"gemini-2.5-flash\",       # Production-ready\n        \"gemini-2.5-pro\",         # Most capable\n    ]\n\n    def __init__(self, client: genai.Client):\n        self.client = client\n        self.validation_failures = 0  # Track quality issues only\n        self.safety_blocks = 0        # Track Gemini safety blocks\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        \"\"\"Track Gemini-specific error types.\"\"\"\n        if isinstance(exception, ValidationError):\n            self.validation_failures += 1\n        elif \"SAFETY\" in str(exception) or \"BLOCKED\" in str(exception):\n            self.safety_blocks += 1\n            # Note: Could adjust safety_settings on retry\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[PersonData, TokenUsage]:\n        # Select model based on validation failures (not total attempts)\n        model_index = min(self.validation_failures, len(self.MODELS) - 1)\n        model = self.MODELS[model_index]\n\n        # Adjust safety settings if we've hit safety blocks\n        config = genai.types.GenerateContentConfig(\n            temperature=0.7,\n            response_mime_type=\"application/json\",\n            response_schema=PersonData,\n        )\n\n        if self.safety_blocks &gt; 0:\n            # Could make safety_settings more permissive\n            # config.safety_settings = [...]\n            pass\n\n        response = await self.client.aio.models.generate_content(\n            model=model,\n            contents=prompt,\n            config=config,\n        )\n\n        output = PersonData.model_validate_json(response.text)\n        usage = response.usage_metadata\n        tokens: TokenUsage = {\n            \"input_tokens\": usage.prompt_token_count or 0,\n            \"output_tokens\": usage.candidates_token_count or 0,\n            \"total_tokens\": usage.total_token_count or 0,\n        }\n\n        return output, tokens\n</code></pre> <p>Cost Savings:</p> <ul> <li>Validation error \u2192 Escalate to gemini-2.5-pro (quality issue)</li> <li>Network error \u2192 Retry with gemini-2.5-flash-lite (transient issue)</li> <li>Rate limit error \u2192 Retry with gemini-2.5-flash-lite (API quota)</li> <li>Safety block \u2192 Retry with same model, adjusted safety settings</li> <li>Result: 60-80% cost reduction vs. always using gemini-2.5-pro</li> </ul>"},{"location":"GEMINI_INTEGRATION/#smart-retry-prompts-for-gemini","title":"Smart Retry Prompts for Gemini","text":"<p>Build targeted retry prompts based on Gemini validation errors:</p> <pre><code>class SmartRetryGeminiStrategy(LLMCallStrategy[PersonData]):\n    \"\"\"Tell Gemini exactly what failed in previous attempt.\"\"\"\n\n    def __init__(self, client: genai.Client):\n        self.client = client\n        self.last_error = None\n        self.last_response = None\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        \"\"\"Track validation errors for smart retry.\"\"\"\n        if isinstance(exception, ValidationError):\n            self.last_error = exception\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[PersonData, TokenUsage]:\n        if attempt == 1:\n            final_prompt = prompt\n        else:\n            # Build focused retry prompt\n            final_prompt = self._create_retry_prompt(prompt)\n\n        config = genai.types.GenerateContentConfig(\n            temperature=0.7,\n            response_mime_type=\"application/json\",\n            response_schema=PersonData,\n        )\n\n        response = await self.client.aio.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=final_prompt,\n            config=config,\n        )\n\n        try:\n            output = PersonData.model_validate_json(response.text)\n            usage = response.usage_metadata\n            tokens: TokenUsage = {\n                \"input_tokens\": usage.prompt_token_count or 0,\n                \"output_tokens\": usage.candidates_token_count or 0,\n                \"total_tokens\": usage.total_token_count or 0,\n            }\n            return output, tokens\n        except ValidationError as e:\n            self.last_response = response.text\n            raise  # Framework calls on_error, then retries\n\n    def _create_retry_prompt(self, original_prompt: str) -&gt; str:\n        \"\"\"Create targeted retry prompt with field-specific feedback.\"\"\"\n        if not self.last_error:\n            return original_prompt\n\n        # Parse which fields succeeded vs failed\n        failed_fields = []\n        for error in self.last_error.errors():\n            field = \".\".join(str(loc) for loc in error[\"loc\"])\n            msg = error[\"msg\"]\n            failed_fields.append(f\"  - {field}: {msg}\")\n\n        retry_prompt = f\"\"\"RETRY REQUEST: The previous response had validation errors.\n\nORIGINAL REQUEST:\n{original_prompt}\n\nVALIDATION ERRORS TO FIX:\n{chr(10).join(failed_fields)}\n\nPlease provide a complete, valid JSON response that fixes these specific validation errors.\nEnsure all fields match the required schema exactly.\"\"\"\n\n        return retry_prompt\n</code></pre> <p>Benefits:</p> <ul> <li>Gemini knows exactly what went wrong</li> <li>Focused on fixing specific fields</li> <li>Higher success rate on retries</li> <li>Lower token usage (shorter prompts)</li> </ul> <p>Complete Examples:</p> <ul> <li><code>examples/example_smart_model_escalation.py</code> - Full implementation</li> <li><code>examples/example_gemini_smart_retry.py</code> - Complete smart retry example</li> </ul>"},{"location":"GEMINI_INTEGRATION/#resources","title":"Resources","text":"<ul> <li>Gemini API Docs: https://ai.google.dev/gemini-api/docs</li> <li>Python SDK: https://googleapis.github.io/python-genai/</li> <li>Pricing: https://ai.google.dev/pricing</li> <li>Models: https://ai.google.dev/gemini-api/docs/models/gemini</li> <li>Get API Key: https://aistudio.google.com/apikey</li> <li>Quickstart: https://ai.google.dev/gemini-api/docs/quickstart</li> <li>async-batch-llm API Docs: docs/API.md</li> </ul>"},{"location":"GEMINI_INTEGRATION/#support","title":"Support","text":"<p>For issues with:</p> <ul> <li>batch-llm: https://github.com/geoff-davis/async-batch-llm/issues</li> <li>Gemini API: https://developers.google.com/support</li> <li>google-genai SDK: https://github.com/googleapis/python-genai/issues</li> </ul>"},{"location":"MIGRATION_V0_4/","title":"Migration Guide: v0.3.x \u2192 v0.4.0","text":"<p>This guide helps you migrate from async-batch-llm v0.3.x to v0.4.0, which introduces strategy lifecycle management with context managers.</p>"},{"location":"MIGRATION_V0_4/#summary-of-changes","title":"Summary of Changes","text":"<p>v0.4.0 adds strategy lifecycle management using Python's context manager pattern (<code>async with</code>). The main breaking change is that per-item cleanup has been removed in favor of processor-level cleanup.</p>"},{"location":"MIGRATION_V0_4/#breaking-changes","title":"Breaking Changes","text":""},{"location":"MIGRATION_V0_4/#1-per-item-cleanup-removed","title":"1. Per-Item Cleanup Removed","text":"<p>What Changed:</p> <ul> <li>v0.3.x: <code>strategy.cleanup()</code> was called after each work item completed</li> <li>v0.4.0: <code>strategy.cleanup()</code> is only called once when exiting the context manager</li> </ul> <p>Why This Matters:</p> <p>In v0.3.x, if you had expensive resources in your strategy (like database connections or caches), they were created in <code>prepare()</code> and destroyed in <code>cleanup()</code> for EVERY work item. This was inefficient for shared resources.</p> <p>In v0.4.0, resources are created once in <code>prepare()</code> and destroyed once in <code>cleanup()</code> when the processor exits, which is more efficient for batch processing.</p> <p>Migration Required If:</p> <p>You rely on <code>cleanup()</code> being called after each item to release resources or save state.</p> <p>How to Migrate:</p>"},{"location":"MIGRATION_V0_4/#option-1-use-context-manager-recommended","title":"Option 1: Use Context Manager (Recommended)","text":"<p>Wrap your processor in <code>async with</code> to enable automatic cleanup on exit:</p> <pre><code># v0.3.x - cleanup called after each item\nprocessor = ParallelBatchProcessor(config=config)\nawait processor.add_work(LLMWorkItem(...))\nresult = await processor.process_all()\n# cleanup() was called N times (once per item)\n\n# v0.4.0 - cleanup called once on exit\nasync with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(LLMWorkItem(...))\n    result = await processor.process_all()\n    # All work completed\n# cleanup() called here (once total)\n</code></pre>"},{"location":"MIGRATION_V0_4/#option-2-keep-backward-compatible-behavior","title":"Option 2: Keep Backward Compatible Behavior","text":"<p>If you don't use the context manager, cleanup is never called (backward compatible):</p> <pre><code># v0.4.0 without context manager - no cleanup\nprocessor = ParallelBatchProcessor(config=config)\nawait processor.add_work(LLMWorkItem(...))\nresult = await processor.process_all()\n# No cleanup() called - same as v0.2.0 behavior\n</code></pre> <p>This preserves backward compatibility but means resources won't be automatically cleaned up.</p>"},{"location":"MIGRATION_V0_4/#option-3-manual-cleanup-not-recommended","title":"Option 3: Manual Cleanup (Not Recommended)","text":"<p>You can manually call cleanup if needed, but this is discouraged:</p> <pre><code>processor = ParallelBatchProcessor(config=config)\ntry:\n    await processor.add_work(LLMWorkItem(...))\n    result = await processor.process_all()\nfinally:\n    # Manually cleanup strategies\n    for strategy in processor._prepared_strategies:\n        await strategy.cleanup()\n</code></pre> <p>Best Practice: Use the context manager pattern (Option 1) for automatic resource management.</p>"},{"location":"MIGRATION_V0_4/#2-production-caches-should-not-be-cleaned-up","title":"2. Production Caches Should Not Be Cleaned Up","text":"<p>What Changed:</p> <p>If you're using strategies with long-lived resources (like production caches intended to persist across batches), you need to make <code>cleanup()</code> a no-op.</p> <p>Example:</p> <pre><code>class ProdCachedStrategy(GeminiCachedStrategy):\n    \"\"\"Production strategy with persistent cache.\"\"\"\n\n    async def cleanup(self) -&gt; None:\n        \"\"\"\n        Don't delete cache - it should persist across batches.\n\n        Override parent's cleanup() to prevent cache deletion.\n        \"\"\"\n        # Do nothing - cache persists for cost optimization\n        pass\n</code></pre> <p>Why: In v0.3.x, cleanup was called per-item so you couldn't have persistent caches. In v0.4.0, cleanup is called once at the end, so you need to explicitly prevent cache deletion if you want it to persist.</p>"},{"location":"MIGRATION_V0_4/#new-features","title":"New Features","text":""},{"location":"MIGRATION_V0_4/#1-runtimeerror-when-adding-work-after-processing-starts","title":"1. RuntimeError When Adding Work After Processing Starts","text":"<p>What Changed:</p> <p>Calling <code>add_work()</code> after <code>process_all()</code> has started now raises <code>RuntimeError</code>.</p> <p>Why: This prevents race conditions and ensures all work is queued before processing begins.</p> <p>Example:</p> <pre><code>async with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(LLMWorkItem(item_id=\"1\", ...))\n\n    # Start processing\n    result = await processor.process_all()\n\n    # This now raises RuntimeError\n    try:\n        await processor.add_work(LLMWorkItem(item_id=\"2\", ...))\n    except RuntimeError as e:\n        print(f\"Cannot add work after processing starts: {e}\")\n</code></pre> <p>Migration: If you need to process multiple batches, create a new processor instance for each batch:</p> <pre><code># Process first batch\nasync with ParallelBatchProcessor(config=config) as processor1:\n    await processor1.add_work(LLMWorkItem(item_id=\"1\", ...))\n    result1 = await processor1.process_all()\n\n# Process second batch with new processor\nasync with ParallelBatchProcessor(config=config) as processor2:\n    await processor2.add_work(LLMWorkItem(item_id=\"2\", ...))\n    result2 = await processor2.process_all()\n</code></pre>"},{"location":"MIGRATION_V0_4/#2-shared-strategy-instances","title":"2. Shared Strategy Instances","text":"<p>What Changed:</p> <p>Shared strategy instances are now properly supported - they're prepared once and cleaned up once.</p> <p>Example:</p> <pre><code># Create shared strategy for cost optimization\nshared_strategy = GeminiCachedStrategy(\n    model=\"gemini-2.0-flash\",\n    system_instruction=\"...\",  # Expensive to cache\n)\n\nasync with ParallelBatchProcessor(config=config) as processor:\n    # Use same strategy for all items\n    for i in range(100):\n        await processor.add_work(\n            LLMWorkItem(item_id=f\"item_{i}\", strategy=shared_strategy, prompt=f\"...\")\n        )\n\n    result = await processor.process_all()\n    # shared_strategy.prepare() called once\n    # shared_strategy.execute() called 100 times\n# shared_strategy.cleanup() called once\n</code></pre> <p>Benefit: Sharing strategies saves memory and avoids duplicate cache creation costs.</p>"},{"location":"MIGRATION_V0_4/#non-breaking-changes","title":"Non-Breaking Changes","text":""},{"location":"MIGRATION_V0_4/#strategy-without-prepare-or-cleanup","title":"Strategy Without prepare() or cleanup()","text":"<p>Strategies don't need to implement <code>prepare()</code> or <code>cleanup()</code> - they're optional:</p> <pre><code>class SimpleStrategy(LLMCallStrategy[str]):\n    \"\"\"Minimal strategy without lifecycle methods.\"\"\"\n\n    async def execute(self, prompt, attempt, timeout, state=None):\n        # Just do the work\n        return output, tokens\n\n# Works fine - no prepare() or cleanup() needed\nstrategy = SimpleStrategy()\nasync with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(LLMWorkItem(strategy=strategy, ...))\n    result = await processor.process_all()\n</code></pre>"},{"location":"MIGRATION_V0_4/#migration-checklist","title":"Migration Checklist","text":"<ul> <li>[ ] Wrap all <code>ParallelBatchProcessor</code> usage in <code>async with</code> context managers</li> <li>[ ] Review custom strategies with <code>cleanup()</code> methods</li> <li>[ ] For temporary resources: Keep cleanup implementation (will be called once on exit)</li> <li>[ ] For persistent caches: Override <code>cleanup()</code> to be a no-op</li> <li>[ ] Update code that calls <code>add_work()</code> after <code>process_all()</code></li> <li>[ ] Create new processor instances for additional batches</li> <li>[ ] Run tests to verify cleanup behavior is correct</li> <li>[ ] Update documentation/examples to use context manager pattern</li> </ul>"},{"location":"MIGRATION_V0_4/#testing-your-migration","title":"Testing Your Migration","text":"<p>Run these tests to verify your migration:</p> <pre><code># Test 1: Verify cleanup is called with context manager\nstrategy = YourStrategy()\nasync with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(LLMWorkItem(strategy=strategy, ...))\n    result = await processor.process_all()\n    assert not strategy.cleanup_called, \"Cleanup not called yet\"\n# Assert cleanup was called after exiting context\nassert strategy.cleanup_called, \"Cleanup should be called on exit\"\n\n# Test 2: Verify backward compatibility without context manager\nstrategy = YourStrategy()\nprocessor = ParallelBatchProcessor(config=config)\nawait processor.add_work(LLMWorkItem(strategy=strategy, ...))\nresult = await processor.process_all()\nassert not strategy.cleanup_called, \"Cleanup not called without context manager\"\n\n# Test 3: Verify RuntimeError on late add_work()\nasync with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(LLMWorkItem(...))\n    await processor.process_all()\n\n    try:\n        await processor.add_work(LLMWorkItem(...))\n        assert False, \"Should have raised RuntimeError\"\n    except RuntimeError:\n        pass  # Expected\n</code></pre>"},{"location":"MIGRATION_V0_4/#need-help","title":"Need Help?","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check the CHANGELOG.md for detailed changes</li> <li>Review test_strategy_lifecycle.py for examples</li> <li>File an issue at https://github.com/geoff-davis/async-batch-llm/issues</li> </ol>"},{"location":"MIGRATION_V0_4/#benefits-of-v040","title":"Benefits of v0.4.0","text":"<p>After migration, you get:</p> <ol> <li>Better resource management - Cleanup happens at the right time (once per batch)</li> <li>Cost optimization - Shared strategies with persistent caches work correctly</li> <li>Clear lifecycle - Prepare on first use, cleanup on exit (Pythonic context managers)</li> <li>Fail-fast - Runtime errors prevent invalid usage patterns</li> <li>Backward compatible - Existing code without context managers still works</li> </ol>"},{"location":"contributing/","title":"Contributing to async-batch-llm","text":"<p>Thank you for considering contributing to async-batch-llm!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-and-install","title":"1. Clone and Install","text":"<pre><code>git clone https://github.com/geoff-davis/async-batch-llm.git\ncd async-batch-llm\n\n# Create virtual environment and install dependencies\nuv venv\nuv sync --all-extras\n</code></pre>"},{"location":"contributing/#2-install-pre-commit-hooks","title":"2. Install Pre-commit Hooks","text":"<pre><code>uv run pre-commit install\n</code></pre> <p>This will automatically run code quality checks before each commit.</p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_basic.py -v\n\n# Run with coverage\nuv run pytest --cov=async_batch_llm --cov-report=html\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<p>Always run quality checks before committing:</p> <pre><code># Format code\nuv run ruff format src/ tests/ examples/\n\n# Lint and auto-fix issues\nuv run ruff check src/ tests/ examples/ --fix\n\n# Verify linting passes\nuv run ruff check src/ tests/ examples/\n\n# Type check\nuv run mypy src/async_batch_llm/ --ignore-missing-imports\n\n# Or run all checks at once\nmake ci\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Build and preview documentation:</p> <pre><code># Install docs dependencies\nuv sync --extra docs\n\n# Serve docs locally\nuv run mkdocs serve\n\n# Build docs\nuv run mkdocs build\n</code></pre> <p>Then visit http://localhost:8000</p>"},{"location":"contributing/#markdown-linting","title":"Markdown Linting","text":"<pre><code># Lint markdown files\nnpx markdownlint-cli2 \"README.md\" \"docs/**/*.md\" \"CLAUDE.md\"\n\n# Auto-fix markdown issues\nnpx markdownlint-cli2 \"README.md\" \"docs/**/*.md\" \"CLAUDE.md\" --fix\n\n# Or use make target\nmake markdown-lint-fix\n</code></pre>"},{"location":"contributing/#pre-commit-checklist","title":"Pre-Commit Checklist","text":"<p>Before committing, ensure:</p> <ol> <li>\u2705 All tests pass: <code>uv run pytest</code></li> <li>\u2705 Linting passes: <code>uv run ruff check src/ tests/</code></li> <li>\u2705 Type checking passes: <code>uv run mypy src/async_batch_llm/</code></li> <li>\u2705 Markdown is clean: <code>make markdown-lint</code></li> </ol> <p>Or run everything at once:</p> <pre><code>make ci\n</code></pre>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ol> <li>Create a feature branch: <code>git checkout -b feature/your-feature</code></li> <li>Write tests: Add tests for new functionality</li> <li>Update docs: Update relevant documentation</li> <li>Run quality checks: Ensure all checks pass</li> <li>Write clear commit messages: Explain the \"why\" not just \"what\"</li> <li>Open PR: Provide a clear description of changes</li> </ol>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>async-batch-llm/\n\u251c\u2500\u2500 src/async_batch_llm/          # Main package\n\u2502   \u251c\u2500\u2500 base.py             # Core data models\n\u2502   \u251c\u2500\u2500 parallel.py         # Main processor\n\u2502   \u251c\u2500\u2500 llm_strategies/     # Strategy implementations\n\u2502   \u251c\u2500\u2500 observers/          # Observer implementations\n\u2502   \u2514\u2500\u2500 testing/            # Testing utilities\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 examples/               # Example scripts\n\u251c\u2500\u2500 docs/                   # Documentation\n\u2514\u2500\u2500 CLAUDE.md              # AI assistant context\n</code></pre>"},{"location":"contributing/#adding-new-strategies","title":"Adding New Strategies","text":"<p>To add a new LLM provider strategy:</p> <ol> <li>Create strategy in <code>src/async_batch_llm/llm_strategies/</code></li> <li>Implement <code>LLMCallStrategy</code> protocol</li> <li>Add tests in <code>tests/</code></li> <li>Add example in <code>examples/</code></li> <li>Update documentation in <code>docs/examples/custom-strategies.md</code></li> </ol> <p>Example:</p> <pre><code>from async_batch_llm import LLMCallStrategy\n\nclass MyProviderStrategy(LLMCallStrategy[str]):\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Your implementation\n        return output, tokens\n</code></pre>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue</li> <li>Start a discussion</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with async-batch-llm.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install async-batch-llm with the extras you need:</p> <pre><code># Basic installation\npip install async-batch-llm\n\n# With PydanticAI support (recommended for structured output)\npip install 'async-batch-llm[pydantic-ai]'\n\n# With Google Gemini support\npip install 'async-batch-llm[gemini]'\n\n# With everything\npip install 'async-batch-llm[all]'\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#1-strategy-pattern","title":"1. Strategy Pattern","text":"<p>async-batch-llm uses a strategy pattern to support any LLM provider. A strategy encapsulates:</p> <ul> <li>How to call the LLM</li> <li>How to handle errors</li> <li>How to manage resources (e.g., caches)</li> </ul> <pre><code>from async_batch_llm import LLMCallStrategy\n\nclass MyCustomStrategy(LLMCallStrategy[str]):\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Call your LLM here\n        response = await my_llm.generate(prompt)\n        tokens = {\"input_tokens\": 100, \"output_tokens\": 50, \"total_tokens\": 150}\n        return response, tokens\n</code></pre>"},{"location":"getting-started/#2-work-items","title":"2. Work Items","text":"<p>Each task is represented by an <code>LLMWorkItem</code>:</p> <pre><code>from async_batch_llm import LLMWorkItem\n\nwork_item = LLMWorkItem(\n    item_id=\"unique-id\",\n    strategy=my_strategy,\n    prompt=\"Your prompt here\",\n    context={\"metadata\": \"optional\"}\n)\n</code></pre>"},{"location":"getting-started/#3-parallel-processing","title":"3. Parallel Processing","text":"<p>The <code>ParallelBatchProcessor</code> manages parallel execution:</p> <pre><code>from async_batch_llm import ParallelBatchProcessor, ProcessorConfig\n\nconfig = ProcessorConfig(\n    max_workers=5,\n    timeout_per_item=30.0,\n)\n\nasync with ParallelBatchProcessor(config=config) as processor:\n    await processor.add_work(work_item)\n    result = await processor.process_all()\n</code></pre>"},{"location":"getting-started/#built-in-strategies","title":"Built-in Strategies","text":""},{"location":"getting-started/#pydanticai-strategy","title":"PydanticAI Strategy","text":"<p>For structured output with validation:</p> <pre><code>from async_batch_llm import PydanticAIStrategy\nfrom pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass Output(BaseModel):\n    field1: str\n    field2: int\n\nagent = Agent(\"gemini-2.5-flash\", result_type=Output)\nstrategy = PydanticAIStrategy(agent=agent)\n</code></pre>"},{"location":"getting-started/#gemini-strategy","title":"Gemini Strategy","text":"<p>Direct Gemini API calls:</p> <pre><code>from async_batch_llm.llm_strategies.gemini import GeminiStrategy\nfrom google import genai\n\nclient = genai.Client(api_key=\"your-key\")\nstrategy = GeminiStrategy(\n    client=client,\n    model=\"gemini-2.5-flash\",\n    output_type=str\n)\n</code></pre>"},{"location":"getting-started/#gemini-cached-strategy","title":"Gemini Cached Strategy","text":"<p>With context caching for repeated prompts:</p> <pre><code>from async_batch_llm.llm_strategies.gemini import GeminiCachedStrategy\n\nstrategy = GeminiCachedStrategy(\n    client=client,\n    model=\"gemini-2.5-flash\",\n    system_instruction=\"Your RAG context here...\",\n    output_type=str\n)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Examples - See more usage examples</li> <li>Custom Strategies - Build your own strategies</li> <li>Advanced Patterns - Learn advanced techniques</li> </ul>"},{"location":"api/core/","title":"Core API Reference","text":""},{"location":"api/core/#parallelbatchprocessor","title":"ParallelBatchProcessor","text":""},{"location":"api/core/#async_batch_llm.ParallelBatchProcessor","title":"async_batch_llm.ParallelBatchProcessor","text":"<pre><code>ParallelBatchProcessor(max_workers: int | None = None, post_processor: PostProcessorFunc[TOutput, TContext] | None = None, timeout_per_item: float | None = None, rate_limit_cooldown: float | None = None, config: ProcessorConfig | None = None, error_classifier: ErrorClassifier | None = None, rate_limit_strategy: RateLimitStrategy | None = None, middlewares: list[Middleware[TInput, TOutput, TContext]] | None = None, observers: list[ProcessorObserver] | None = None, progress_callback: ProgressCallbackFunc | None = None)\n</code></pre> <p>               Bases: <code>BatchProcessor[TInput, TOutput, TContext]</code>, <code>Generic[TInput, TOutput, TContext]</code></p> <p>Batch processor that executes items in parallel as individual agent calls.</p> <p>This refactored version uses: - Pluggable error classification (provider-agnostic) - Pluggable rate limit strategies - Middleware pipeline for extensibility - Observer pattern for monitoring - Configuration objects for easier setup</p> <p>Initialize the parallel batch processor.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int | None</code> <p>Maximum concurrent workers (deprecated, use config)</p> <code>None</code> <code>post_processor</code> <code>PostProcessorFunc[TOutput, TContext] | None</code> <p>Optional async function called after each successful item</p> <code>None</code> <code>timeout_per_item</code> <code>float | None</code> <p>Timeout per item in seconds (deprecated, use config)</p> <code>None</code> <code>rate_limit_cooldown</code> <code>float | None</code> <p>Cooldown duration (deprecated, use config)</p> <code>None</code> <code>config</code> <code>ProcessorConfig | None</code> <p>Processor configuration object (recommended)</p> <code>None</code> <code>error_classifier</code> <code>ErrorClassifier | None</code> <p>Strategy for classifying errors (default: DefaultErrorClassifier)</p> <code>None</code> <code>rate_limit_strategy</code> <code>RateLimitStrategy | None</code> <p>Strategy for handling rate limits</p> <code>None</code> <code>middlewares</code> <code>list[Middleware[TInput, TOutput, TContext]] | None</code> <p>List of middleware to apply</p> <code>None</code> <code>observers</code> <code>list[ProcessorObserver] | None</code> <p>List of observers for events</p> <code>None</code> <code>progress_callback</code> <code>ProgressCallbackFunc | None</code> <p>Optional callback(completed, total, current_item_id) for progress updates</p> <code>None</code> Source code in <code>src/async_batch_llm/parallel.py</code> <pre><code>def __init__(\n    self,\n    max_workers: int | None = None,\n    post_processor: PostProcessorFunc[TOutput, TContext] | None = None,\n    timeout_per_item: float | None = None,\n    rate_limit_cooldown: float | None = None,  # Deprecated, use config\n    # New parameters\n    config: ProcessorConfig | None = None,\n    error_classifier: ErrorClassifier | None = None,\n    rate_limit_strategy: RateLimitStrategy | None = None,\n    middlewares: list[Middleware[TInput, TOutput, TContext]] | None = None,\n    observers: list[ProcessorObserver] | None = None,\n    progress_callback: \"ProgressCallbackFunc | None\" = None,\n):\n    \"\"\"\n    Initialize the parallel batch processor.\n\n    Args:\n        max_workers: Maximum concurrent workers (deprecated, use config)\n        post_processor: Optional async function called after each successful item\n        timeout_per_item: Timeout per item in seconds (deprecated, use config)\n        rate_limit_cooldown: Cooldown duration (deprecated, use config)\n        config: Processor configuration object (recommended)\n        error_classifier: Strategy for classifying errors (default: DefaultErrorClassifier)\n        rate_limit_strategy: Strategy for handling rate limits\n        middlewares: List of middleware to apply\n        observers: List of observers for events\n        progress_callback: Optional callback(completed, total, current_item_id) for progress updates\n    \"\"\"\n    import warnings\n\n    # Emit deprecation warnings for legacy parameters\n    if max_workers is not None:\n        warnings.warn(\n            \"The 'max_workers' parameter is deprecated. \"\n            \"Use ProcessorConfig(max_workers=...) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if timeout_per_item is not None:\n        warnings.warn(\n            \"The 'timeout_per_item' parameter is deprecated. \"\n            \"Use ProcessorConfig(timeout_per_item=...) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if rate_limit_cooldown is not None:\n        warnings.warn(\n            \"The 'rate_limit_cooldown' parameter is deprecated. \"\n            \"Use ProcessorConfig(rate_limit=RateLimitConfig(cooldown_seconds=...)) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    # Handle backward compatibility\n    if config is None:\n        from .core import RateLimitConfig\n\n        config = ProcessorConfig(\n            max_workers=max_workers or 5,\n            timeout_per_item=timeout_per_item or 120.0,\n            rate_limit=RateLimitConfig(cooldown_seconds=rate_limit_cooldown or 300.0),\n        )\n    else:\n        # Override config with explicit parameters if provided\n        if max_workers is not None:\n            config.max_workers = max_workers\n        if timeout_per_item is not None:\n            config.timeout_per_item = timeout_per_item\n        if rate_limit_cooldown is not None:\n            config.rate_limit.cooldown_seconds = rate_limit_cooldown\n\n    config.validate()\n\n    super().__init__(\n        config.max_workers,\n        post_processor,\n        max_queue_size=config.max_queue_size,\n        progress_callback=progress_callback,\n        progress_callback_timeout=config.progress_callback_timeout,\n    )\n    self.config = config\n\n    # Set up strategies\n    self.error_classifier = error_classifier or DefaultErrorClassifier()\n    self.rate_limit_strategy = rate_limit_strategy or ExponentialBackoffStrategy(\n        initial_cooldown=config.rate_limit.cooldown_seconds,\n        backoff_multiplier=config.rate_limit.backoff_multiplier,\n        slow_start_items=config.rate_limit.slow_start_items,\n        slow_start_initial_delay=config.rate_limit.slow_start_initial_delay,\n        slow_start_final_delay=config.rate_limit.slow_start_final_delay,\n    )\n\n    # Set up middleware and observers\n    self.middlewares = middlewares or []\n    self.observers = observers or []\n\n    # Rate limit coordination\n    self._rate_limit_event = asyncio.Event()\n    self._rate_limit_event.set()  # Start in \"not paused\" state\n    self._in_cooldown = False\n    self._cooldown_generation = 0  # Track which cooldown cycle we're in (fixes race condition)\n    self._cooldown_complete_generation = 0\n    self._current_generation_event: asyncio.Event = asyncio.Event()\n    self._current_generation_event.set()\n    self._items_since_resume = 0\n    self._slow_start_active = False\n    self._consecutive_rate_limits = 0\n\n    # Thread safety locks\n    self._rate_limit_lock = asyncio.Lock()\n    self._stats_lock = asyncio.Lock()\n    self._results_lock = asyncio.Lock()\n\n    # Strategy lifecycle management (v0.2.0)\n    # Track which strategy instances have been prepared to avoid duplicate prepare() calls\n    self._prepared_strategies: weakref.WeakSet[LLMCallStrategy[Any]] = weakref.WeakSet()\n    self._strategy_lock = asyncio.Lock()  # Protect strategy initialization\n\n    # Proactive rate limiting (prevents hitting rate limits)\n    if config.max_requests_per_minute:\n        from aiolimiter import AsyncLimiter\n\n        # aiolimiter doesn't have explicit burst_size - it uses max_rate as burst capacity\n        # To support burst_size, we'd need to use max_rate + burst_size\n        # For now, we use max_rate directly (no additional burst)\n        self._proactive_rate_limiter: AsyncLimiter | None = AsyncLimiter(\n            max_rate=config.max_requests_per_minute,\n            time_period=60,  # per minute\n        )\n    else:\n        self._proactive_rate_limiter = None\n\n    self._strategies_cleaned_up = False\n</code></pre>"},{"location":"api/core/#async_batch_llm.ParallelBatchProcessor.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Context manager exit - ensures cleanup of strategies and resources.</p> <p>Calls cleanup() on all prepared strategies, then delegates to parent cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <p>Exception type (if any exception occurred)</p> required <code>exc_val</code> <p>Exception value (if any exception occurred)</p> required <code>exc_tb</code> <p>Exception traceback (if any exception occurred)</p> required <p>Returns:</p> Type Description <p>False to indicate exceptions should not be suppressed</p> Source code in <code>src/async_batch_llm/parallel.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"\n    Context manager exit - ensures cleanup of strategies and resources.\n\n    Calls cleanup() on all prepared strategies, then delegates to parent cleanup.\n\n    Args:\n        exc_type: Exception type (if any exception occurred)\n        exc_val: Exception value (if any exception occurred)\n        exc_tb: Exception traceback (if any exception occurred)\n\n    Returns:\n        False to indicate exceptions should not be suppressed\n    \"\"\"\n    await self._cleanup_strategies()\n\n    # Call parent cleanup to handle workers and queue\n    await self.cleanup()\n    return False  # Don't suppress exceptions\n</code></pre>"},{"location":"api/core/#async_batch_llm.ParallelBatchProcessor.get_stats","title":"get_stats  <code>async</code>","text":"<pre><code>get_stats() -&gt; dict\n</code></pre> <p>Get processor statistics (thread-safe).</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing processing statistics including:</p> <code>dict</code> <ul> <li>processed: Number of items processed</li> </ul> <code>dict</code> <ul> <li>succeeded: Number of successful items</li> </ul> <code>dict</code> <ul> <li>failed: Number of failed items</li> </ul> <code>dict</code> <ul> <li>rate_limit_count: Number of rate limit errors encountered</li> </ul> <code>dict</code> <ul> <li>error_counts: Dictionary of error types and their counts</li> </ul> <code>dict</code> <ul> <li>total: Total number of items queued</li> </ul> <code>dict</code> <ul> <li>start_time: Timestamp when processing started</li> </ul> Source code in <code>src/async_batch_llm/parallel.py</code> <pre><code>async def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get processor statistics (thread-safe).\n\n    Returns:\n        Dictionary containing processing statistics including:\n        - processed: Number of items processed\n        - succeeded: Number of successful items\n        - failed: Number of failed items\n        - rate_limit_count: Number of rate limit errors encountered\n        - error_counts: Dictionary of error types and their counts\n        - total: Total number of items queued\n        - start_time: Timestamp when processing started\n    \"\"\"\n    async with self._stats_lock:\n        return self._stats.copy()\n</code></pre>"},{"location":"api/core/#async_batch_llm.ParallelBatchProcessor.shutdown","title":"shutdown  <code>async</code>","text":"<pre><code>shutdown()\n</code></pre> <p>Clean up resources: flush observers and cancel pending tasks.</p> Source code in <code>src/async_batch_llm/parallel.py</code> <pre><code>async def shutdown(self):\n    \"\"\"Clean up resources: flush observers and cancel pending tasks.\"\"\"\n    await self._cleanup_strategies()\n    await self.cleanup()\n</code></pre>"},{"location":"api/core/#llmworkitem","title":"LLMWorkItem","text":""},{"location":"api/core/#async_batch_llm.LLMWorkItem","title":"async_batch_llm.LLMWorkItem  <code>dataclass</code>","text":"<pre><code>LLMWorkItem(item_id: str, strategy: LLMCallStrategy[TOutput], prompt: str = '', context: TContext | None = None)\n</code></pre> <p>               Bases: <code>Generic[TInput, TOutput, TContext]</code></p> <p>Represents a single work item to be processed by an LLM strategy.</p> <p>Attributes:</p> Name Type Description <code>item_id</code> <code>str</code> <p>Unique identifier for this work item</p> <code>strategy</code> <code>LLMCallStrategy[TOutput]</code> <p>LLM call strategy that encapsulates how to make the LLM call</p> <code>prompt</code> <code>str</code> <p>The prompt/input to pass to the LLM</p> <code>context</code> <code>TContext | None</code> <p>Optional context data passed through to results/post-processor</p>"},{"location":"api/core/#async_batch_llm.LLMWorkItem.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Validate work item fields.</p> Source code in <code>src/async_batch_llm/base.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate work item fields.\"\"\"\n    if not self.item_id or not isinstance(self.item_id, str):\n        raise ValueError(\n            f\"item_id must be a non-empty string (got {type(self.item_id).__name__}: {repr(self.item_id)}). \"\n            f\"Provide a unique string identifier for this work item.\"\n        )\n    if not self.item_id.strip():\n        raise ValueError(\n            f\"item_id cannot be whitespace only (got {repr(self.item_id)}). \"\n            f\"Provide a non-whitespace string identifier.\"\n        )\n</code></pre>"},{"location":"api/core/#workitemresult","title":"WorkItemResult","text":""},{"location":"api/core/#async_batch_llm.WorkItemResult","title":"async_batch_llm.WorkItemResult  <code>dataclass</code>","text":"<pre><code>WorkItemResult(item_id: str, success: bool, output: TOutput | None = None, error: str | None = None, context: TContext | None = None, token_usage: TokenUsage = (lambda: {'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0})(), gemini_safety_ratings: dict[str, str] | None = None)\n</code></pre> <p>               Bases: <code>Generic[TOutput, TContext]</code></p> <p>Result of processing a single work item.</p> <p>Attributes:</p> Name Type Description <code>item_id</code> <code>str</code> <p>ID of the work item</p> <code>success</code> <code>bool</code> <p>Whether processing succeeded</p> <code>output</code> <code>TOutput | None</code> <p>Agent output if successful, None if failed</p> <code>error</code> <code>str | None</code> <p>Error message if failed, None if successful</p> <code>context</code> <code>TContext | None</code> <p>Context data from the work item</p> <code>token_usage</code> <code>TokenUsage</code> <p>Token usage stats (input_tokens, output_tokens, total_tokens)</p> <code>gemini_safety_ratings</code> <code>dict[str, str] | None</code> <p>Gemini API safety ratings if available</p>"},{"location":"api/core/#processorconfig","title":"ProcessorConfig","text":""},{"location":"api/core/#async_batch_llm.ProcessorConfig","title":"async_batch_llm.ProcessorConfig  <code>dataclass</code>","text":"<pre><code>ProcessorConfig(max_workers: int = 5, timeout_per_item: float = 120.0, retry: RetryConfig = RetryConfig(), rate_limit: RateLimitConfig = RateLimitConfig(), max_requests_per_minute: float | None = None, progress_interval: int = 10, progress_callback_timeout: float | None = 5.0, enable_detailed_logging: bool = False, max_queue_size: int = 0, dry_run: bool = False)\n</code></pre> <p>Complete configuration for batch processor.</p>"},{"location":"api/core/#async_batch_llm.ProcessorConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration on construction.</p> Source code in <code>src/async_batch_llm/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration on construction.\"\"\"\n    self.validate()\n</code></pre>"},{"location":"api/core/#async_batch_llm.ProcessorConfig.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate complete configuration.</p> Source code in <code>src/async_batch_llm/core/config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate complete configuration.\"\"\"\n    if self.max_workers &lt; 1:\n        raise ValueError(\n            f\"max_workers must be &gt;= 1 (got {self.max_workers}). \"\n            f\"Set config.max_workers to a positive integer (typical: 5-20).\"\n        )\n    if self.timeout_per_item &lt;= 0:\n        raise ValueError(\n            f\"timeout_per_item must be &gt; 0 (got {self.timeout_per_item}). \"\n            f\"Set config.timeout_per_item to a positive number in seconds (typical: 60-300).\"\n        )\n    if self.progress_interval &lt; 1:\n        raise ValueError(\n            f\"progress_interval must be &gt;= 1 (got {self.progress_interval}). \"\n            f\"Set config.progress_interval to a positive integer.\"\n        )\n    if self.progress_callback_timeout is not None and self.progress_callback_timeout &lt;= 0:\n        raise ValueError(\n            f\"progress_callback_timeout must be &gt; 0 (got {self.progress_callback_timeout}). \"\n            f\"Set config.progress_callback_timeout to None to disable or a positive number of seconds.\"\n        )\n    if self.max_queue_size &lt; 0:\n        raise ValueError(\n            f\"max_queue_size must be &gt;= 0 (got {self.max_queue_size}). \"\n            f\"Set config.max_queue_size to 0 for unlimited, or a positive number to limit queue size.\"\n        )\n    if self.max_requests_per_minute is not None and self.max_requests_per_minute &lt;= 0:\n        raise ValueError(\n            f\"max_requests_per_minute must be &gt; 0 or None (got {self.max_requests_per_minute}). \"\n            f\"Set config.max_requests_per_minute to None to disable proactive rate limiting, \"\n            f\"or a positive number (typical: 10-500 requests/minute).\"\n        )\n\n    # Validate nested configs first\n    self.retry.validate()\n    self.rate_limit.validate()\n\n    # Cross-field validations\n    if self.max_queue_size &gt; 0 and self.max_queue_size &lt; self.max_workers:\n        logger.warning(\n            f\"max_queue_size ({self.max_queue_size}) is less than max_workers ({self.max_workers}). \"\n            f\"This may cause workers to starve waiting for work. \"\n            f\"Consider setting max_queue_size &gt;= max_workers or 0 for unlimited.\"\n        )\n\n    if self.timeout_per_item &lt; self.retry.initial_wait:\n        logger.warning(\n            f\"timeout_per_item ({self.timeout_per_item}s) is less than \"\n            f\"retry.initial_wait ({self.retry.initial_wait}s). \"\n            f\"This means the timeout may occur before the first retry delay completes. \"\n            f\"Consider increasing timeout_per_item or decreasing retry.initial_wait.\"\n        )\n\n    # Calculate maximum possible retry wait time\n    max_total_retry_wait = 0.0\n    for attempt in range(self.retry.max_attempts - 1):  # -1 because first attempt has no wait\n        wait_time = min(\n            self.retry.initial_wait * (self.retry.exponential_base**attempt),\n            self.retry.max_wait,\n        )\n        max_total_retry_wait += wait_time\n\n    if max_total_retry_wait &gt; 0 and self.timeout_per_item &lt; max_total_retry_wait * 0.5:\n        jitter_note = (\n            \" (with jitter, actual delays will be 50-100% of this)\" if self.retry.jitter else \"\"\n        )\n        logger.warning(\n            f\"timeout_per_item ({self.timeout_per_item}s) may be too short for retry strategy. \"\n            f\"With {self.retry.max_attempts} attempts, retry delays could total up to \"\n            f\"{max_total_retry_wait:.1f}s{jitter_note}. \"\n            f\"Consider increasing timeout_per_item to at least {max_total_retry_wait * 2:.1f}s.\"\n        )\n\n    # Validate proactive rate limit vs workers\n    if self.max_requests_per_minute is not None:\n        requests_per_second = self.max_requests_per_minute / 60.0\n        if requests_per_second &lt; self.max_workers:\n            logger.warning(\n                f\"max_requests_per_minute ({self.max_requests_per_minute}) is less than \"\n                f\"max_workers ({self.max_workers}). \"\n                f\"At {requests_per_second:.2f} requests/second with {self.max_workers} workers, \"\n                f\"workers may frequently wait for rate limit tokens. \"\n                f\"Consider reducing max_workers to {int(requests_per_second)} or increasing \"\n                f\"max_requests_per_minute.\"\n            )\n</code></pre>"},{"location":"api/core/#batchresult","title":"BatchResult","text":""},{"location":"api/core/#async_batch_llm.BatchResult","title":"async_batch_llm.BatchResult  <code>dataclass</code>","text":"<pre><code>BatchResult(results: list[WorkItemResult[TOutput, TContext]], total_items: int = 0, succeeded: int = 0, failed: int = 0, total_input_tokens: int = 0, total_output_tokens: int = 0, total_cached_tokens: int = 0)\n</code></pre> <p>               Bases: <code>Generic[TOutput, TContext]</code></p> <p>Result of processing a batch of work items.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[WorkItemResult[TOutput, TContext]]</code> <p>List of individual work item results</p> <code>total_items</code> <code>int</code> <p>Total number of items in the batch</p> <code>succeeded</code> <code>int</code> <p>Number of successful items</p> <code>failed</code> <code>int</code> <p>Number of failed items</p> <code>total_input_tokens</code> <code>int</code> <p>Sum of input tokens across all items</p> <code>total_output_tokens</code> <code>int</code> <p>Sum of output tokens across all items</p> <code>total_cached_tokens</code> <code>int</code> <p>Sum of cached input tokens across all items (v0.2.0)</p>"},{"location":"api/core/#async_batch_llm.BatchResult.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Calculate summary statistics from results.</p> Source code in <code>src/async_batch_llm/base.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Calculate summary statistics from results.\"\"\"\n    self.total_items = len(self.results)\n    self.succeeded = sum(1 for r in self.results if r.success)\n    self.failed = sum(1 for r in self.results if not r.success)\n    self.total_input_tokens = sum(r.token_usage.get(\"input_tokens\", 0) for r in self.results)\n    self.total_output_tokens = sum(r.token_usage.get(\"output_tokens\", 0) for r in self.results)\n    # v0.2.0: Aggregate cached tokens\n    self.total_cached_tokens = sum(\n        r.token_usage.get(\"cached_input_tokens\", 0) for r in self.results\n    )\n</code></pre>"},{"location":"api/core/#async_batch_llm.BatchResult.cache_hit_rate","title":"cache_hit_rate","text":"<pre><code>cache_hit_rate() -&gt; float\n</code></pre> <p>Calculate cache hit rate as percentage of input tokens that were cached.</p> <p>Returns:</p> Type Description <code>float</code> <p>Percentage (0.0 to 100.0) of input tokens served from cache</p> Source code in <code>src/async_batch_llm/base.py</code> <pre><code>def cache_hit_rate(self) -&gt; float:\n    \"\"\"\n    Calculate cache hit rate as percentage of input tokens that were cached.\n\n    Returns:\n        Percentage (0.0 to 100.0) of input tokens served from cache\n    \"\"\"\n    if self.total_input_tokens == 0:\n        return 0.0\n    return (self.total_cached_tokens / self.total_input_tokens) * 100.0\n</code></pre>"},{"location":"api/core/#async_batch_llm.BatchResult.effective_input_tokens","title":"effective_input_tokens","text":"<pre><code>effective_input_tokens() -&gt; int\n</code></pre> <p>Calculate effective input tokens (actual cost after caching).</p> <p>Gemini charges 10% of the normal price for cached tokens.</p> <p>Returns:</p> Type Description <code>int</code> <p>Effective number of input tokens billed</p> Source code in <code>src/async_batch_llm/base.py</code> <pre><code>def effective_input_tokens(self) -&gt; int:\n    \"\"\"\n    Calculate effective input tokens (actual cost after caching).\n\n    Gemini charges 10% of the normal price for cached tokens.\n\n    Returns:\n        Effective number of input tokens billed\n    \"\"\"\n    # Cached tokens cost 10% of normal, so discount is 90%\n    discount = int(self.total_cached_tokens * 0.9)\n    return self.total_input_tokens - discount\n</code></pre>"},{"location":"api/observers/","title":"Observers API Reference","text":""},{"location":"api/observers/#processorobserver","title":"ProcessorObserver","text":""},{"location":"api/observers/#async_batch_llm.observers.ProcessorObserver","title":"async_batch_llm.observers.ProcessorObserver","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for processor event observers.</p>"},{"location":"api/observers/#async_batch_llm.observers.ProcessorObserver.on_event","title":"on_event  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>on_event(event: ProcessingEvent, data: dict[str, Any]) -&gt; None\n</code></pre> <p>Handle processor event.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>ProcessingEvent</code> <p>The event type</p> required <code>data</code> <code>dict[str, Any]</code> <p>Event-specific data</p> required Source code in <code>src/async_batch_llm/observers/base.py</code> <pre><code>@abstractmethod\nasync def on_event(\n    self,\n    event: ProcessingEvent,\n    data: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    Handle processor event.\n\n    Args:\n        event: The event type\n        data: Event-specific data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observers/#baseobserver","title":"BaseObserver","text":""},{"location":"api/observers/#async_batch_llm.observers.BaseObserver","title":"async_batch_llm.observers.BaseObserver","text":"<p>               Bases: <code>ProcessorObserver</code></p> <p>Base observer with no-op implementation.</p>"},{"location":"api/observers/#async_batch_llm.observers.BaseObserver.on_event","title":"on_event  <code>async</code>","text":"<pre><code>on_event(event: ProcessingEvent, data: dict[str, Any]) -&gt; None\n</code></pre> <p>Default: do nothing.</p> Source code in <code>src/async_batch_llm/observers/base.py</code> <pre><code>async def on_event(\n    self,\n    event: ProcessingEvent,\n    data: dict[str, Any],\n) -&gt; None:\n    \"\"\"Default: do nothing.\"\"\"\n    pass\n</code></pre>"},{"location":"api/observers/#metricsobserver","title":"MetricsObserver","text":""},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver","title":"async_batch_llm.observers.MetricsObserver","text":"<pre><code>MetricsObserver(*, max_processing_samples: int = 100)\n</code></pre> <p>               Bases: <code>BaseObserver</code></p> <p>Collect metrics for monitoring (thread-safe).</p> <p>Initialize metrics collector.</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>def __init__(self, *, max_processing_samples: int = 100):\n    \"\"\"Initialize metrics collector.\"\"\"\n    if max_processing_samples &lt;= 0:\n        raise ValueError(\"max_processing_samples must be positive\")\n    self.metrics: dict[str, Any] = {\n        \"items_processed\": 0,\n        \"items_succeeded\": 0,\n        \"items_failed\": 0,\n        \"rate_limits_hit\": 0,\n        \"total_cooldown_time\": 0.0,\n        \"processing_times\": [],\n        \"error_counts\": {},\n        \"processing_times_count\": 0,\n        \"processing_times_sum\": 0.0,\n    }\n    self._processing_times: list[float] = []\n    self._max_processing_samples = max_processing_samples\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_dict","title":"export_dict  <code>async</code>","text":"<pre><code>export_dict() -&gt; dict[str, Any]\n</code></pre> <p>Export metrics as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing all metrics and computed statistics</p> Example <p>observer = MetricsObserver()</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>async def export_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Export metrics as a dictionary.\n\n    Returns:\n        Dictionary containing all metrics and computed statistics\n\n    Example:\n        &gt;&gt;&gt; observer = MetricsObserver()\n        &gt;&gt;&gt; # ... process items ...\n        &gt;&gt;&gt; data = await observer.export_dict()\n        &gt;&gt;&gt; print(data[\"success_rate\"])\n    \"\"\"\n    return await self.get_metrics()\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_dict--process-items","title":"... process items ...","text":"<p>data = await observer.export_dict() print(data[\"success_rate\"])</p>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_json","title":"export_json  <code>async</code>","text":"<pre><code>export_json() -&gt; str\n</code></pre> <p>Export metrics as JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>JSON string containing all metrics and computed statistics</p> Example <p>observer = MetricsObserver()</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>async def export_json(self) -&gt; str:\n    \"\"\"Export metrics as JSON string.\n\n    Returns:\n        JSON string containing all metrics and computed statistics\n\n    Example:\n        &gt;&gt;&gt; observer = MetricsObserver()\n        &gt;&gt;&gt; # ... process items ...\n        &gt;&gt;&gt; json_str = await observer.export_json()\n        &gt;&gt;&gt; print(json_str)\n    \"\"\"\n    metrics = await self.get_metrics()\n    # Convert processing_times list to just count for cleaner export\n    export_data = {\n        **{k: v for k, v in metrics.items() if k != \"processing_times\"},\n        \"processing_times_count\": metrics.get(\"processing_times_count\", 0),\n    }\n    return json.dumps(export_data, indent=2)\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_json--process-items","title":"... process items ...","text":"<p>json_str = await observer.export_json() print(json_str)</p>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_prometheus","title":"export_prometheus  <code>async</code>","text":"<pre><code>export_prometheus() -&gt; str\n</code></pre> <p>Export metrics in Prometheus text format.</p> <p>Returns:</p> Type Description <code>str</code> <p>Prometheus-formatted metrics string</p> Example <p>observer = MetricsObserver()</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>async def export_prometheus(self) -&gt; str:\n    \"\"\"Export metrics in Prometheus text format.\n\n    Returns:\n        Prometheus-formatted metrics string\n\n    Example:\n        &gt;&gt;&gt; observer = MetricsObserver()\n        &gt;&gt;&gt; # ... process items ...\n        &gt;&gt;&gt; prom_text = await observer.export_prometheus()\n        &gt;&gt;&gt; print(prom_text)\n        # HELP async_batch_llm_items_processed Total items processed\n        # TYPE async_batch_llm_items_processed counter\n        async_batch_llm_items_processed 100\n        ...\n    \"\"\"\n    metrics = await self.get_metrics()\n\n    lines = []\n\n    # Counter metrics\n    counters = [\n        (\"items_processed\", \"Total items processed\"),\n        (\"items_succeeded\", \"Total items succeeded\"),\n        (\"items_failed\", \"Total items failed\"),\n        (\"rate_limits_hit\", \"Total rate limits encountered\"),\n    ]\n\n    for metric_name, help_text in counters:\n        lines.append(f\"# HELP async_batch_llm_{metric_name} {help_text}\")\n        lines.append(f\"# TYPE async_batch_llm_{metric_name} counter\")\n        lines.append(f\"async_batch_llm_{metric_name} {metrics.get(metric_name, 0)}\")\n        lines.append(\"\")\n\n    # Gauge metrics\n    gauges = [\n        (\"avg_processing_time\", \"Average processing time in seconds\"),\n        (\"success_rate\", \"Success rate (0.0 to 1.0)\"),\n        (\"total_cooldown_time\", \"Total time spent in rate limit cooldown (seconds)\"),\n        (\"processing_times_count\", \"Number of recorded processing time samples\"),\n    ]\n\n    for metric_name, help_text in gauges:\n        lines.append(f\"# HELP async_batch_llm_{metric_name} {help_text}\")\n        lines.append(f\"# TYPE async_batch_llm_{metric_name} gauge\")\n        lines.append(f\"async_batch_llm_{metric_name} {metrics.get(metric_name, 0)}\")\n        lines.append(\"\")\n\n    # Error counts as labeled counter\n    error_counts = metrics.get(\"error_counts\", {})\n    if error_counts:\n        lines.append(\"# HELP async_batch_llm_errors_total Total errors by type\")\n        lines.append(\"# TYPE async_batch_llm_errors_total counter\")\n        for error_type, count in error_counts.items():\n            # Sanitize error type for Prometheus label\n            safe_type = error_type.replace('\"', '\\\\\"')\n            lines.append(f'async_batch_llm_errors_total{{error_type=\"{safe_type}\"}} {count}')\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_prometheus--process-items","title":"... process items ...","text":"<p>prom_text = await observer.export_prometheus() print(prom_text)</p>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_prometheus--help-async_batch_llm_items_processed-total-items-processed","title":"HELP async_batch_llm_items_processed Total items processed","text":""},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.export_prometheus--type-async_batch_llm_items_processed-counter","title":"TYPE async_batch_llm_items_processed counter","text":"<p>async_batch_llm_items_processed 100 ...</p>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.get_metrics","title":"get_metrics  <code>async</code>","text":"<pre><code>get_metrics() -&gt; dict[str, Any]\n</code></pre> <p>Get collected metrics with computed statistics (thread-safe).</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>async def get_metrics(self) -&gt; dict[str, Any]:\n    \"\"\"Get collected metrics with computed statistics (thread-safe).\"\"\"\n    async with self._lock:\n        return {\n            **{k: v for k, v in self.metrics.items() if k != \"processing_times\"},\n            \"processing_times\": list(self._processing_times),\n            \"avg_processing_time\": (\n                self.metrics[\"processing_times_sum\"] / self.metrics[\"processing_times_count\"]\n                if self.metrics[\"processing_times_count\"] &gt; 0\n                else 0\n            ),\n            \"success_rate\": (\n                self.metrics[\"items_succeeded\"] / self.metrics[\"items_processed\"]\n                if self.metrics[\"items_processed\"] &gt; 0\n                else 0\n            ),\n        }\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.on_event","title":"on_event  <code>async</code>","text":"<pre><code>on_event(event: ProcessingEvent, data: dict[str, Any]) -&gt; None\n</code></pre> <p>Collect metrics from events (thread-safe).</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>async def on_event(\n    self,\n    event: ProcessingEvent,\n    data: dict[str, Any],\n) -&gt; None:\n    \"\"\"Collect metrics from events (thread-safe).\"\"\"\n    async with self._lock:\n        if event == ProcessingEvent.ITEM_COMPLETED:\n            self.metrics[\"items_processed\"] += 1\n            self.metrics[\"items_succeeded\"] += 1\n            if \"duration\" in data:\n                duration = float(data[\"duration\"])\n                self.metrics[\"processing_times_sum\"] += duration\n                self.metrics[\"processing_times_count\"] += 1\n                self._processing_times.append(duration)\n                if len(self._processing_times) &gt; self._max_processing_samples:\n                    self._processing_times.pop(0)\n\n        elif event == ProcessingEvent.ITEM_FAILED:\n            self.metrics[\"items_processed\"] += 1\n            self.metrics[\"items_failed\"] += 1\n            if \"error_type\" in data:\n                error_type = data[\"error_type\"]\n                self.metrics[\"error_counts\"][error_type] = (\n                    self.metrics[\"error_counts\"].get(error_type, 0) + 1\n                )\n\n        elif event == ProcessingEvent.RATE_LIMIT_HIT:\n            self.metrics[\"rate_limits_hit\"] += 1\n\n        elif event == ProcessingEvent.COOLDOWN_ENDED:\n            if \"duration\" in data:\n                self.metrics[\"total_cooldown_time\"] += data[\"duration\"]\n</code></pre>"},{"location":"api/observers/#async_batch_llm.observers.MetricsObserver.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset all metrics.</p> Source code in <code>src/async_batch_llm/observers/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics.\"\"\"\n    self.metrics = {\n        \"items_processed\": 0,\n        \"items_succeeded\": 0,\n        \"items_failed\": 0,\n        \"rate_limits_hit\": 0,\n        \"total_cooldown_time\": 0.0,\n        \"processing_times\": [],\n        \"error_counts\": {},\n        \"processing_times_count\": 0,\n        \"processing_times_sum\": 0.0,\n    }\n    self._processing_times = []\n</code></pre>"},{"location":"api/strategies/","title":"Strategies API Reference","text":""},{"location":"api/strategies/#llmcallstrategy","title":"LLMCallStrategy","text":""},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy","title":"async_batch_llm.LLMCallStrategy","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TOutput]</code></p> <p>Abstract base class for LLM call strategies.</p> <p>A strategy encapsulates how LLM calls are made, including: - Resource initialization (caches, clients) - Call execution with retries - Resource cleanup</p> <p>The framework calls: 1. prepare() once before any retries 2. execute() for each attempt (including retries) 3. cleanup() once after all attempts complete or fail</p>"},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Clean up resources after all retry attempts complete.</p> <p>Called once per work item after processing finishes (success or failure).</p> <p>Use this for: - Closing connections/sessions - Releasing locks - Logging final metrics - Deleting temporary files</p> <p>Do NOT use this for: - Deleting caches intended for reuse across runs - Destructive cleanup that prevents resource reuse</p> <p>Note on Caches (v0.2.0): For reusable resources like Gemini caches with TTLs, consider letting them expire naturally to enable cost savings across multiple pipeline runs. See <code>GeminiCachedStrategy</code> for an example.</p> <p>Default: no-op</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def cleanup(self) -&gt; None:\n    \"\"\"\n    Clean up resources after all retry attempts complete.\n\n    Called once per work item after processing finishes (success or failure).\n\n    **Use this for:**\n    - Closing connections/sessions\n    - Releasing locks\n    - Logging final metrics\n    - Deleting temporary files\n\n    **Do NOT use this for:**\n    - Deleting caches intended for reuse across runs\n    - Destructive cleanup that prevents resource reuse\n\n    **Note on Caches (v0.2.0):**\n    For reusable resources like Gemini caches with TTLs, consider letting\n    them expire naturally to enable cost savings across multiple pipeline\n    runs. See `GeminiCachedStrategy` for an example.\n\n    Default: no-op\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy.dry_run","title":"dry_run  <code>async</code>","text":"<pre><code>dry_run(prompt: str) -&gt; tuple[TOutput, TokenUsage]\n</code></pre> <p>Return mock output for dry-run mode (testing without API calls).</p> <p>Override this method to provide realistic mock data for testing. Default implementation returns placeholder values that may not match your output type.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt that would have been sent to the LLM</p> required <p>Returns:</p> Type Description <code>tuple[TOutput, TokenUsage]</code> <p>Tuple of (mock_output, mock_token_usage)</p> <p>Default behavior: - Returns string \"[DRY-RUN] Mock output\" as output - Returns mock token usage: 100 input, 50 output, 150 total</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def dry_run(self, prompt: str) -&gt; tuple[TOutput, TokenUsage]:\n    \"\"\"\n    Return mock output for dry-run mode (testing without API calls).\n\n    Override this method to provide realistic mock data for testing.\n    Default implementation returns placeholder values that may not match\n    your output type.\n\n    Args:\n        prompt: The prompt that would have been sent to the LLM\n\n    Returns:\n        Tuple of (mock_output, mock_token_usage)\n\n    Default behavior:\n    - Returns string \"[DRY-RUN] Mock output\" as output\n    - Returns mock token usage: 100 input, 50 output, 150 total\n    \"\"\"\n    mock_output: TOutput = f\"[DRY-RUN] Mock output for prompt: {prompt[:50]}...\"  # type: ignore[assignment]\n    mock_tokens: TokenUsage = {\n        \"input_tokens\": 100,\n        \"output_tokens\": 50,\n        \"total_tokens\": 150,\n    }\n    return mock_output, mock_tokens\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy.execute","title":"execute  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>execute(prompt: str, attempt: int, timeout: float, state: RetryState | None = None) -&gt; tuple[TOutput, TokenUsage]\n</code></pre> <p>Execute an LLM call for the given attempt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>attempt</code> <code>int</code> <p>Which retry attempt this is (1, 2, 3, ...)</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for response (seconds)</p> required <code>state</code> <code>RetryState | None</code> <p>Optional retry state that persists across attempts (v0.3.0)</p> <code>None</code> <p>Returns:</p> Type Description <code>TOutput</code> <p>Tuple of (output, token_usage)</p> <code>TokenUsage</code> <p>where token_usage is a TokenUsage dict with optional keys:</p> <code>tuple[TOutput, TokenUsage]</code> <p>input_tokens, output_tokens, total_tokens, cached_input_tokens</p> <p>Note (v0.3.0):     The state parameter allows strategies to maintain state across retry     attempts for multi-stage retry patterns. See RetryState documentation     for examples.</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>@abstractmethod\nasync def execute(\n    self, prompt: str, attempt: int, timeout: float, state: \"RetryState | None\" = None\n) -&gt; tuple[TOutput, TokenUsage]:\n    \"\"\"\n    Execute an LLM call for the given attempt.\n\n    Args:\n        prompt: The prompt to send to the LLM\n        attempt: Which retry attempt this is (1, 2, 3, ...)\n        timeout: Maximum time to wait for response (seconds)\n        state: Optional retry state that persists across attempts (v0.3.0)\n\n    Returns:\n        Tuple of (output, token_usage)\n        where token_usage is a TokenUsage dict with optional keys:\n        input_tokens, output_tokens, total_tokens, cached_input_tokens\n\n    Raises:\n        Any exception to trigger retry (if retryable) or failure\n\n    Note (v0.3.0):\n        The state parameter allows strategies to maintain state across retry\n        attempts for multi-stage retry patterns. See RetryState documentation\n        for examples.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy.on_error","title":"on_error  <code>async</code>","text":"<pre><code>on_error(exception: Exception, attempt: int, state: RetryState | None = None) -&gt; None\n</code></pre> <p>Handle errors that occur during execute().</p> <p>Called by the framework when execute() raises an exception, before deciding whether to retry. This allows strategies to: - Inspect the error type to adjust retry behavior - Store error information for use in next attempt - Modify prompts based on validation errors - Track error patterns across attempts</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>Exception</code> <p>The exception that was raised during execute()</p> required <code>attempt</code> <code>int</code> <p>Which attempt number failed (1, 2, 3, ...)</p> required <code>state</code> <code>RetryState | None</code> <p>Optional retry state that persists across attempts (v0.3.0)</p> <code>None</code> <p>Default: no-op</p> <p>Example (v0.2.0):     async def on_error(self, exception: Exception, attempt: int) -&gt; None:         # Store last error for smart retry logic         self.last_error = exception</p> <pre><code>    # Track validation errors vs network errors\n    if isinstance(exception, ValidationError):\n        self.should_escalate_model = True\n</code></pre> <p>Example (v0.3.0 with retry state):     async def on_error(         self, exception: Exception, attempt: int, state: RetryState | None = None     ) -&gt; None:         if state:             # Track validation errors separately from other errors             if isinstance(exception, ValidationError):                 count = state.get('validation_failures', 0) + 1                 state.set('validation_failures', count)                 # Save partial results for recovery                 if hasattr(exception, 'partial_data'):                     state.set('partial_data', exception.partial_data)</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def on_error(\n    self, exception: Exception, attempt: int, state: \"RetryState | None\" = None\n) -&gt; None:\n    \"\"\"\n    Handle errors that occur during execute().\n\n    Called by the framework when execute() raises an exception, before\n    deciding whether to retry. This allows strategies to:\n    - Inspect the error type to adjust retry behavior\n    - Store error information for use in next attempt\n    - Modify prompts based on validation errors\n    - Track error patterns across attempts\n\n    Args:\n        exception: The exception that was raised during execute()\n        attempt: Which attempt number failed (1, 2, 3, ...)\n        state: Optional retry state that persists across attempts (v0.3.0)\n\n    Default: no-op\n\n    Example (v0.2.0):\n        async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n            # Store last error for smart retry logic\n            self.last_error = exception\n\n            # Track validation errors vs network errors\n            if isinstance(exception, ValidationError):\n                self.should_escalate_model = True\n\n    Example (v0.3.0 with retry state):\n        async def on_error(\n            self, exception: Exception, attempt: int, state: RetryState | None = None\n        ) -&gt; None:\n            if state:\n                # Track validation errors separately from other errors\n                if isinstance(exception, ValidationError):\n                    count = state.get('validation_failures', 0) + 1\n                    state.set('validation_failures', count)\n                    # Save partial results for recovery\n                    if hasattr(exception, 'partial_data'):\n                        state.set('partial_data', exception.partial_data)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.LLMCallStrategy.prepare","title":"prepare  <code>async</code>","text":"<pre><code>prepare() -&gt; None\n</code></pre> <p>Initialize resources before making any LLM calls.</p> <p>Called once per work item before any retry attempts. Use this to set up caches, initialize clients, etc.</p> <p>Default: no-op</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def prepare(self) -&gt; None:\n    \"\"\"\n    Initialize resources before making any LLM calls.\n\n    Called once per work item before any retry attempts.\n    Use this to set up caches, initialize clients, etc.\n\n    Default: no-op\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#pydanticaistrategy","title":"PydanticAIStrategy","text":""},{"location":"api/strategies/#async_batch_llm.PydanticAIStrategy","title":"async_batch_llm.PydanticAIStrategy","text":"<pre><code>PydanticAIStrategy(agent: Agent[None, TOutput])\n</code></pre> <p>               Bases: <code>LLMCallStrategy[TOutput]</code></p> <p>Strategy for using PydanticAI agents.</p> <p>This strategy wraps a PydanticAI agent, providing a clean interface for batch processing. The agent handles all model interaction, validation, and parsing.</p> <p>Best for: Structured output with Pydantic models, using PydanticAI's features.</p> <p>Initialize PydanticAI strategy.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent[None, TOutput]</code> <p>Configured PydanticAI agent</p> required Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>def __init__(self, agent: \"Agent[None, TOutput]\"):\n    \"\"\"\n    Initialize PydanticAI strategy.\n\n    Args:\n        agent: Configured PydanticAI agent\n    \"\"\"\n    if Agent is Any:\n        raise ImportError(\n            \"pydantic-ai is required for PydanticAIStrategy. \"\n            \"Install with: pip install 'async-batch-llm[pydantic-ai]'\"\n        )\n\n    self.agent = agent\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.PydanticAIStrategy.dry_run","title":"dry_run  <code>async</code>","text":"<pre><code>dry_run(prompt: str) -&gt; tuple[TOutput, TokenUsage]\n</code></pre> <p>Return mock output based on agent's result_type for dry-run mode.</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def dry_run(self, prompt: str) -&gt; tuple[TOutput, TokenUsage]:\n    \"\"\"Return mock output based on agent's result_type for dry-run mode.\"\"\"\n    # Try to create a mock instance of the expected output type\n    try:\n        from pydantic import BaseModel\n\n        result_type = self.agent.result_type  # type: ignore[attr-defined]\n\n        # If result_type is a Pydantic model, try to create an instance\n        if isinstance(result_type, type) and issubclass(result_type, BaseModel):\n            # Use model_construct to create instance without validation\n            # This allows creating instances even with required fields\n            mock_output: TOutput = result_type.model_construct()  # type: ignore[assignment]\n        else:\n            # For non-Pydantic types, use base class default\n            return await super().dry_run(prompt)\n\n    except Exception:\n        # If anything fails, fall back to base class default\n        return await super().dry_run(prompt)\n\n    # Return mock output with realistic token usage\n    mock_tokens: TokenUsage = {\n        \"input_tokens\": len(prompt.split()),  # Rough estimate\n        \"output_tokens\": 50,\n        \"total_tokens\": len(prompt.split()) + 50,\n    }\n\n    return mock_output, mock_tokens\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.PydanticAIStrategy.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(prompt: str, attempt: int, timeout: float, state: RetryState | None = None) -&gt; tuple[TOutput, TokenUsage]\n</code></pre> <p>Execute PydanticAI agent call.</p> <p>Note: timeout parameter is provided for information but timeout enforcement is handled by the framework wrapping this call in asyncio.wait_for().</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>attempt</code> <code>int</code> <p>Which retry attempt this is (1, 2, 3, ...)</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for response (seconds)</p> required <code>state</code> <code>RetryState | None</code> <p>Optional retry state (v0.3.0, unused by this strategy)</p> <code>None</code> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def execute(\n    self, prompt: str, attempt: int, timeout: float, state: RetryState | None = None\n) -&gt; tuple[TOutput, TokenUsage]:\n    \"\"\"Execute PydanticAI agent call.\n\n    Note: timeout parameter is provided for information but timeout enforcement\n    is handled by the framework wrapping this call in asyncio.wait_for().\n\n    Args:\n        prompt: The prompt to send to the LLM\n        attempt: Which retry attempt this is (1, 2, 3, ...)\n        timeout: Maximum time to wait for response (seconds)\n        state: Optional retry state (v0.3.0, unused by this strategy)\n    \"\"\"\n    result = await self.agent.run(prompt)\n\n    # Extract token usage FIRST (before accessing result.output which may fail validation)\n    usage = result.usage()\n    tokens: TokenUsage = {\n        \"input_tokens\": usage.request_tokens if usage else 0,\n        \"output_tokens\": usage.response_tokens if usage else 0,\n        \"total_tokens\": usage.total_tokens if usage else 0,\n    }\n\n    # Access result.output (may raise validation errors)\n    try:\n        output = result.output\n    except Exception as e:\n        # Attach token usage to exception so framework can track it\n        if not hasattr(e, \"__dict__\"):\n            # For built-in exceptions without __dict__, wrap in TokenTrackingError\n            wrapped = TokenTrackingError(str(e), token_usage=tokens)\n            wrapped.__cause__ = e\n            raise wrapped from e\n        else:\n            e.__dict__[\"_failed_token_usage\"] = tokens\n            raise\n\n    return output, tokens\n</code></pre>"},{"location":"api/strategies/#geministrategy","title":"GeminiStrategy","text":""},{"location":"api/strategies/#async_batch_llm.GeminiStrategy","title":"async_batch_llm.GeminiStrategy","text":"<pre><code>GeminiStrategy(model: str, client: Client, response_parser: Callable[[Any], TOutput], config: GenerateContentConfig | None = None, include_metadata: bool = False)\n</code></pre> <p>               Bases: <code>LLMCallStrategy[TOutput]</code></p> <p>Strategy for calling Google Gemini API directly.</p> <p>This strategy uses the google-genai SDK to make direct API calls without caching. Best for one-off calls or when caching isn't needed.</p> <p>Initialize Gemini strategy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name (e.g., \"gemini-2.5-flash\")</p> required <code>client</code> <code>Client</code> <p>Initialized Gemini client</p> required <code>response_parser</code> <code>Callable[[Any], TOutput]</code> <p>Function to parse response into TOutput</p> required <code>config</code> <code>GenerateContentConfig | None</code> <p>Optional generation config (temperature, etc.)</p> <code>None</code> <code>include_metadata</code> <code>bool</code> <p>If True, return GeminiResponse with safety ratings (v0.3.0)</p> <code>False</code> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    client: \"genai.Client\",\n    response_parser: Callable[[Any], TOutput],\n    config: \"GenerateContentConfig | None\" = None,\n    include_metadata: bool = False,  # v0.3.0: Opt-in for safety ratings\n):\n    \"\"\"\n    Initialize Gemini strategy.\n\n    Args:\n        model: Model name (e.g., \"gemini-2.5-flash\")\n        client: Initialized Gemini client\n        response_parser: Function to parse response into TOutput\n        config: Optional generation config (temperature, etc.)\n        include_metadata: If True, return GeminiResponse with safety ratings (v0.3.0)\n    \"\"\"\n    if genai is None:\n        raise ImportError(\n            \"google-genai is required for GeminiStrategy. \"\n            \"Install with: pip install 'async-batch-llm[gemini]'\"\n        )\n\n    self.model = model\n    self.client = client\n    self.response_parser = response_parser\n    self.config = config\n    self.include_metadata = include_metadata\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.GeminiStrategy.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(prompt: str, attempt: int, timeout: float, state: RetryState | None = None) -&gt; tuple[TOutput | GeminiResponse[TOutput], TokenUsage]\n</code></pre> <p>Execute Gemini API call.</p> <p>Note: timeout parameter is provided for information but timeout enforcement is handled by the framework wrapping this call in asyncio.wait_for().</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>attempt</code> <code>int</code> <p>Which retry attempt this is (1, 2, 3, ...)</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for response (seconds)</p> required <code>state</code> <code>RetryState | None</code> <p>Optional retry state (v0.3.0, unused by this strategy)</p> <code>None</code> <p>Returns:</p> Type Description <code>TOutput | GeminiResponse[TOutput]</code> <p>Tuple of (output, token_usage) where output is either:</p> <code>TokenUsage</code> <ul> <li>TOutput if include_metadata=False (default)</li> </ul> <code>tuple[TOutput | GeminiResponse[TOutput], TokenUsage]</code> <ul> <li>GeminiResponse[TOutput] if include_metadata=True (v0.3.0)</li> </ul> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def execute(\n    self, prompt: str, attempt: int, timeout: float, state: RetryState | None = None\n) -&gt; tuple[TOutput | GeminiResponse[TOutput], TokenUsage]:\n    \"\"\"Execute Gemini API call.\n\n    Note: timeout parameter is provided for information but timeout enforcement\n    is handled by the framework wrapping this call in asyncio.wait_for().\n\n    Args:\n        prompt: The prompt to send to the LLM\n        attempt: Which retry attempt this is (1, 2, 3, ...)\n        timeout: Maximum time to wait for response (seconds)\n        state: Optional retry state (v0.3.0, unused by this strategy)\n\n    Returns:\n        Tuple of (output, token_usage) where output is either:\n        - TOutput if include_metadata=False (default)\n        - GeminiResponse[TOutput] if include_metadata=True (v0.3.0)\n    \"\"\"\n    # Make the call\n    response = await self.client.aio.models.generate_content(\n        model=self.model,\n        contents=prompt,\n        config=self.config,\n    )\n\n    # Extract token usage FIRST (before parsing/validation that might fail)\n    # This ensures we track tokens even when validation errors occur\n    usage_metadata = getattr(response, \"usage_metadata\", None)\n    if usage_metadata is not None:\n        input_tokens = getattr(usage_metadata, \"prompt_token_count\", 0) or 0\n        output_tokens = getattr(usage_metadata, \"candidates_token_count\", 0) or 0\n        total_tokens = getattr(usage_metadata, \"total_token_count\", 0) or 0\n    else:\n        input_tokens = output_tokens = total_tokens = 0\n\n    tokens: TokenUsage = {\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"total_tokens\": total_tokens,\n    }\n\n    # Parse output (may raise validation errors)\n    try:\n        output = self.response_parser(response)\n    except Exception as e:\n        # Attach token usage to exception so framework can track it\n        if not hasattr(e, \"__dict__\"):\n            # For built-in exceptions without __dict__, wrap in TokenTrackingError\n            wrapped = TokenTrackingError(str(e), token_usage=tokens)\n            wrapped.__cause__ = e\n            raise wrapped from e\n        else:\n            e.__dict__[\"_failed_token_usage\"] = tokens\n            raise\n\n    # Return with metadata if requested (v0.3.0)\n    if self.include_metadata:\n        safety_ratings = _extract_safety_ratings(response)\n        finish_reason = None\n        try:\n            if hasattr(response, \"candidates\") and response.candidates:\n                candidate = response.candidates[0]\n                if hasattr(candidate, \"finish_reason\"):\n                    finish_reason = str(candidate.finish_reason)\n        except Exception as e:\n            logger.warning(f\"Failed to extract finish_reason: {e}\")\n\n        return GeminiResponse(\n            output=output,\n            safety_ratings=safety_ratings,\n            finish_reason=finish_reason,\n            token_usage=tokens,\n            raw_response=response,\n        ), tokens\n\n    return output, tokens\n</code></pre>"},{"location":"api/strategies/#geminicachedstrategy","title":"GeminiCachedStrategy","text":""},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy","title":"async_batch_llm.GeminiCachedStrategy","text":"<pre><code>GeminiCachedStrategy(model: str, client: Client, response_parser: Callable[[Any], TOutput], cached_content: list[Content], cache_ttl_seconds: int = 3600, cache_refresh_threshold: float = 0.1, cache_renewal_buffer_seconds: int = 300, auto_renew: bool = True, config: GenerateContentConfig | None = None, include_metadata: bool = False, cache_tags: dict[str, str] | None = None)\n</code></pre> <p>               Bases: <code>LLMCallStrategy[TOutput]</code></p> <p>Strategy for calling Google Gemini API with context caching.</p> <p>This strategy creates a Gemini cache for the system instruction and/or initial context, then uses it across all retry attempts. The cache is automatically refreshed if it's close to expiring, and deleted on cleanup.</p> <p>Best for: Repeated calls with large shared context (RAG, long documents).</p>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy--critical-for-cost-optimization","title":"CRITICAL FOR COST OPTIMIZATION:","text":"<p>Create ONE instance and reuse it across ALL work items to share the cache. This provides 70-90% cost savings compared to creating new instances per item.</p> <p>CORRECT usage (70-90% savings):     &gt;&gt;&gt; # Create one strategy     &gt;&gt;&gt; strategy = GeminiCachedStrategy(     ...     model=\"gemini-2.0-flash\",     ...     client=client,     ...     response_parser=lambda r: str(r.text),     ...     cached_content=[system_instruction, context_docs],     ... )     &gt;&gt;&gt;     &gt;&gt;&gt; # Reuse for all items     &gt;&gt;&gt; for doc in documents:     ...     work_item = LLMWorkItem(strategy=strategy, ...)  # REUSE same strategy     ...     await processor.add_work(work_item)</p> <p>WRONG usage (creates new cache per item - expensive!):     &gt;&gt;&gt; for doc in documents:     ...     strategy = GeminiCachedStrategy(...)  # NEW instance per loop - DON'T DO THIS!     ...     work_item = LLMWorkItem(strategy=strategy, ...)</p> <p>Cost comparison (100 items with 500 cached tokens): - Wrong approach: $10.00 (no caching benefit) - Right approach: $3.00 (70% savings from shared cache)</p> <p>Initialize Gemini cached strategy with automatic cache renewal (v0.2.0).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name (e.g., \"gemini-2.5-flash\")</p> required <code>client</code> <code>Client</code> <p>Initialized Gemini client</p> required <code>response_parser</code> <code>Callable[[Any], TOutput]</code> <p>Function to parse response into TOutput</p> required <code>cached_content</code> <code>list[Content]</code> <p>Content to cache (system instructions, documents)</p> required <code>cache_ttl_seconds</code> <code>int</code> <p>Cache TTL in seconds (default: 3600 = 1 hour)</p> <code>3600</code> <code>cache_refresh_threshold</code> <code>float</code> <p>(Deprecated) Use cache_renewal_buffer_seconds instead</p> <code>0.1</code> <code>cache_renewal_buffer_seconds</code> <code>int</code> <p>Renew cache this many seconds before expiration to avoid expiration errors (default: 300 = 5 minutes)</p> <code>300</code> <code>auto_renew</code> <code>bool</code> <p>Automatically renew expired caches in execute() (default: True)</p> <code>True</code> <code>config</code> <code>GenerateContentConfig | None</code> <p>Optional generation config</p> <code>None</code> <code>include_metadata</code> <code>bool</code> <p>If True, return GeminiResponse with safety ratings (v0.3.0)</p> <code>False</code> <code>cache_tags</code> <code>dict[str, str] | None</code> <p>Tags for precise cache matching (v0.3.0). If provided, will only reuse caches with matching tags. Useful to prevent accidental cache reuse when prompt/content changes.</p> <code>None</code> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    client: \"genai.Client\",\n    response_parser: Callable[[Any], TOutput],\n    cached_content: list[\"Content\"],\n    cache_ttl_seconds: int = 3600,\n    cache_refresh_threshold: float = 0.1,  # Deprecated in favor of cache_renewal_buffer_seconds\n    cache_renewal_buffer_seconds: int = 300,  # v0.2.0: Renew 5min before expiration\n    auto_renew: bool = True,  # v0.2.0: Automatically renew expired caches\n    config: \"GenerateContentConfig | None\" = None,\n    include_metadata: bool = False,  # v0.3.0: Opt-in for safety ratings\n    cache_tags: dict[str, str] | None = None,  # v0.3.0: Tags for cache matching\n):\n    \"\"\"\n    Initialize Gemini cached strategy with automatic cache renewal (v0.2.0).\n\n    Args:\n        model: Model name (e.g., \"gemini-2.5-flash\")\n        client: Initialized Gemini client\n        response_parser: Function to parse response into TOutput\n        cached_content: Content to cache (system instructions, documents)\n        cache_ttl_seconds: Cache TTL in seconds (default: 3600 = 1 hour)\n        cache_refresh_threshold: (Deprecated) Use cache_renewal_buffer_seconds instead\n        cache_renewal_buffer_seconds: Renew cache this many seconds before expiration\n            to avoid expiration errors (default: 300 = 5 minutes)\n        auto_renew: Automatically renew expired caches in execute() (default: True)\n        config: Optional generation config\n        include_metadata: If True, return GeminiResponse with safety ratings (v0.3.0)\n        cache_tags: Tags for precise cache matching (v0.3.0). If provided, will only\n            reuse caches with matching tags. Useful to prevent accidental cache reuse\n            when prompt/content changes.\n    \"\"\"\n    if genai is None:\n        raise ImportError(\n            \"google-genai is required for GeminiCachedStrategy. \"\n            \"Install with: pip install 'async-batch-llm[gemini]'\"\n        )\n\n    # Validate cache parameters (v0.4.0)\n    if cache_renewal_buffer_seconds &gt;= cache_ttl_seconds:\n        raise ValueError(\n            f\"cache_renewal_buffer_seconds ({cache_renewal_buffer_seconds}) \"\n            f\"must be less than cache_ttl_seconds ({cache_ttl_seconds}). \"\n            f\"Typical value: 5-10 minutes (300-600 seconds). \"\n            f\"This prevents renewal from triggering before cache is created.\"\n        )\n\n    # Allow short TTLs for testing (&lt; 10 seconds), warn for production values\n    if 10 &lt;= cache_ttl_seconds &lt; 60:\n        import warnings\n\n        warnings.warn(\n            f\"cache_ttl_seconds ({cache_ttl_seconds}) is less than 60 seconds. \"\n            f\"Very short TTLs defeat the purpose of caching. \"\n            f\"Recommended minimum: 300 seconds (5 minutes).\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    self.model = model\n    self.client = client\n    self.response_parser = response_parser\n    self.cached_content = cached_content\n    self.cache_ttl_seconds = cache_ttl_seconds\n    self.cache_refresh_threshold = cache_refresh_threshold  # Deprecated\n    self.cache_renewal_buffer_seconds = cache_renewal_buffer_seconds  # v0.2.0\n    self.auto_renew = auto_renew  # v0.2.0\n    self.config = config\n    self.include_metadata = include_metadata  # v0.3.0\n    self.cache_tags = cache_tags or {}  # v0.3.0\n\n    self._cache: Any = None  # Type: CachedContent after prepare()\n    self._cache_created_at: float | None = None\n    self._cache_lock: Any = None  # v0.2.0: asyncio.Lock, created in prepare()\n\n    # Detect API version (v0.2.0)\n    self._api_version = self._detect_google_genai_version()\n    logger.debug(f\"Detected google-genai API version: {self._api_version}\")\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy.cleanup","title":"cleanup  <code>async</code>","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Cleanup hook - preserves cache for reuse by default (v0.2.0).</p> <p>By default, this method does NOT delete the cache. The cache remains active until its TTL expires, allowing reuse across multiple runs within the TTL window (e.g., 1 hour).</p> <p>This enables significant cost savings when running multiple batches: - First run: Creates cache, pays full cost - Subsequent runs (within TTL): Reuse cache, 70-90% cost reduction</p> <p>To delete the cache immediately (e.g., for cleanup in tests), call:     await strategy.delete_cache()</p> <p>See docs/GEMINI_INTEGRATION.md for cache lifecycle best practices.</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def cleanup(self) -&gt; None:\n    \"\"\"\n    Cleanup hook - preserves cache for reuse by default (v0.2.0).\n\n    By default, this method does NOT delete the cache. The cache remains\n    active until its TTL expires, allowing reuse across multiple runs\n    within the TTL window (e.g., 1 hour).\n\n    This enables significant cost savings when running multiple batches:\n    - First run: Creates cache, pays full cost\n    - Subsequent runs (within TTL): Reuse cache, 70-90% cost reduction\n\n    To delete the cache immediately (e.g., for cleanup in tests), call:\n        await strategy.delete_cache()\n\n    See docs/GEMINI_INTEGRATION.md for cache lifecycle best practices.\n    \"\"\"\n    if self._cache:\n        logger.info(\n            f\"Leaving cache active for reuse: {self._cache.name} \"\n            f\"(TTL: {self.cache_ttl_seconds}s, will expire naturally)\"\n        )\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy.delete_cache","title":"delete_cache  <code>async</code>","text":"<pre><code>delete_cache() -&gt; None\n</code></pre> <p>Explicitly delete the Gemini cache (v0.2.0).</p> <p>Call this when you want to immediately delete the cache instead of letting it expire naturally. Useful for: - Test cleanup - One-off batch jobs where reuse isn't needed - Updating cached content (delete old, create new)</p> Example <p>strategy = GeminiCachedStrategy(...)</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def delete_cache(self) -&gt; None:\n    \"\"\"\n    Explicitly delete the Gemini cache (v0.2.0).\n\n    Call this when you want to immediately delete the cache instead of\n    letting it expire naturally. Useful for:\n    - Test cleanup\n    - One-off batch jobs where reuse isn't needed\n    - Updating cached content (delete old, create new)\n\n    Example:\n        strategy = GeminiCachedStrategy(...)\n        # ... use strategy ...\n        await strategy.delete_cache()  # Explicit cleanup\n    \"\"\"\n    if self._cache:\n        try:\n            await self.client.aio.caches.delete(name=self._cache.name)\n            logger.info(f\"Deleted Gemini cache: {self._cache.name}\")\n            self._cache = None\n            self._cache_created_at = None\n        except Exception as e:\n            logger.warning(\n                f\"Failed to delete Gemini cache '{self._cache.name}': {e}. \"\n                \"Cache may have already expired or been deleted.\"\n            )\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy.delete_cache--use-strategy","title":"... use strategy ...","text":"<p>await strategy.delete_cache()  # Explicit cleanup</p>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(prompt: str, attempt: int, timeout: float, state: RetryState | None = None) -&gt; tuple[TOutput | GeminiResponse[TOutput], TokenUsage]\n</code></pre> <p>Execute Gemini API call with automatic cache renewal (v0.2.0).</p> <p>Note: timeout parameter is provided for information but timeout enforcement is handled by the framework wrapping this call in asyncio.wait_for().</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the LLM</p> required <code>attempt</code> <code>int</code> <p>Which retry attempt this is (1, 2, 3, ...)</p> required <code>timeout</code> <code>float</code> <p>Maximum time to wait for response (seconds)</p> required <code>state</code> <code>RetryState | None</code> <p>Optional retry state (v0.3.0, unused by this strategy)</p> <code>None</code> <p>Returns:</p> Type Description <code>TOutput | GeminiResponse[TOutput]</code> <p>Tuple of (output, token_usage) where output is either:</p> <code>TokenUsage</code> <ul> <li>TOutput if include_metadata=False (default)</li> </ul> <code>tuple[TOutput | GeminiResponse[TOutput], TokenUsage]</code> <ul> <li>GeminiResponse[TOutput] if include_metadata=True (v0.3.0)</li> </ul> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def execute(\n    self, prompt: str, attempt: int, timeout: float, state: RetryState | None = None\n) -&gt; tuple[TOutput | GeminiResponse[TOutput], TokenUsage]:\n    \"\"\"Execute Gemini API call with automatic cache renewal (v0.2.0).\n\n    Note: timeout parameter is provided for information but timeout enforcement\n    is handled by the framework wrapping this call in asyncio.wait_for().\n\n    Args:\n        prompt: The prompt to send to the LLM\n        attempt: Which retry attempt this is (1, 2, 3, ...)\n        timeout: Maximum time to wait for response (seconds)\n        state: Optional retry state (v0.3.0, unused by this strategy)\n\n    Returns:\n        Tuple of (output, token_usage) where output is either:\n        - TOutput if include_metadata=False (default)\n        - GeminiResponse[TOutput] if include_metadata=True (v0.3.0)\n    \"\"\"\n    # Check and renew cache if expired (proactive renewal to avoid errors)\n    if self.auto_renew and self._is_cache_expired():\n        logger.info(\n            \"Cache expired or about to expire, renewing before API call \"\n            f\"(age: {time.time() - (self._cache_created_at or 0):.0f}s, \"\n            f\"renewal buffer: {self.cache_renewal_buffer_seconds}s)\"\n        )\n\n        # Use lock to prevent concurrent renewal\n        if self._cache_lock is None:\n            import asyncio\n\n            self._cache_lock = asyncio.Lock()\n\n        async with self._cache_lock:\n            # Double-check after acquiring lock\n            if self._is_cache_expired():\n                # Clear cache reference to force creation of new cache\n                self._cache = None\n                self._cache_created_at = None\n                await self._find_or_create_cache()\n\n    if self._cache is None:\n        raise RuntimeError(\"Cache not initialized - prepare() was not called\")\n\n    # Make the call using the cache\n    # In google-genai v1.46+, cached_content must be passed in the config dict\n    config_with_cache = {\n        **(self.config.__dict__ if hasattr(self.config, \"__dict__\") else {}),\n        \"cached_content\": self._cache.name,\n    }\n\n    response = await self.client.aio.models.generate_content(\n        model=self.model,\n        contents=prompt,\n        config=config_with_cache,\n    )\n\n    # Extract token usage FIRST (before parsing/validation that might fail)\n    # This ensures we track tokens even when validation errors occur\n    usage_metadata = getattr(response, \"usage_metadata\", None)\n    if usage_metadata is not None:\n        input_tokens = getattr(usage_metadata, \"prompt_token_count\", 0) or 0\n        output_tokens = getattr(usage_metadata, \"candidates_token_count\", 0) or 0\n        total_tokens = getattr(usage_metadata, \"total_token_count\", 0) or 0\n        cached_tokens = (\n            getattr(usage_metadata, \"cached_content_token_count\", 0) or 0\n            if hasattr(usage_metadata, \"cached_content_token_count\")\n            else 0\n        )\n    else:\n        input_tokens = output_tokens = total_tokens = cached_tokens = 0\n\n    tokens: TokenUsage = {\n        \"input_tokens\": input_tokens,\n        \"output_tokens\": output_tokens,\n        \"total_tokens\": total_tokens,\n        \"cached_input_tokens\": cached_tokens,\n    }\n\n    # Parse output (may raise validation errors)\n    try:\n        output = self.response_parser(response)\n    except Exception as e:\n        # Attach token usage to exception so framework can track it\n        if not hasattr(e, \"__dict__\"):\n            # For built-in exceptions without __dict__, wrap in TokenTrackingError\n            wrapped = TokenTrackingError(str(e), token_usage=tokens)\n            wrapped.__cause__ = e\n            raise wrapped from e\n        else:\n            e.__dict__[\"_failed_token_usage\"] = tokens\n            raise\n\n    # Return with metadata if requested (v0.3.0)\n    if self.include_metadata:\n        safety_ratings = _extract_safety_ratings(response)\n        finish_reason = None\n        try:\n            if hasattr(response, \"candidates\") and response.candidates:\n                candidate = response.candidates[0]\n                if hasattr(candidate, \"finish_reason\"):\n                    finish_reason = str(candidate.finish_reason)\n        except Exception as e:\n            logger.warning(f\"Failed to extract finish_reason: {e}\")\n\n        return GeminiResponse(\n            output=output,\n            safety_ratings=safety_ratings,\n            finish_reason=finish_reason,\n            token_usage=tokens,\n            raw_response=response,\n        ), tokens\n\n    return output, tokens\n</code></pre>"},{"location":"api/strategies/#async_batch_llm.GeminiCachedStrategy.prepare","title":"prepare  <code>async</code>","text":"<pre><code>prepare() -&gt; None\n</code></pre> <p>Find or create the Gemini cache (v0.2.0).</p> Source code in <code>src/async_batch_llm/llm_strategies.py</code> <pre><code>async def prepare(self) -&gt; None:\n    \"\"\"Find or create the Gemini cache (v0.2.0).\"\"\"\n    await self._find_or_create_cache()\n</code></pre>"},{"location":"archive/","title":"Archived Documentation","text":"<p>This directory contains historical planning, migration, and development notes that are no longer relevant to current users but are preserved for reference and context.</p>"},{"location":"archive/#contents","title":"Contents","text":""},{"location":"archive/#implementation-plans-historical","title":"Implementation Plans (Historical)","text":"<ul> <li>IMPLEMENTATION_PLAN_V0_2.md - Planning doc for v0.2.0 development</li> <li>IMPLEMENTATION_PLAN_V0_3.md - Planning doc for v0.3.0 development</li> <li>IMPROVEMENT_PLAN.md - Improvement roadmap from v0.3.0 era</li> </ul>"},{"location":"archive/#migration-guides-pre-release-versions","title":"Migration Guides (Pre-Release Versions)","text":"<p>These migration guides documented intermediate versions during development but are no longer needed since the package will be released starting with v0.4.0.</p> <ul> <li>MIGRATION.md - Old migration hub (replaced by version-specific guides)</li> <li>MIGRATION_V0_1.md - v0.0.x \u2192 v0.1 migration (strategy pattern introduction)</li> <li>MIGRATION_V0_2.md - v0.1 \u2192 v0.2 migration (superseded)</li> <li>MIGRATION_V0_3.md - v0.2 \u2192 v0.3 migration (superseded)</li> </ul>"},{"location":"archive/#development-feedback-historical-issues","title":"Development Feedback (Historical Issues)","text":"<ul> <li>BATCH_LLM_FEEDBACK.md - Notes on rate limit test issues and race condition fixes</li> <li>BATCH_LLM_FEEDBACK_OLD.md - Notes on shared strategy pattern issues that led to v0.4.0</li> </ul>"},{"location":"archive/#current-documentation","title":"Current Documentation","text":"<p>For current documentation, see the parent <code>docs/</code> directory:</p> <ul> <li>API.md - Complete API reference (v0.4.0)</li> <li>MIGRATION_V0_4.md - Migration guide for v0.4.0 (context manager pattern)</li> <li>GEMINI_INTEGRATION.md - Gemini-specific integration guide</li> </ul> <p>Since the package is being released at v0.4.0, users will start fresh with the current API without needing migration guides from unreleased versions.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/","title":"batch-llm Development Feedback","text":""},{"location":"archive/BATCH_LLM_FEEDBACK/#phase-4-task-46-worst-case-rate-limit-tests-incomplete","title":"Phase 4 Task 4.6: Worst-Case Rate Limit Tests - Incomplete","text":""},{"location":"archive/BATCH_LLM_FEEDBACK/#status","title":"Status","text":"<p>Task 4.6 from IMPROVEMENT_PLAN.md was started but not completed due to test hanging issues.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#what-was-done","title":"What Was Done","text":"<ol> <li>Created <code>tests/test_worst_case_rate_limit.py</code> with 3 comprehensive tests:</li> <li><code>test_all_workers_hit_rate_limit_simultaneously()</code> - Tests multiple workers hitting rate limits</li> <li><code>test_cascading_rate_limits_under_high_load()</code> - Tests consecutive rate limit waves</li> <li> <p><code>test_rate_limit_with_mixed_success_and_failures()</code> - Tests partial failures</p> </li> <li> <p>Added <code>FastRateLimitClassifier</code> helper class to reduce rate limit wait time from 5 minutes to 1 second for testing</p> </li> <li> <p>Simplified the first test by removing <code>asyncio.Barrier</code> synchronization which was causing hangs</p> </li> </ol>"},{"location":"archive/BATCH_LLM_FEEDBACK/#issue-encountered","title":"Issue Encountered","text":"<p>The tests consistently hang/timeout and never complete.</p> <p>After multiple attempts with different approaches:</p> <ul> <li>Original test used <code>asyncio.Barrier</code> for perfect synchronization - hung indefinitely</li> <li>Simplified version without barrier still hangs after 5+ minutes of execution</li> <li>Even with 1-second rate limit cooldown, tests don't progress</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#root-cause-analysis-confirmed","title":"Root Cause Analysis (CONFIRMED)","text":"<p>Identified a race condition deadlock in rate limit coordination when multiple workers hit rate limits simultaneously.</p> <p>The bug occurs because:</p> <ol> <li>Worker 1 detects rate limit, calls <code>_handle_rate_limit()</code>, sets <code>_in_cooldown=True</code>, clears event</li> <li>Worker 1 performs the cooldown sleep</li> <li>Workers 2-10 detect rate limits but are still in exception handler (haven't reached <code>_handle_rate_limit()</code> yet)</li> <li>Worker 1 completes cooldown, sets <code>_in_cooldown=False</code>, sets event</li> <li>Worker 1 loops back, picks up new work, passes through <code>await self._rate_limit_event.wait()</code> (event is SET)</li> <li>Workers 2-10 finally reach <code>_handle_rate_limit()</code>, see <code>_in_cooldown=FALSE</code> (Worker 1 already reset it!)</li> <li>Worker 2 wins race, sets <code>_in_cooldown=TRUE</code>, clears event</li> <li>Worker 1 is now processing while event is CLEARED - when it needs to wait later, it deadlocks</li> <li>This cascades into complex deadlock scenarios</li> </ol> <p>Attempted Fixes (both failed):</p> <ol> <li>Made workers that detect <code>_in_cooldown=True</code> wait on the event before returning - still hangs</li> <li>Added check before calling <code>_handle_rate_limit()</code> to skip if already in cooldown - still hangs</li> </ol> <p>The fundamental issue: <code>_in_cooldown</code> is reset too early, before all workers that detected the rate limit have had a chance to synchronize. Need a generation counter or epoch-based approach to track which \"cooldown cycle\" each worker is participating in.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#attempted-fix-generation-counter-also-failed","title":"Attempted Fix - Generation Counter (ALSO FAILED)","text":"<p>Added generation counter to track cooldown cycles:</p> <ul> <li>Added <code>_cooldown_generation</code> field to track which cooldown cycle</li> <li>Workers record generation when detecting active cooldown</li> <li>Wait for THAT specific generation to complete</li> <li>Made entire check-and-decision atomic within single lock acquisition</li> </ul> <p>Result: Tests still hang, even with 1-second cooldown (after fixing rate limit strategy).</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#additional-discovery-test-configuration-error","title":"Additional Discovery - Test Configuration Error","text":"<p>The original tests were hanging because they used <code>FastRateLimitClassifier</code> but the actual cooldown duration comes from <code>RateLimitStrategy</code>, not the error classifier! Fixed by:</p> <ul> <li>Created <code>FastRateLimitStrategy(FixedDelayStrategy)</code> with 1-second cooldown</li> <li>Updated tests to use <code>rate_limit_strategy=FastRateLimitStrategy()</code> instead of <code>error_classifier=Fast RateLimitClassifier()</code></li> </ul> <p>Even after this fix, tests still hang with 1-second cooldown, confirming this is a genuine deadlock, not just slow tests.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#current-status","title":"Current Status","text":"<p>The rate limit coordination issue is complex and still unresolved after multiple fix attempts:</p> <ol> <li>Original race condition with <code>_in_cooldown</code> being reset too early</li> <li>Attempt 1: Make workers wait when detecting <code>_in_cooldown=True</code> - failed</li> <li>Attempt 2: Add pre-check before calling <code>_handle_rate_limit()</code> - failed</li> <li>Attempt 3: Generation counter approach - still fails</li> </ol> <p>The generation counter implementation is theoretically sound but something else is causing the deadlock. Possible issues:</p> <ul> <li>Event might not be getting set properly after cooldown</li> <li>Workers might be waiting on wrong generation</li> <li>Multiple workers re-queueing same item causing queue to never empty</li> <li>Slow-start delays compounding with test timeouts</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#recommendation","title":"Recommendation","text":"<p>This issue requires deeper investigation with:</p> <ol> <li>Detailed debug logging throughout rate limit handling</li> <li>Simpler reproduction - test with 2 workers, minimal items</li> <li>Event state tracking - log every <code>.set()</code> and <code>.clear()</code> call</li> <li>Queue state tracking - log queue size and task_done() calls</li> <li>Consider alternative approaches:</li> <li>Use a semaphore instead of Event</li> <li>Redesign rate limit handling to not re-queue items</li> <li>Have coordinator worker handle all rate-limited items</li> </ol>"},{"location":"archive/BATCH_LLM_FEEDBACK/#files-modified","title":"Files Modified","text":"<ul> <li><code>src/batch_llm/parallel.py</code> - Added generation counter, made atomic check</li> <li><code>tests/test_worst_case_rate_limit.py</code> - Created with <code>FastRateLimitStrategy</code></li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#next-steps-for-future-development","title":"Next Steps for Future Development","text":"<p>SHORT TERM: Mark these tests as <code>@pytest.mark.skip</code> with explanation - the framework needs architectural changes to handle this worst-case scenario.</p> <p>LONG TERM: Redesign rate limit coordination:</p> <ul> <li>Consider using a dedicated rate limit manager</li> <li>Separate \"waiting for cooldown\" from \"item needs retry\"</li> <li>Only one worker coordinates cooldown, others just wait</li> <li>Avoid re-queueing items during rate limit (handle differently)</li> </ul> <p>The test file structure and approach are sound - the framework's rate limit coordination needs architectural improvements to handle simultaneous rate limits from multiple workers reliably.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#additional-issues-found","title":"Additional Issues Found","text":""},{"location":"archive/BATCH_LLM_FEEDBACK/#mypy-type-errors-in-llm_strategiespy-not-fixed","title":"Mypy Type Errors in llm_strategies.py (NOT FIXED)","text":"<p>Found 12 mypy type errors in <code>src/batch_llm/llm_strategies.py</code>:</p> <ul> <li>Return type incompatibility with <code>GeminiResponse[TOutput]</code> vs <code>TOutput</code></li> <li><code>TokenUsage</code> type mismatches (TypedDict vs dict[str, int])</li> <li><code>AsyncPager</code> iteration issues</li> <li>Type ignore comment issues</li> </ul> <p>These should be addressed but are not blocking - the code runs correctly despite the type errors. Recommend fixing as part of general code quality improvements.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#strategy-lifecycle-management-target-v040","title":"Strategy Lifecycle Management (Target: v0.4.0)","text":""},{"location":"archive/BATCH_LLM_FEEDBACK/#problem-statement","title":"Problem Statement","text":"<p>Currently, <code>ParallelBatchProcessor</code> doesn't call <code>prepare()</code> or <code>cleanup()</code> on strategy instances, leaving lifecycle management entirely to users. This creates ambiguity:</p> <ol> <li>When should <code>prepare()</code> be called? Before first use? Once per unique strategy instance?</li> <li>When should <code>cleanup()</code> be called? After each item? After all items? At processor shutdown?</li> <li>How should strategies be shared? Users create shared instances for efficiency    (e.g., cached prompts), but the framework provides no lifecycle support.</li> </ol> <p>This leads to:</p> <ul> <li>Resource leaks: Strategies with expensive resources aren't cleaned up reliably</li> <li>Inefficient workarounds: Users manually manage prepare/cleanup outside framework</li> <li>Inconsistent behavior: Different users implement different patterns</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#proposed-solution-context-manager-lifecycle-hybrid-approach","title":"Proposed Solution: Context Manager Lifecycle (Hybrid Approach)","text":"<p>Implement per-processor lifecycle management using Python's context manager pattern:</p> <pre><code># Recommended pattern (with automatic cleanup)\nasync with ParallelBatchProcessor(...) as processor:\n    await processor.add_work(LLMWorkItem(strategy=shared_strategy, ...))\n    await processor.add_work(LLMWorkItem(strategy=shared_strategy, ...))\n    result = await processor.process_all()\n    # Strategies automatically cleaned up on exit\n\n# Backward compatible pattern (no cleanup)\nprocessor = ParallelBatchProcessor(...)\nawait processor.add_work(...)\nresult = await processor.process_all()\n# No cleanup (preserves existing behavior)\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK/#key-design-points","title":"Key Design Points","text":"<ol> <li>Track strategies in <code>add_work()</code> (no prepare yet, just tracking)</li> <li>Prepare all strategies in first <code>process_all()</code> call (has all strategies, no race conditions)</li> <li>Cleanup all strategies in <code>__aexit__()</code> only (when using context manager)</li> <li>Backward compatible - non-context-manager usage doesn't cleanup</li> <li>Prevent mid-processing changes - raise RuntimeError if add_work() called after    process_all() starts</li> </ol>"},{"location":"archive/BATCH_LLM_FEEDBACK/#implementation-sketch","title":"Implementation Sketch","text":"<pre><code>class ParallelBatchProcessor:\n    def __init__(self, ...):\n        self._unique_strategies: dict[int, LLMCallStrategy] = {}\n        self._prepared_strategy_ids: set[int] = set()\n        self._processing_started = False\n\n    async def add_work(self, work_item: LLMWorkItem):\n        if self._processing_started:\n            raise RuntimeError(\n                \"Cannot add work after process_all() has started\"\n            )\n\n        strategy_id = id(work_item.strategy)\n        if strategy_id not in self._unique_strategies:\n            self._unique_strategies[strategy_id] = work_item.strategy\n\n        await self._queue.put(work_item)\n        self._stats.total += 1\n\n    async def _prepare_all_strategies(self):\n        \"\"\"Idempotent - safe to call multiple times.\"\"\"\n        for strategy_id, strategy in self._unique_strategies.items():\n            if strategy_id in self._prepared_strategy_ids:\n                continue\n\n            if hasattr(strategy, 'prepare'):\n                await strategy.prepare()\n                self._prepared_strategy_ids.add(strategy_id)\n\n    async def process_all(self) -&gt; BatchResult:\n        self._processing_started = True\n        await self._prepare_all_strategies()\n\n        try:\n            result = await self._process_items()\n            return result\n        finally:\n            pass  # Cleanup in __aexit__\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        for strategy in self._unique_strategies.values():\n            if hasattr(strategy, 'cleanup'):\n                try:\n                    await strategy.cleanup()\n                except Exception as e:\n                    logger.warning(f\"Cleanup failed: {e}\")\n        return False\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK/#benefits","title":"Benefits","text":"<ol> <li>\u2705 Clear lifecycle: prepare on first use, cleanup on exit</li> <li>\u2705 No race conditions: prepare happens atomically before processing</li> <li>\u2705 Backward compatible: existing code works unchanged</li> <li>\u2705 Pythonic: follows standard context manager pattern</li> <li>\u2705 Supports sharing: shared strategies prepared once, cleaned up once</li> <li>\u2705 Fail-fast: prepare errors happen before processing starts</li> </ol>"},{"location":"archive/BATCH_LLM_FEEDBACK/#strategy-contract","title":"Strategy Contract","text":"<pre><code>class LLMCallStrategy(ABC):\n    async def prepare(self) -&gt; None:\n        \"\"\"\n        Called once per processor instance before first use.\n\n        MUST be idempotent - may be called multiple times if strategy\n        is used across multiple processors.\n\n        Use cases:\n        - Create expensive resources (caches, connections)\n        - One-time initialization\n        \"\"\"\n        pass\n\n    async def cleanup(self) -&gt; None:\n        \"\"\"\n        Called once per processor instance after all work completes.\n\n        For long-lived resources intended to be reused across batches\n        (like production caches), make this a no-op.\n\n        Only called when using context manager.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK/#testing-strategy","title":"Testing Strategy","text":"<p>Key tests needed:</p> <ul> <li>Shared strategy prepared once, cleaned up once</li> <li>Multiple unique strategies each get prepare/cleanup</li> <li>Cleanup happens even on processing error</li> <li>Cleanup error doesn't fail batch (logged)</li> <li>Backward compatibility (no context manager = no cleanup)</li> <li>Cannot add_work() after process_all() starts</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#migration-guide","title":"Migration Guide","text":"<p>For Users: Just wrap in <code>async with</code>:</p> <pre><code># Before\nprocessor = ParallelBatchProcessor(...)\nawait processor.add_work(...)\nresult = await processor.process_all()\n\n# After (recommended)\nasync with ParallelBatchProcessor(...) as processor:\n    await processor.add_work(...)\n    result = await processor.process_all()\n</code></pre> <p>For Strategy Authors: If your strategy needs cleanup, implement <code>cleanup()</code>. For production caches that should persist, make cleanup a no-op.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK/#open-questions","title":"Open Questions","text":"<ol> <li>Should we support multiple <code>process_all()</code> calls?</li> <li>Proposed: No (raises RuntimeError)</li> <li> <p>Rationale: Simpler semantics, encourages new processor for new batch</p> </li> <li> <p>Should cleanup errors fail the batch?</p> </li> <li>Proposed: No (log warnings only)</li> <li> <p>Rationale: Cleanup failures shouldn't invalidate successful work</p> </li> <li> <p>Should we add per-item <code>on_item_complete()</code> hook?</p> </li> <li>Proposed: Wait for user demand (YAGNI)</li> <li>Rationale: Easier to add later than remove</li> </ol>"},{"location":"archive/BATCH_LLM_FEEDBACK/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li>[ ] Add tracking fields to <code>__init__</code></li> <li>[ ] Track strategies in <code>add_work()</code> with RuntimeError check</li> <li>[ ] Add <code>_prepare_all_strategies()</code> helper (idempotent)</li> <li>[ ] Call prepare in <code>process_all()</code></li> <li>[ ] Implement <code>__aenter__</code> and <code>__aexit__</code></li> <li>[ ] Add comprehensive tests</li> <li>[ ] Document strategy contract</li> <li>[ ] Update all examples to use context manager</li> <li>[ ] Add migration guide</li> <li>[ ] Update CLAUDE.md</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK/#status_1","title":"Status","text":"<p>Design phase - Feedback collected, ready for implementation in v0.4.0</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/","title":"batch-llm Feedback: Shared Strategy Pattern Issues","text":""},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#context","title":"Context","text":"<p>While migrating <code>enrich_works.py</code> to use the standalone <code>batch-llm</code> package (v0.1.0), we encountered several issues related to sharing a single strategy instance across multiple work items for cost optimization with Gemini prompt caching.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#use-case-gemini-prompt-caching","title":"Use Case: Gemini Prompt Caching","text":"<p>Our enrichment pipeline processes thousands of books, each with unique data but sharing the same prompt instructions (~500 tokens). Gemini offers 90% discount on cached tokens, providing 70-75% cost reduction overall.</p> <p>Goal: Create one Gemini cache and share it across all work items.</p> <p>Implementation:</p> <pre><code># Create one shared strategy\nstrategy = create_cached_strategy(\n    model_name=model_name,\n    cache_ttl_seconds=3600,\n)\n\n# Use same strategy for all work items\nfor work in works:\n    work_item = LLMWorkItem(\n        item_id=work.work_key,\n        strategy=strategy,  # SHARED instance\n        prompt=format_work_prompt(work),\n        context=work,\n    )\n    await processor.add_work(work_item)\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issues-encountered","title":"Issues Encountered","text":""},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-1-multiple-prepare-calls-on-shared-strategy","title":"Issue 1: Multiple <code>prepare()</code> Calls on Shared Strategy","text":"<p>Problem: <code>ParallelBatchProcessor</code> calls <code>strategy.prepare()</code> once per work item, even when the same strategy instance is reused. With concurrent workers, multiple workers call <code>prepare()</code> simultaneously on the shared strategy.</p> <p>Impact:</p> <ul> <li>Created 2+ Gemini caches instead of 1</li> <li>Wasted API quota and cost</li> <li>Negated caching benefits</li> </ul> <p>Our Workaround:</p> <pre><code>class GeminiEnrichmentCachedStrategy:\n    def __init__(self, ...):\n        self._cache = None\n        self._cache_lock = asyncio.Lock()\n\n    async def prepare(self) -&gt; None:\n        \"\"\"Create cache only once, even with concurrent calls.\"\"\"\n        async with self._cache_lock:\n            if self._cache is not None:\n                return  # Already prepared\n\n            # Create cache...\n</code></pre> <p>Why This Works But Isn't Ideal:</p> <ul> <li>Requires every strategy to implement idempotency guards</li> <li>Lock overhead on every prepare() call</li> <li>Unclear from framework whether strategies should be shared</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-2-api-version-incompatibility","title":"Issue 2: API Version Incompatibility","text":"<p>Problem: <code>batch-llm</code> v0.1.0's <code>GeminiCachedStrategy</code> uses an older Gemini API:</p> <pre><code># batch-llm v0.1.0 (works with older google-genai)\nself._cache = await self.client.aio.caches.create(\n    model=self.model,\n    contents=self.cached_content,\n    ttl=f\"{self.cache_ttl_seconds}s\",\n)\n</code></pre> <p>But <code>google-genai</code> v1.46+ changed the API:</p> <pre><code># google-genai v1.46+ (required for newer features)\nself._cache = await self.client.aio.caches.create(\n    model=self.model,\n    config=CreateCachedContentConfig(\n        contents=self.cached_content,\n        ttl=f\"{self.cache_ttl_seconds}s\",\n    ),\n)\n</code></pre> <p>Impact: Forced to write custom strategy instead of using built-in <code>GeminiCachedStrategy</code>.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-3-missing-safety-ratings-in-geminicachedstrategy","title":"Issue 3: Missing Safety Ratings in <code>GeminiCachedStrategy</code>","text":"<p>Problem: The built-in <code>GeminiCachedStrategy</code> uses a <code>response_parser</code> function that only receives the parsed response, not the raw Gemini response object.</p> <p>Impact: Cannot access Gemini safety ratings (harassment, hate speech, etc.) which are critical for content filtering.</p> <p>Our Need:</p> <pre><code># We need access to raw response\nresponse = await client.aio.models.generate_content(...)\n\n# Extract safety ratings from response metadata\nsafety_ratings = {}\nfor rating in response.candidates[0].safety_ratings:\n    safety_ratings[rating.category] = rating.probability\n\n# AND parse the content\nmetadata = parse_json_response(response.text)\n</code></pre> <p>Current GeminiCachedStrategy Limitation:</p> <pre><code># response_parser only gets parsed response, not raw response object\ndef response_parser(response) -&gt; CanonicalBookMetadata:\n    # No access to response.candidates[0].safety_ratings\n    return CanonicalBookMetadata(**json.loads(response.text))\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#recommendations-for-batch-llm","title":"Recommendations for batch-llm","text":""},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#1-document-shared-strategy-pattern","title":"1. Document Shared Strategy Pattern","text":"<p>Add to README:</p> <pre><code>## Sharing Strategies Across Work Items\n\nFor cost optimization (e.g., Gemini prompt caching), you may want to share\na single strategy instance across all work items:\n\n```python\n# Create one strategy\nstrategy = GeminiCachedStrategy(...)\n\n# Reuse for all items\nfor item in items:\n    work_item = LLMWorkItem(\n        strategy=strategy,  # Shared instance\n        prompt=format_prompt(item),\n        ...\n    )\n</code></pre> <p>Important: When sharing strategies, ensure your <code>prepare()</code> method is idempotent and thread-safe. The framework calls <code>prepare()</code> once per work item, but shared strategies should only initialize once:</p> <pre><code>class MySharedStrategy:\n    def __init__(self):\n        self._prepared = False\n        self._lock = asyncio.Lock()\n\n    async def prepare(self) -&gt; None:\n        async with self._lock:\n            if self._prepared:\n                return\n            # ... expensive initialization ...\n            self._prepared = True\n</code></pre> <pre><code>### 2. Add `PrepareOnce` Mixin or Decorator\n\n**Proposed API**:\n\n```python\nfrom async_batch_llm.mixins import PrepareOnceMixin\n\nclass MyStrategy(PrepareOnceMixin):\n    async def _prepare_once(self) -&gt; None:\n        \"\"\"Called exactly once, even with concurrent prepare() calls.\"\"\"\n        # Expensive initialization here\n        pass\n\n    # Framework calls prepare(), mixin ensures _prepare_once() runs once\n</code></pre> <p>Or as a decorator:</p> <pre><code>from async_batch_llm.decorators import prepare_once\n\nclass MyStrategy:\n    @prepare_once\n    async def prepare(self) -&gt; None:\n        \"\"\"Automatically guarded - only runs once.\"\"\"\n        # Expensive initialization\n        pass\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#3-update-geminicachedstrategy-for-google-genai-v146","title":"3. Update <code>GeminiCachedStrategy</code> for google-genai v1.46+","text":"<p>Current Code (batch-llm v0.1.0):</p> <pre><code>self._cache = await self.client.aio.caches.create(\n    model=self.model,\n    contents=self.cached_content,\n    ttl=f\"{self.cache_ttl_seconds}s\",\n)\n</code></pre> <p>Updated for v1.46+:</p> <pre><code>from google.genai.types import CreateCachedContentConfig\n\nself._cache = await self.client.aio.caches.create(\n    model=self.model,\n    config=CreateCachedContentConfig(\n        contents=self.cached_content,\n        ttl=f\"{self.cache_ttl_seconds}s\",\n    ),\n)\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#4-add-raw-response-access-to-geminicachedstrategy","title":"4. Add Raw Response Access to <code>GeminiCachedStrategy</code>","text":"<p>Current Limitation:</p> <pre><code>class GeminiCachedStrategy:\n    def __init__(\n        self,\n        response_parser: Callable[[Any], TOutput],  # Only gets parsed response\n        ...\n    ):\n</code></pre> <p>Proposed Enhancement:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass GeminiResponse(Generic[TOutput]):\n    \"\"\"Container for parsed output and raw response metadata.\"\"\"\n    output: TOutput\n    safety_ratings: dict[str, str] | None\n    finish_reason: str | None\n    token_usage: dict[str, int]\n    raw_response: Any  # Full response object\n\nclass GeminiCachedStrategy:\n    def __init__(\n        self,\n        response_parser: Callable[[Any], TOutput],\n        include_metadata: bool = False,  # Opt-in for backward compatibility\n        ...\n    ):\n        self.include_metadata = include_metadata\n\n    async def execute(self, ...) -&gt; tuple[TOutput | GeminiResponse[TOutput], dict[str, int]]:\n        response = await self.client.aio.models.generate_content(...)\n\n        output = self.response_parser(response)\n\n        if self.include_metadata:\n            # Extract safety ratings, finish reason, etc.\n            safety_ratings = self._extract_safety_ratings(response)\n            return GeminiResponse(\n                output=output,\n                safety_ratings=safety_ratings,\n                finish_reason=response.candidates[0].finish_reason,\n                token_usage=token_usage,\n                raw_response=response,\n            ), token_usage\n\n        return output, token_usage\n</code></pre> <p>Usage:</p> <pre><code>strategy = GeminiCachedStrategy(\n    response_parser=parse_metadata,\n    include_metadata=True,  # Get full response wrapper\n)\n\n# Later, in post-processor:\nresult = work_item_result.output  # GeminiResponse\nmetadata = result.output  # Parsed CanonicalBookMetadata\nsafety_ratings = result.safety_ratings  # Safety ratings!\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#alternative-framework-level-strategy-lifecycle","title":"Alternative: Framework-Level Strategy Lifecycle","text":"<p>More Ambitious Approach: Framework manages strategy lifecycle explicitly.</p> <pre><code># Processor manages strategy preparation\nprocessor = ParallelBatchProcessor(...)\n\n# Register strategies before adding work\nawait processor.register_strategy(\"enrichment\", strategy)\n\n# Work items reference strategies by name\nwork_item = LLMWorkItem(\n    strategy_id=\"enrichment\",  # Reference, not instance\n    ...\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Framework calls <code>prepare()</code> exactly once per registered strategy</li> <li>Framework calls <code>cleanup()</code> automatically at end</li> <li>Clear ownership and lifecycle</li> </ul> <p>Drawback: More complex API change</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-4-cached-token-tracking-not-exposed","title":"Issue 4: Cached Token Tracking Not Exposed","text":"<p>Problem: The batch-llm framework's <code>BatchResult</code> doesn't track cached tokens, only <code>total_input_tokens</code> and <code>total_output_tokens</code>.</p> <p>Impact:</p> <ul> <li>Can't measure cache effectiveness (70-75% of tokens are cached)</li> <li>Can't calculate actual cost savings</li> <li>Can't verify caching is working</li> </ul> <p>Our Workaround:</p> <pre><code># In enrich_works.py - manually extract and sum cached tokens\ntotal_cached_tokens = sum(\n    r.token_usage.get(\"cached_tokens\", 0)\n    for r in result.results\n    if r.token_usage is not None\n)\ntoken_usage.add(result.total_input_tokens, result.total_output_tokens, total_cached_tokens)\n</code></pre> <p>Why This Is Needed: Gemini returns <code>cached_content_token_count</code> in <code>usage_metadata</code>, but batch-llm doesn't aggregate it into <code>BatchResult</code>.</p> <p>Recommendation: Add <code>total_cached_tokens</code> to <code>BatchResult</code>:</p> <pre><code>@dataclass\nclass BatchResult:\n    total_input_tokens: int\n    total_output_tokens: int\n    total_cached_tokens: int = 0  # NEW: Track cached tokens separately\n    ...\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-5-cache-lifecycle-unclear","title":"Issue 5: Cache Lifecycle Unclear","text":"<p>Problem: The framework calls <code>cleanup()</code> at the end of processing, but it's unclear whether this hook should:</p> <ul> <li>Delete resources (caches, connections) for cleanup, or</li> <li>Preserve resources (caches) for reuse across runs</li> </ul> <p>Impact:</p> <ul> <li>Initially implemented cleanup() to delete caches</li> <li>This prevented reuse and wasted the 1-hour TTL</li> <li>Cost savings disappeared because each run created a new cache</li> </ul> <p>Our Solution: Changed cleanup() to NOT delete caches:</p> <pre><code>async def cleanup(self) -&gt; None:\n    \"\"\"Don't delete cache - let it expire naturally for reuse.\"\"\"\n    if self._cache:\n        logger.info(\n            f\"Leaving cache active for reuse: {self._cache.name} \"\n            f\"(expires in {self.cache_ttl_seconds}s from creation)\"\n        )\n</code></pre> <p>Result: Multiple runs within 1 hour now reuse the same cache, providing 70-75% cost savings.</p> <p>Extension - Cache Renewal for Long-Running Jobs:</p> <p>For pipelines that run longer than the cache TTL (e.g., 3+ hours), we also need to:</p> <ul> <li>Track cache creation time</li> <li>Check if cache is expired before each API call</li> <li>Automatically renew expired caches</li> </ul> <p>This requires additional strategy code that batch-llm doesn't provide:</p> <pre><code>def _is_cache_expired(self) -&gt; bool:\n    \"\"\"Check if cache has expired.\"\"\"\n    if self._cache is None or self._cache_created_at is None:\n        return True\n    cache_age_seconds = time.time() - self._cache_created_at\n    renewal_buffer = 300  # Renew 5 min before expiration\n    return cache_age_seconds &gt;= (self.cache_ttl_seconds - renewal_buffer)\n\nasync def execute(self, prompt: str, attempt: int, timeout: float):\n    \"\"\"Execute with automatic cache renewal.\"\"\"\n    # Check and renew cache if expired\n    if self._is_cache_expired():\n        await self.prepare()  # Creates new cache or reuses existing\n    # ... continue with API call\n</code></pre> <p>CRITICAL BUG - Cache Reuse Creation Time:</p> <p>When reusing an existing cache (found via list/search), you MUST use the cache's actual creation time, not the current time:</p> <pre><code># \u274c WRONG - Causes cache expiration to fail\nfor cache in caches:\n    if cache.model == target_model:\n        self._cache = cache\n        self._cache_created_at = time.time()  # BUG: Uses current time!\n        return\n\n# \u2705 CORRECT - Uses actual cache creation time\nfor cache in caches:\n    if cache.model == target_model:\n        self._cache = cache\n        # Use cache's actual creation time from Google's servers\n        if hasattr(cache, 'create_time') and cache.create_time:\n            self._cache_created_at = cache.create_time.timestamp()\n        else:\n            # Fallback: assume old to trigger renewal\n            self._cache_created_at = time.time() - self.cache_ttl_seconds\n        return\n</code></pre> <p>Why This Matters:</p> <p>Example with the bug:</p> <ul> <li>Cache created at 10:19, expires at 11:19 (1-hour TTL)</li> <li>New run at 11:00 finds and reuses cache</li> <li>Bug: Sets <code>_cache_created_at = 11:00</code> (should be 10:19)</li> <li>Thinks cache expires at 12:00 (11:00 + 3600s)</li> <li>Actual expiration: 11:19</li> <li>Result: API calls fail with \"cache expired\" between 11:19-12:00</li> </ul> <p>Impact: Expiration detection fails, causing API errors that bypass proactive renewal.</p> <p>Recommendation: Document the cleanup() hook's purpose more clearly:</p> <pre><code>async def cleanup(self) -&gt; None:\n    \"\"\"\n    Cleanup hook called after all work items are processed.\n\n    Use this for:\n    - Closing connections/sessions\n    - Releasing locks\n    - Logging final metrics\n\n    Do NOT use this for:\n    - Deleting caches intended for reuse across runs\n    - Destructive cleanup that prevents resource reuse\n\n    Note: For reusable resources like Gemini caches with TTLs,\n    consider letting them expire naturally to enable cost savings\n    across multiple pipeline runs.\n    \"\"\"\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-6-cache-matching-precision","title":"Issue 6: Cache Matching Precision","text":"<p>Problem: When checking for existing caches, we need to match:</p> <ul> <li>Model name (important)</li> <li>Cache content/prompt (ideally, but hard to check without fetching)</li> </ul> <p>Our Approach: Simple model name matching with <code>.endswith()</code>:</p> <pre><code>for cache in caches:\n    # Cache model is full path: \"projects/.../models/gemini-2.5-flash-lite...\"\n    # Our model is short name: \"gemini-2.5-flash-lite-preview-09-2025\"\n    if cache.model.endswith(self.model_name.value):\n        self._cache = cache\n        logger.info(f\"Reusing existing Gemini cache: {self._cache.name}\")\n        return\n</code></pre> <p>Trade-off:</p> <ul> <li>\u2705 Simple and fast (no need to fetch cache content)</li> <li>\u26a0\ufe0f Will reuse cache even if prompt changed</li> <li>\u26a0\ufe0f If prompt changes, LLM will get inconsistent instructions</li> </ul> <p>Better Solution: Framework could support cache tagging:</p> <pre><code># When creating cache, add metadata\nself._cache = await self.client.aio.caches.create(\n    model=self.model_name.value,\n    config=CreateCachedContentConfig(...),\n    metadata={\"prompt_version\": \"v2\", \"purpose\": \"enrichment\"},  # NEW\n)\n\n# When looking for cache, match on metadata\nfor cache in caches:\n    if (cache.model.endswith(self.model_name.value) and\n        cache.metadata.get(\"prompt_version\") == \"v2\" and\n        cache.metadata.get(\"purpose\") == \"enrichment\"):\n        return cache\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-7-cache-expiration-error-handling","title":"Issue 7: Cache Expiration Error Handling","text":"<p>Problem: When caches expire on Google's servers (after TTL), subsequent API calls fail with:</p> <pre><code>400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Cache content 5025785736947826688 is expired.', 'status': 'INVALID_ARGUMENT'}}\n</code></pre> <p>This happens in multi-hour pipelines where:</p> <ul> <li>Local expiration detection doesn't catch all edge cases</li> <li>Google expires cache before local check detects it</li> <li>Workers continue using stale cache reference</li> </ul> <p>Impact:</p> <ul> <li>Pipeline failures in long-running jobs</li> <li>No automatic recovery from expired caches</li> <li>Requires manual intervention to restart</li> </ul> <p>Our Workaround: Two-part solution in custom error classifier + strategy:</p> <p>Part 1: Error Classifier (enrich_works.py)</p> <pre><code>class EnrichmentErrorClassifier(GeminiErrorClassifier):\n    def classify(self, exception: Exception) -&gt; ErrorInfo:\n        # CRITICAL: Check cache expiration FIRST, before parent class\n        # Parent class may classify ClientError differently, preventing our check\n        error_str = str(exception)\n        if \"Cache content\" in error_str and \"expired\" in error_str.lower():\n            return ErrorInfo(\n                is_retryable=True,  # Retry - cache will be renewed\n                is_rate_limit=False,\n                is_timeout=False,\n                error_category=\"cache_expired\",\n            )\n\n        # ... other custom error checks (JSON, validation, etc.) ...\n\n        # Delegate to parent class LAST\n        return super().classify(exception)\n</code></pre> <p>Part 2: Proactive Renewal (enrich.py)</p> <pre><code>async def execute(self, prompt: str, attempt: int, timeout: float):\n    \"\"\"Execute with proactive cache renewal.\"\"\"\n    # Check if cache needs renewal before processing\n    if self._is_cache_expired():\n        logger.info(\"Cache expired or about to expire, renewing before next API call\")\n        # Force cache renewal by clearing current cache reference\n        async with self._cache_lock:\n            self._cache = None\n            self._cache_created_at = None\n        await self.prepare()  # Creates new cache or reuses existing\n\n    # ... continue with API call\n</code></pre> <p>Why This Works:</p> <ul> <li>Ordering is critical: Cache check must come BEFORE parent class delegation</li> <li>Error classifier ensures expired cache errors are retried (not permanent failures)</li> <li>Proactive renewal (5-min buffer) prevents most expiration errors</li> <li>Forced cache reset ensures prepare() creates/finds new cache instead of reusing stale ref</li> <li>Combined approach handles both proactive and reactive scenarios</li> </ul> <p>Gotcha: If cache expiration check happens after <code>super().classify()</code>, the parent class may classify the <code>ClientError</code> as non-retryable before your custom check runs. Always check cache expiration FIRST.</p> <p>Recommendation for batch-llm:</p> <p>Option 1: Built-in Cache Expiration Handling (preferred)</p> <p>Add cache expiration detection to <code>GeminiCachedStrategy</code>:</p> <pre><code>class GeminiCachedStrategy:\n    def __init__(self, cache_ttl_seconds: int = 3600, ...):\n        self.cache_ttl_seconds = cache_ttl_seconds\n        self.cache_renewal_buffer = 300  # Renew 5min before expiration\n        self._cache_created_at: float | None = None\n\n    def _is_cache_expired(self) -&gt; bool:\n        \"\"\"Check if cache has expired or is about to expire.\"\"\"\n        if self._cache is None or self._cache_created_at is None:\n            return True\n        import time\n        cache_age = time.time() - self._cache_created_at\n        return cache_age &gt;= (self.cache_ttl_seconds - self.cache_renewal_buffer)\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        \"\"\"Execute with automatic cache renewal.\"\"\"\n        # Renew cache if expired or about to expire\n        if self._is_cache_expired():\n            async with self._cache_lock:\n                self._cache = None\n                self._cache_created_at = None\n            await self.prepare()\n\n        # ... continue with API call\n</code></pre>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#option-2-document-error-classification-pattern","title":"Option 2: Document Error Classification Pattern","text":"<p>Add to README/documentation:</p> <pre><code>## Handling Cache Expiration in Long Pipelines\n\nFor pipelines running longer than your cache TTL (e.g., 3+ hours with 1-hour cache):\n\n1. **Add custom error classifier** to detect and retry cache expiration errors:\n\n   ```python\n   class MyCachedErrorClassifier(GeminiErrorClassifier):\n       def classify(self, exception: Exception) -&gt; ErrorInfo:\n           # IMPORTANT: Check BEFORE calling super().classify()\n           # Parent may classify ClientError as non-retryable\n           error_str = str(exception)\n           if \"Cache content\" in error_str and \"expired\" in error_str.lower():\n               return ErrorInfo(is_retryable=True, error_category=\"cache_expired\")\n           return super().classify(exception)\n\n   processor = ParallelBatchProcessor(\n       error_classifier=MyCachedErrorClassifier()\n   )\n   ```\n\n   **Critical**: Check cache expiration BEFORE delegating to parent class.\n\n1. **Implement proactive renewal** in your strategy's execute() method with a 5-minute buffer.\n</code></pre> <p>Benefit of Option 1: Users get cache expiration handling out-of-the-box, no custom code needed.</p> <p>Benefit of Option 2: Simpler framework, users can customize renewal behavior.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#issue-8-per-work-item-state-for-advanced-retry-strategies","title":"Issue 8: Per-Work-Item State for Advanced Retry Strategies","text":"<p>Problem: Cannot implement sophisticated retry strategies that need per-work-item state across retry attempts.</p> <p>Use Case: Multi-stage validation recovery with network error resilience:</p> <ol> <li>Stage 1: Full prompt at temperature 0.0</li> <li>Stage 2: Partial recovery at temperature 0.0 (fix only failed fields - 81% cheaper)</li> <li>Stage 3: Full prompt at temperature 0.25</li> <li>Stage 4: Partial recovery at temperature 0.25</li> </ol> <p>Desired Retry Logic:</p> <ul> <li>ValidationError \u2192 advance to next stage, increment total_prompts</li> <li>Network/timeout error \u2192 retry same stage, increment total_prompts</li> <li>Fail when total_prompts &gt; threshold (e.g., 5)</li> </ul> <p>Why This Is Better:</p> <ul> <li>\u2705 Partial recovery attempts (stages 2, 4) are 81% cheaper than full retries</li> <li>\u2705 Network errors at stage 4 only retry stage 4 (not stages 1-3)</li> <li>\u2705 Total prompt limit prevents runaway costs from repeated network errors</li> <li>\u2705 Progressive temperature only when partial recovery fails</li> </ul> <p>Current Framework Limitation: Cannot persist state between retry attempts:</p> <pre><code>class MyStrategy:\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Problem: Where to store?\n        # - Which stage are we on? (1-4)\n        # - How many total prompts have we made?\n        # - What was the ValidationError from previous stage?\n        # - What was the partial_data for partial recovery?\n\n        # Strategy instance is SHARED across all work items\n        # \u2192 Can't use instance variables (state collision)\n\n        # execute() only receives (prompt, attempt, timeout)\n        # \u2192 No work item context or per-item state\n</code></pre> <p>Root Causes:</p> <ol> <li>Strategy shared across work items - Instance variables cause state collision</li> <li>execute() signature - No access to work item context or per-item state</li> <li>on_error() signature - No access to work item context to store error details</li> <li>No state parameter - Framework doesn't provide state storage/retrieval mechanism</li> </ol> <p>Recommendation: Add per-work-item state management to framework.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#proposed-api-state-parameter","title":"Proposed API: State Parameter","text":"<p>Add optional <code>state</code> parameter that strategies can use to persist data across retry attempts:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any\n\n@dataclass\nclass RetryState:\n    \"\"\"Mutable state that persists across retry attempts for a work item.\"\"\"\n    data: dict[str, Any]  # User-defined state\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        return self.data.get(key, default)\n\n    def set(self, key: str, value: Any) -&gt; None:\n        self.data[key] = value\n\nclass Strategy(Protocol):\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,  # NEW: Per-work-item state\n    ) -&gt; tuple[TOutput, dict[str, int]]:\n        ...\n\n    async def on_error(\n        self,\n        error: Exception,\n        attempt: int,\n        state: RetryState | None = None,  # NEW: Access to state\n    ) -&gt; None:\n        ...\n</code></pre> <p>Usage Example: Multi-stage validation recovery</p> <pre><code>class MultiStageStrategy:\n    async def execute(self, prompt: str, attempt: int, timeout: float, state: RetryState | None = None):\n        # Initialize state on first attempt\n        if state is None or attempt == 1:\n            state = RetryState(data={\"stage\": 1, \"total_prompts\": 0})\n\n        stage = state.get(\"stage\", 1)\n        total_prompts = state.get(\"total_prompts\", 0)\n\n        # Enforce total prompt limit\n        if total_prompts &gt;= 5:\n            raise ValueError(\"Exceeded maximum prompts (5)\")\n\n        try:\n            if stage in [1, 3]:\n                # Full prompt stages\n                temp = 0.0 if stage == 1 else 0.25\n                result = await self._call_gemini(prompt, temp)\n                state.set(\"total_prompts\", total_prompts + 1)\n                return result\n\n            elif stage in [2, 4]:\n                # Partial recovery stages\n                temp = 0.0 if stage == 2 else 0.25\n                last_error = state.get(\"last_validation_error\")\n                partial_data = state.get(\"partial_data\")\n                result = await self._partial_recovery(last_error, partial_data, temp)\n                state.set(\"total_prompts\", total_prompts + 1)\n                return result\n\n        except ValidationError as e:\n            # Validation error: advance to next stage\n            state.set(\"stage\", stage + 1)\n            state.set(\"last_validation_error\", e)\n            state.set(\"partial_data\", e.partial_data)\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Re-raise for framework to retry\n\n        except (NetworkError, TimeoutError):\n            # Network error: retry same stage\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Re-raise for framework to retry\n\n    async def on_error(self, error: Exception, attempt: int, state: RetryState | None = None):\n        # Can inspect/modify state if needed\n        if state:\n            logger.info(f\"Stage {state.get('stage')}, Total prompts: {state.get('total_prompts')}\")\n</code></pre> <p>Framework Implementation Notes:</p> <ol> <li>State Storage: Framework maintains <code>Dict[work_item_id, RetryState]</code></li> <li>State Lifecycle:</li> <li>Created on first attempt</li> <li>Passed to <code>execute()</code> and <code>on_error()</code> on every attempt</li> <li>Cleared when work item succeeds or exhausts retries</li> <li>Thread Safety: State access is serialized per work item (no concurrent retries for same item)</li> <li>Backward Compatibility: <code>state</code> parameter is optional, existing strategies work unchanged</li> </ol> <p>Benefits:</p> <ul> <li>\u2705 Enables sophisticated multi-stage retry strategies</li> <li>\u2705 Strategies can track custom metrics (stage, total_prompts, error history)</li> <li>\u2705 No state collision (each work item has isolated state)</li> <li>\u2705 Clean API (no global state or threading issues)</li> <li>\u2705 Backward compatible (optional parameter)</li> </ul> <p>Alternative Simpler Approach: Pass work item context to execute()</p> <pre><code>async def execute(\n    self,\n    prompt: str,\n    attempt: int,\n    timeout: float,\n    context: Any = None,  # NEW: Work item context (user-defined)\n) -&gt; tuple[TOutput, dict[str, int]]:\n    ...\n</code></pre> <p>This allows strategies to use work item ID as key for instance-level state dict, but requires manual state management and cleanup.</p>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#summary","title":"Summary","text":"<p>The shared strategy pattern is valuable for cost optimization but currently requires:</p> <ol> <li>Manual idempotency guards in every strategy</li> <li>Custom strategies for newer API versions</li> <li>Custom strategies to access response metadata</li> <li>Manual tracking and aggregation of cached tokens</li> <li>Careful lifecycle management (cleanup vs. reuse)</li> <li>Manual cache matching logic</li> <li>Custom error classifiers and renewal logic for cache expiration</li> <li>Cannot implement advanced multi-stage retry strategies without per-work-item state</li> </ol> <p>Quick Wins:</p> <ul> <li>Document shared strategy pattern and best practices</li> <li>Add <code>PrepareOnceMixin</code> for idempotency</li> <li>Update <code>GeminiCachedStrategy</code> for google-genai v1.46+</li> <li>Add <code>include_metadata</code> option to expose safety ratings</li> <li>Add <code>total_cached_tokens</code> to <code>BatchResult</code></li> <li>Clarify cleanup() hook purpose in documentation</li> <li>Add built-in cache expiration handling or document the pattern</li> </ul> <p>Longer Term:</p> <ul> <li>Consider explicit strategy lifecycle management in framework</li> <li>Support cache metadata/tagging for better matching</li> <li>Built-in cache renewal with configurable TTL and renewal buffer</li> <li>Add per-work-item state management for advanced retry strategies (see Issue #8)</li> </ul>"},{"location":"archive/BATCH_LLM_FEEDBACK_OLD/#our-implementation","title":"Our Implementation","text":"<p>For reference, our working implementation is in:</p> <ul> <li><code>backend/ingest/openlibrary/enrichment/enrich.py</code>: <code>GeminiEnrichmentCachedStrategy</code></li> <li><code>backend/ingest/openlibrary/enrichment/enrich_works.py</code>: <code>EnrichmentErrorClassifier</code> + usage pattern</li> </ul> <p>Key features:</p> <ul> <li>Async lock for idempotency</li> <li>google-genai v1.46 compatibility</li> <li>Safety rating extraction</li> <li>Cached token tracking and reporting</li> <li>Automatic cache renewal for long-running jobs (5-minute buffer)</li> <li>Cache expiration error detection and retry (in error classifier)</li> <li>Proactive cache renewal before execute() calls</li> <li>Intelligent cache reuse (checks existing caches before creating new ones)</li> <li>Shared across all work items (tested with 2-10 workers)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/","title":"Implementation Plan: batch-llm v0.2.0","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#executive-summary","title":"Executive Summary","text":"<p>This document outlines a comprehensive plan to address issues identified in real-world production usage of batch-llm v0.1.0, specifically around shared strategy instances for cost optimization with Gemini prompt caching.</p> <p>Target Version: v0.2.0 Estimated Scope: 3-5 days of development + testing Breaking Changes: Minimal (mostly additions, a few signature changes with backward compatibility)</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issues-being-addressed","title":"Issues Being Addressed","text":"<p>Based on feedback from production usage (see <code>BATCH_LLM_FEEDBACK.md</code>):</p> Issue Priority Impact Complexity #1: Multiple <code>prepare()</code> calls on shared strategies Critical High Medium #2: API version incompatibility (google-genai v1.46+) Critical High Low #3: Missing safety ratings access High Medium Medium #4: No cached token tracking in <code>BatchResult</code> High Medium Low #5: Unclear cache lifecycle (cleanup() hook) Medium Medium Low (docs) #6: Cache matching precision Medium Medium Medium #7: Cache expiration error handling High High Medium #8: No per-work-item state for retry strategies High High High"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#release-strategy","title":"Release Strategy","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#phase-1-core-fixes-v020","title":"Phase 1: Core Fixes (v0.2.0)","text":"<p>Issues: #1, #2, #4, #5, #7 Goal: Fix critical production issues, minimal breaking changes Timeline: Week 1</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#phase-2-advanced-features-v030","title":"Phase 2: Advanced Features (v0.3.0)","text":"<p>Issues: #3, #6, #8 Goal: Add per-work-item state, rich response metadata, cache management Timeline: Week 2-3</p> <p>This document covers Phase 1 (v0.2.0) only.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#detailed-implementation-plan-v020","title":"Detailed Implementation Plan (v0.2.0)","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issue-1-multiple-prepare-calls-on-shared-strategies","title":"Issue #1: Multiple <code>prepare()</code> Calls on Shared Strategies","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#problem","title":"Problem","text":"<p>Framework calls <code>strategy.prepare()</code> once per work item. When the same strategy instance is shared across multiple work items (for caching cost optimization), <code>prepare()</code> is called multiple times concurrently, creating multiple caches.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#solution","title":"Solution","text":"<p>Add framework-level strategy instance tracking and call <code>prepare()</code> only once per unique strategy instance.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#implementation","title":"Implementation","text":"<p>File: <code>src/batch_llm/parallel.py</code></p> <pre><code>class ParallelBatchProcessor:\n    def __init__(self, ...):\n        # ... existing code ...\n        self._prepared_strategies: set[int] = set()  # Track by id()\n        self._strategy_lock = asyncio.Lock()  # Protect strategy initialization\n\n    async def _ensure_strategy_prepared(\n        self, strategy: LLMCallStrategy[TOutput]\n    ) -&gt; None:\n        \"\"\"Ensure strategy is prepared exactly once, even with concurrent calls.\"\"\"\n        strategy_id = id(strategy)\n\n        # Fast path: already prepared (no lock needed for read)\n        if strategy_id in self._prepared_strategies:\n            return\n\n        # Slow path: acquire lock and prepare\n        async with self._strategy_lock:\n            # Double-check after acquiring lock (another worker may have prepared)\n            if strategy_id in self._prepared_strategies:\n                return\n\n            await strategy.prepare()\n            self._prepared_strategies.add(strategy_id)\n            logger.debug(\n                f\"Prepared strategy {strategy.__class__.__name__} \"\n                f\"(id={strategy_id})\"\n            )\n\n    async def _process_item(\n        self, work_item: LLMWorkItem[TInput, TOutput, TContext]\n    ) -&gt; WorkItemResult[TOutput, TContext]:\n        # Ensure strategy is prepared before processing\n        await self._ensure_strategy_prepared(work_item.strategy)\n\n        # ... rest of existing code ...\n</code></pre> <p>File: <code>src/batch_llm/base.py</code></p> <pre><code>class BatchProcessor:\n    async def cleanup(self):\n        \"\"\"Clean up resources including strategy cleanup.\"\"\"\n        # ... existing worker cleanup code ...\n\n        # Cleanup strategies (call cleanup() once per unique strategy)\n        if hasattr(self, '_prepared_strategies'):\n            seen_strategies: set[int] = set()\n            # Iterate through all work items to find unique strategies\n            # Note: This requires tracking work items or strategies differently\n            # For v0.2.0, we'll document that strategies should handle their\n            # own cleanup in a reusable way (see Issue #5)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#breaking-changes","title":"Breaking Changes","text":"<p>None - This is purely internal framework behavior.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#tests","title":"Tests","text":"<p>File: <code>tests/test_shared_strategies.py</code> (new)</p> <pre><code>@pytest.mark.asyncio\nasync def test_shared_strategy_prepare_called_once():\n    \"\"\"Test that prepare() is called only once for shared strategy.\"\"\"\n    class CountingStrategy(LLMCallStrategy[str]):\n        prepare_count = 0\n\n        async def prepare(self):\n            self.prepare_count += 1\n            await asyncio.sleep(0.1)  # Simulate slow preparation\n\n        async def execute(self, prompt, attempt, timeout):\n            return f\"Response: {prompt}\", {\"input_tokens\": 10}\n\n    strategy = CountingStrategy()\n    config = ProcessorConfig(max_workers=5, timeout_per_item=10.0)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        # Add 20 work items sharing the same strategy\n        for i in range(20):\n            await processor.add_work(\n                LLMWorkItem(item_id=f\"item_{i}\", strategy=strategy, prompt=f\"Test {i}\")\n            )\n\n        result = await processor.process_all()\n\n    # All items should succeed\n    assert result.succeeded == 20\n\n    # prepare() should be called exactly once\n    assert strategy.prepare_count == 1\n\n\n@pytest.mark.asyncio\nasync def test_different_strategies_prepare_called_separately():\n    \"\"\"Test that different strategy instances each get prepare() called.\"\"\"\n    class CountingStrategy(LLMCallStrategy[str]):\n        def __init__(self):\n            self.prepare_count = 0\n\n        async def prepare(self):\n            self.prepare_count += 1\n\n        async def execute(self, prompt, attempt, timeout):\n            return f\"Response\", {\"input_tokens\": 10}\n\n    strategy1 = CountingStrategy()\n    strategy2 = CountingStrategy()\n\n    config = ProcessorConfig(max_workers=5, timeout_per_item=10.0)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        await processor.add_work(\n            LLMWorkItem(item_id=\"item_1\", strategy=strategy1, prompt=\"Test\")\n        )\n        await processor.add_work(\n            LLMWorkItem(item_id=\"item_2\", strategy=strategy2, prompt=\"Test\")\n        )\n\n        result = await processor.process_all()\n\n    assert result.succeeded == 2\n    assert strategy1.prepare_count == 1\n    assert strategy2.prepare_count == 1\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates","title":"Documentation Updates","text":"<p>File: <code>README.md</code></p> <p>Add new section:</p> <pre><code>### Sharing Strategies for Cost Optimization\n\nFor cost optimization (e.g., Gemini prompt caching), you can share a single strategy instance across all work items:\n\n```python\n# Create one cached strategy\nstrategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    cached_content=[...],\n    cache_ttl_seconds=3600,\n)\n\n# Reuse the same strategy for all items\nfor item in items:\n    work_item = LLMWorkItem(\n        item_id=item.id,\n        strategy=strategy,  # Shared instance\n        prompt=format_prompt(item),\n    )\n    await processor.add_work(work_item)\n</code></pre> <p>Benefits:</p> <ul> <li>Single cache created and shared across all work items</li> <li>70-90% cost reduction with Gemini prompt caching</li> <li>Framework ensures <code>prepare()</code> is called only once</li> </ul> <p>Note: The framework automatically handles idempotency - <code>prepare()</code> is called once per unique strategy instance, even with concurrent workers.</p> <pre><code>(end of markdown code block)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issue-2-api-version-incompatibility-google-genai-v146","title":"Issue #2: API Version Incompatibility (google-genai v1.46+)","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#problem_1","title":"Problem","text":"<p><code>GeminiCachedStrategy</code> uses old google-genai API (v1.45 and earlier):</p> <pre><code># Old API (v1.45)\ncache = await client.aio.caches.create(\n    model=model,\n    contents=cached_content,\n    ttl=\"3600s\",\n)\n</code></pre> <p>New API (v1.46+) requires:</p> <pre><code># New API (v1.46+)\nfrom google.genai.types import CreateCachedContentConfig\n\ncache = await client.aio.caches.create(\n    model=model,\n    config=CreateCachedContentConfig(\n        contents=cached_content,\n        ttl=\"3600s\",\n    ),\n)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#solution_1","title":"Solution","text":"<p>Auto-detect API version and use appropriate syntax.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#implementation_1","title":"Implementation","text":"<p>File: <code>src/batch_llm/llm_strategies.py</code></p> <pre><code>class GeminiCachedStrategy(LLMCallStrategy[TOutput]):\n    \"\"\"Strategy for calling Google Gemini API with context caching.\"\"\"\n\n    def __init__(self, ...):\n        # ... existing code ...\n\n        # Detect API version\n        self._api_version = self._detect_google_genai_version()\n        logger.debug(f\"Detected google-genai API version: {self._api_version}\")\n\n    @staticmethod\n    def _detect_google_genai_version() -&gt; str:\n        \"\"\"Detect which google-genai API version is installed.\"\"\"\n        try:\n            from google.genai.types import CreateCachedContentConfig\n            return \"v1.46+\"\n        except ImportError:\n            return \"v1.45\"\n\n    async def prepare(self) -&gt; None:\n        \"\"\"Create the Gemini cache using appropriate API version.\"\"\"\n        if self._api_version == \"v1.46+\":\n            from google.genai.types import CreateCachedContentConfig\n\n            self._cache = await self.client.aio.caches.create(\n                model=self.model,\n                config=CreateCachedContentConfig(\n                    contents=self.cached_content,\n                    ttl=f\"{self.cache_ttl_seconds}s\",\n                ),\n            )\n        else:\n            # Legacy API (v1.45 and earlier)\n            self._cache = await self.client.aio.caches.create(\n                model=self.model,\n                contents=self.cached_content,\n                ttl=f\"{self.cache_ttl_seconds}s\",\n            )\n\n        self._cache_created_at = time.time()\n        logger.info(\n            f\"Created Gemini cache: {self._cache.name} \"\n            f\"(TTL: {self.cache_ttl_seconds}s)\"\n        )\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#breaking-changes_1","title":"Breaking Changes","text":"<p>None - Backward compatible with both API versions.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#tests_1","title":"Tests","text":"<p>File: <code>tests/test_gemini_api_versions.py</code> (new)</p> <pre><code>@pytest.mark.asyncio\nasync def test_api_version_detection():\n    \"\"\"Test that API version detection works correctly.\"\"\"\n    from async_batch_llm.llm_strategies import GeminiCachedStrategy\n\n    version = GeminiCachedStrategy._detect_google_genai_version()\n    assert version in [\"v1.45\", \"v1.46+\"]\n\n\n# Mock-based test that doesn't require actual API calls\n@pytest.mark.asyncio\nasync def test_gemini_cached_strategy_with_both_api_versions():\n    \"\"\"Test GeminiCachedStrategy works with both API versions.\"\"\"\n    # This would use mocking to test both code paths\n    # Implementation depends on mocking strategy\n    pass\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates_1","title":"Documentation Updates","text":"<p>File: <code>docs/GEMINI_INTEGRATION.md</code></p> <p>Update dependency requirements:</p> <pre><code>## Google Gemini API Versions\n\nbatch-llm supports both old and new google-genai API versions:\n\n- **google-genai v1.45 and earlier:** Legacy API (deprecated)\n- **google-genai v1.46+:** New API with `CreateCachedContentConfig` (recommended)\n\nThe framework automatically detects which version you have installed and uses the appropriate API calls.\n\n### Recommended Installation\n\n```bash\n# Install latest google-genai (v1.46+)\npip install 'batch-llm[gemini]'  # Installs google-genai&gt;=1.46\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#legacy-version-support","title":"Legacy Version Support","text":"<p>If you need to use google-genai v1.45 or earlier:</p> <pre><code>pip install 'batch-llm[gemini]' 'google-genai&lt;1.46'\n</code></pre> <p>Note: google-genai v1.45 and earlier are deprecated and may be removed in batch-llm v0.3.0.</p> <pre><code>(end of note)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issue-4-no-cached-token-tracking-in-batchresult","title":"Issue #4: No Cached Token Tracking in BatchResult","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#problem_2","title":"Problem","text":"<p><code>BatchResult</code> aggregates <code>total_input_tokens</code> and <code>total_output_tokens</code> but not <code>cached_input_tokens</code>, making it impossible to measure cache effectiveness.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#solution_2","title":"Solution","text":"<p>Add <code>total_cached_tokens</code> field to <code>BatchResult</code> and aggregate from work item results.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#implementation_2","title":"Implementation","text":"<p>File: <code>src/batch_llm/base.py</code></p> <pre><code>@dataclass\nclass BatchResult(Generic[TOutput, TContext]):\n    \"\"\"Result of processing a batch of work items.\"\"\"\n\n    results: list[WorkItemResult[TOutput, TContext]]\n    total_items: int = 0\n    succeeded: int = 0\n    failed: int = 0\n    total_input_tokens: int = 0\n    total_output_tokens: int = 0\n    total_cached_tokens: int = 0  # NEW: Track cached tokens separately\n\n    def __post_init__(self):\n        \"\"\"Calculate summary statistics from results.\"\"\"\n        self.total_items = len(self.results)\n        self.succeeded = sum(1 for r in self.results if r.success)\n        self.failed = sum(1 for r in self.results if not r.success)\n        self.total_input_tokens = sum(\n            r.token_usage.get(\"input_tokens\", 0) for r in self.results\n        )\n        self.total_output_tokens = sum(\n            r.token_usage.get(\"output_tokens\", 0) for r in self.results\n        )\n        # NEW: Aggregate cached tokens\n        self.total_cached_tokens = sum(\n            r.token_usage.get(\"cached_input_tokens\", 0) for r in self.results\n        )\n\n    def cache_hit_rate(self) -&gt; float:\n        \"\"\"Calculate cache hit rate as percentage of input tokens cached.\"\"\"\n        if self.total_input_tokens == 0:\n            return 0.0\n        return (self.total_cached_tokens / self.total_input_tokens) * 100.0\n\n    def effective_input_tokens(self) -&gt; int:\n        \"\"\"Calculate effective input tokens (actual cost after caching).\"\"\"\n        # Gemini charges 10% for cached tokens\n        return self.total_input_tokens - int(self.total_cached_tokens * 0.9)\n</code></pre> <p>File: <code>src/batch_llm/base.py</code> (ProcessingStats)</p> <pre><code>@dataclass\nclass ProcessingStats:\n    \"\"\"Statistics for batch processing.\"\"\"\n\n    # ... existing fields ...\n    total_cached_tokens: int = 0  # NEW: Cached tokens across all items\n\n    def copy(self) -&gt; dict[str, Any]:\n        \"\"\"Return a dictionary copy of the stats.\"\"\"\n        return {\n            # ... existing fields ...\n            \"total_cached_tokens\": self.total_cached_tokens,  # NEW\n        }\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#breaking-changes_2","title":"Breaking Changes","text":"<p>None - Adding optional fields to dataclasses is backward compatible.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#tests_2","title":"Tests","text":"<p>File: <code>tests/test_token_tracking.py</code> (new)</p> <pre><code>@pytest.mark.asyncio\nasync def test_cached_token_aggregation():\n    \"\"\"Test that cached tokens are properly aggregated.\"\"\"\n    from async_batch_llm.testing import MockStrategy\n\n    class CachedTokenStrategy(LLMCallStrategy[str]):\n        async def execute(self, prompt, attempt, timeout):\n            return \"Response\", {\n                \"input_tokens\": 500,\n                \"output_tokens\": 100,\n                \"total_tokens\": 600,\n                \"cached_input_tokens\": 450,  # 90% cached\n            }\n\n    strategy = CachedTokenStrategy()\n    config = ProcessorConfig(max_workers=2)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        for i in range(10):\n            await processor.add_work(\n                LLMWorkItem(item_id=f\"item_{i}\", strategy=strategy, prompt=\"Test\")\n            )\n\n        result = await processor.process_all()\n\n    assert result.total_input_tokens == 5000\n    assert result.total_output_tokens == 1000\n    assert result.total_cached_tokens == 4500\n    assert result.cache_hit_rate() == 90.0\n    assert result.effective_input_tokens() == 950  # 500 - (450 * 0.9)\n\n\n@pytest.mark.asyncio\nasync def test_no_cached_tokens():\n    \"\"\"Test that missing cached_input_tokens doesn't break aggregation.\"\"\"\n    class NonCachedStrategy(LLMCallStrategy[str]):\n        async def execute(self, prompt, attempt, timeout):\n            return \"Response\", {\n                \"input_tokens\": 100,\n                \"output_tokens\": 50,\n                # No cached_input_tokens\n            }\n\n    strategy = NonCachedStrategy()\n    config = ProcessorConfig(max_workers=1)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        await processor.add_work(\n            LLMWorkItem(item_id=\"item_1\", strategy=strategy, prompt=\"Test\")\n        )\n\n        result = await processor.process_all()\n\n    assert result.total_cached_tokens == 0\n    assert result.cache_hit_rate() == 0.0\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates_2","title":"Documentation Updates","text":"<p>File: <code>README.md</code></p> <p>Add to \"Working with Results\" section:</p> <pre><code>### Cache Effectiveness Metrics\n\nWhen using cached strategies (e.g., `GeminiCachedStrategy`), `BatchResult` provides cache metrics:\n\n```python\nresult = await processor.process_all()\n\nprint(f\"Total input tokens: {result.total_input_tokens}\")\nprint(f\"Cached tokens: {result.total_cached_tokens}\")\nprint(f\"Cache hit rate: {result.cache_hit_rate():.1f}%\")\nprint(f\"Effective cost: {result.effective_input_tokens()} tokens\")\n</code></pre> <p>Example output:</p> <pre><code>Total input tokens: 50000\nCached tokens: 45000\nCache hit rate: 90.0%\nEffective cost: 9500 tokens  # 81% cost reduction!\n</code></pre> <pre><code>(end of section)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issue-5-unclear-cache-lifecycle-cleanup-hook","title":"Issue #5: Unclear Cache Lifecycle (cleanup() Hook)","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#problem_3","title":"Problem","text":"<p>Documentation doesn't clarify whether <code>cleanup()</code> should delete caches or preserve them for reuse across runs.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#solution_3","title":"Solution","text":"<p>Update documentation to clarify cleanup() semantics and provide best practices.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#implementation_3","title":"Implementation","text":"<p>File: <code>src/batch_llm/llm_strategies.py</code></p> <p>Update docstring:</p> <pre><code>class LLMCallStrategy:\n    async def cleanup(self) -&gt; None:\n        \"\"\"\n        Clean up resources after all retry attempts complete.\n\n        Called once per work item after processing finishes (success or failure).\n\n        **Use this for:**\n        - Closing connections/sessions\n        - Releasing locks\n        - Logging final metrics\n        - Deleting temporary files\n\n        **Do NOT use this for:**\n        - Deleting caches intended for reuse across runs\n        - Destructive cleanup that prevents resource reuse\n\n        **Note on Caches:**\n        For reusable resources like Gemini caches with TTLs, consider letting\n        them expire naturally to enable cost savings across multiple pipeline\n        runs. See `GeminiCachedStrategy` for an example.\n\n        Default: no-op\n        \"\"\"\n        pass\n</code></pre> <p>File: <code>src/batch_llm/llm_strategies.py</code></p> <p>Update <code>GeminiCachedStrategy.cleanup()</code>:</p> <pre><code>class GeminiCachedStrategy(LLMCallStrategy[TOutput]):\n    async def cleanup(self) -&gt; None:\n        \"\"\"\n        Cleanup hook - preserves cache for reuse by default.\n\n        By default, this method does NOT delete the cache. The cache remains\n        active until its TTL expires, allowing reuse across multiple runs\n        within the TTL window (e.g., 1 hour).\n\n        This enables significant cost savings when running multiple batches:\n        - First run: Creates cache, pays full cost\n        - Subsequent runs (within TTL): Reuse cache, 70-90% cost reduction\n\n        To delete the cache immediately (e.g., for cleanup in tests):\n\n        ```python\n        strategy = GeminiCachedStrategy(...)\n        # ... use strategy ...\n        await strategy.delete_cache()  # Explicit deletion\n        ```\n        \"\"\"\n        if self._cache:\n            logger.info(\n                f\"Leaving cache active for reuse: {self._cache.name} \"\n                f\"(expires in {self.cache_ttl_seconds}s from creation)\"\n            )\n\n    async def delete_cache(self) -&gt; None:\n        \"\"\"\n        Explicitly delete the Gemini cache.\n\n        Call this when you want to immediately delete the cache instead of\n        letting it expire naturally. Useful for:\n        - Test cleanup\n        - One-off batch jobs where reuse isn't needed\n        - Updating cached content (delete old, create new)\n        \"\"\"\n        if self._cache:\n            try:\n                await self.client.aio.caches.delete(name=self._cache.name)\n                logger.info(f\"Deleted Gemini cache: {self._cache.name}\")\n                self._cache = None\n                self._cache_created_at = None\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to delete Gemini cache '{self._cache.name}': {e}. \"\n                    \"Cache may have already expired or been deleted.\"\n                )\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#breaking-changes_3","title":"Breaking Changes","text":"<p>Behavioral change (non-breaking): <code>GeminiCachedStrategy.cleanup()</code> no longer deletes the cache by default. This is technically a behavioral change, but it's opt-in (cleanup is only called if you call it), and the new behavior is more sensible for production use.</p> <p>Migration: If you were relying on <code>cleanup()</code> to delete caches, call <code>await strategy.delete_cache()</code> explicitly instead.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#tests_3","title":"Tests","text":"<p>File: <code>tests/test_cache_lifecycle.py</code> (new)</p> <pre><code>@pytest.mark.asyncio\nasync def test_cleanup_preserves_cache():\n    \"\"\"Test that cleanup() does not delete cache by default.\"\"\"\n    # Mock test - would verify cleanup() doesn't call delete\n    pass\n\n\n@pytest.mark.asyncio\nasync def test_explicit_cache_deletion():\n    \"\"\"Test that delete_cache() explicitly removes cache.\"\"\"\n    # Mock test - would verify delete_cache() calls API\n    pass\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates_3","title":"Documentation Updates","text":"<p>File: <code>docs/GEMINI_INTEGRATION.md</code></p> <p>Add section:</p> <pre><code>## Cache Lifecycle Management\n\n### Default Behavior: Cache Reuse\n\nBy default, `GeminiCachedStrategy` preserves caches for reuse across runs:\n\n```python\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    result = await processor.process_all()\n# Cleanup is called, but cache is preserved\n\n# Run again within TTL window\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process more items ...\n    # Reuses existing cache if found - 70-90% cost savings!\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#explicit-cache-deletion","title":"Explicit Cache Deletion","text":"<p>To delete caches immediately:</p> <pre><code>strategy = GeminiCachedStrategy(...)\n\nasync with ParallelBatchProcessor(...) as processor:\n    # ... use strategy ...\n    pass\n\n# Explicitly delete cache\nawait strategy.delete_cache()\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#best-practices","title":"Best Practices","text":"<p>Production pipelines (multiple runs):</p> <ul> <li>Let caches expire naturally (don't call <code>delete_cache()</code>)</li> <li>Set TTL to cover expected run frequency (e.g., 1 hour for hourly jobs)</li> <li>Monitor cache reuse via <code>result.cache_hit_rate()</code></li> </ul> <p>Tests and one-off jobs:</p> <ul> <li>Call <code>delete_cache()</code> for cleanup</li> <li>Or use short TTL (60s) to auto-expire quickly</li> </ul> <p>Updating prompts:</p> <ul> <li>Delete old cache before creating new one with updated content</li> <li>Or use cache tagging/metadata to identify versions (v0.3.0 feature)</li> </ul> <pre><code>(end of section)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#issue-7-cache-expiration-error-handling","title":"Issue #7: Cache Expiration Error Handling","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#problem_4","title":"Problem","text":"<p>Long-running pipelines (&gt;1 hour) hit cache expiration errors when Google expires the cache on their servers. No built-in detection or recovery.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#solution_4","title":"Solution","text":"<p>Add proactive cache expiration detection with automatic renewal in <code>GeminiCachedStrategy</code>.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#implementation_4","title":"Implementation","text":"<p>File: <code>src/batch_llm/llm_strategies.py</code></p> <pre><code>class GeminiCachedStrategy(LLMCallStrategy[TOutput]):\n    def __init__(\n        self,\n        model: str,\n        client: \"genai.Client\",\n        response_parser: Callable[[Any], TOutput],\n        cached_content: list[\"Content\"],\n        cache_ttl_seconds: int = 3600,\n        cache_renewal_buffer_seconds: int = 300,  # NEW: Renew 5min before expiration\n        cache_refresh_threshold: float = 0.1,  # Deprecated in favor of buffer\n        config: \"GenerateContentConfig | None\" = None,\n        auto_renew: bool = True,  # NEW: Automatically renew expired caches\n    ):\n        \"\"\"\n        Initialize Gemini cached strategy with automatic renewal.\n\n        Args:\n            cache_ttl_seconds: Cache TTL in seconds (default: 3600 = 1 hour)\n            cache_renewal_buffer_seconds: Renew cache this many seconds before\n                expiration to avoid expiration errors (default: 300 = 5 minutes)\n            auto_renew: Automatically renew expired caches in execute() (default: True)\n        \"\"\"\n        # ... existing code ...\n        self.cache_renewal_buffer_seconds = cache_renewal_buffer_seconds\n        self.auto_renew = auto_renew\n        self._cache_lock = asyncio.Lock()  # Protect cache renewal\n\n    def _is_cache_expired(self) -&gt; bool:\n        \"\"\"Check if cache has expired or is about to expire.\"\"\"\n        if self._cache is None or self._cache_created_at is None:\n            return True\n\n        cache_age = time.time() - self._cache_created_at\n        expires_in = self.cache_ttl_seconds - cache_age\n\n        return expires_in &lt;= self.cache_renewal_buffer_seconds\n\n    async def _find_or_create_cache(self) -&gt; None:\n        \"\"\"Find existing cache or create new one.\"\"\"\n        # Try to find existing cache with same model\n        try:\n            caches = await self.client.aio.caches.list()\n\n            for cache in caches:\n                # Cache model is full path: \"projects/.../models/gemini-...\"\n                # Match by model name suffix\n                if cache.model.endswith(self.model):\n                    self._cache = cache\n\n                    # CRITICAL: Use cache's actual creation time, not current time\n                    if hasattr(cache, 'create_time') and cache.create_time:\n                        self._cache_created_at = cache.create_time.timestamp()\n                    else:\n                        # Fallback: assume old to trigger renewal check\n                        self._cache_created_at = (\n                            time.time() - self.cache_ttl_seconds\n                        )\n\n                    logger.info(\n                        f\"Reusing existing Gemini cache: {self._cache.name} \"\n                        f\"(age: {time.time() - self._cache_created_at:.0f}s)\"\n                    )\n                    return\n        except Exception as e:\n            logger.warning(f\"Failed to list existing caches: {e}\")\n\n        # No existing cache found, create new one\n        await self._create_new_cache()\n\n    async def _create_new_cache(self) -&gt; None:\n        \"\"\"Create a new Gemini cache.\"\"\"\n        api_version = self._detect_google_genai_version()\n\n        if api_version == \"v1.46+\":\n            from google.genai.types import CreateCachedContentConfig\n\n            self._cache = await self.client.aio.caches.create(\n                model=self.model,\n                config=CreateCachedContentConfig(\n                    contents=self.cached_content,\n                    ttl=f\"{self.cache_ttl_seconds}s\",\n                ),\n            )\n        else:\n            self._cache = await self.client.aio.caches.create(\n                model=self.model,\n                contents=self.cached_content,\n                ttl=f\"{self.cache_ttl_seconds}s\",\n            )\n\n        self._cache_created_at = time.time()\n        logger.info(\n            f\"Created new Gemini cache: {self._cache.name} \"\n            f\"(TTL: {self.cache_ttl_seconds}s)\"\n        )\n\n    async def prepare(self) -&gt; None:\n        \"\"\"Find or create the Gemini cache.\"\"\"\n        await self._find_or_create_cache()\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[TOutput, TokenUsage]:\n        \"\"\"Execute with automatic cache renewal.\"\"\"\n        # Check and renew cache if expired (proactive)\n        if self.auto_renew and self._is_cache_expired():\n            logger.info(\n                \"Cache expired or about to expire, renewing before API call\"\n            )\n            async with self._cache_lock:\n                # Double-check after acquiring lock\n                if self._is_cache_expired():\n                    # Clear cache reference to force creation of new cache\n                    self._cache = None\n                    self._cache_created_at = None\n                    await self._find_or_create_cache()\n\n        # ... rest of execute() logic (make API call with cache) ...\n</code></pre> <p>File: <code>src/batch_llm/classifiers/gemini.py</code></p> <p>Update error classifier to detect cache expiration:</p> <pre><code>class GeminiErrorClassifier(ErrorClassifier):\n    def classify(self, exception: Exception) -&gt; ErrorInfo:\n        \"\"\"Classify Gemini API errors.\"\"\"\n        error_str = str(exception)\n\n        # Check for cache expiration FIRST (before parent class checks)\n        # This is critical because ClientError may be classified differently\n        if \"cache\" in error_str.lower() and \"expired\" in error_str.lower():\n            return ErrorInfo(\n                is_retryable=True,  # Retry - cache will be renewed\n                is_rate_limit=False,\n                is_timeout=False,\n                error_category=\"cache_expired\",\n                message=\"Gemini cache expired, will renew and retry\",\n            )\n\n        # ... rest of existing error classification ...\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#breaking-changes_4","title":"Breaking Changes","text":"<p>None - New parameters have sensible defaults, existing code continues to work.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#tests_4","title":"Tests","text":"<p>File: <code>tests/test_cache_expiration.py</code> (new)</p> <pre><code>@pytest.mark.asyncio\nasync def test_cache_expiration_detection():\n    \"\"\"Test that expired caches are detected.\"\"\"\n    # Mock test using strategy with short TTL\n    pass\n\n\n@pytest.mark.asyncio\nasync def test_automatic_cache_renewal():\n    \"\"\"Test that expired caches are automatically renewed.\"\"\"\n    # Mock test simulating cache expiration during execution\n    pass\n\n\n@pytest.mark.asyncio\nasync def test_cache_reuse_creation_time():\n    \"\"\"Test that reused caches use actual creation time, not current time.\"\"\"\n    # This is the critical bug mentioned in feedback\n    pass\n\n\n@pytest.mark.asyncio\nasync def test_cache_expiration_error_classification():\n    \"\"\"Test that cache expiration errors are classified as retryable.\"\"\"\n    from async_batch_llm.classifiers import GeminiErrorClassifier\n\n    classifier = GeminiErrorClassifier()\n\n    # Simulate cache expiration error\n    error = Exception(\n        \"400 INVALID_ARGUMENT. {'error': {'code': 400, \"\n        \"'message': 'Cache content 5025785736947826688 is expired.', \"\n        \"'status': 'INVALID_ARGUMENT'}}\"\n    )\n\n    info = classifier.classify(error)\n\n    assert info.is_retryable is True\n    assert info.error_category == \"cache_expired\"\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates_4","title":"Documentation Updates","text":"<p>File: <code>docs/GEMINI_INTEGRATION.md</code></p> <p>Add section:</p> <pre><code>## Long-Running Pipelines (Cache Expiration Handling)\n\nFor pipelines running longer than your cache TTL (e.g., 3+ hours with 1-hour cache):\n\n### Automatic Renewal (Default)\n\n`GeminiCachedStrategy` automatically renews expired caches:\n\n```python\nstrategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    cached_content=[...],\n    cache_ttl_seconds=3600,  # 1 hour\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n    auto_renew=True,  # Default: automatic renewal\n)\n</code></pre> <p>How it works:</p> <ol> <li>Before each API call, checks if cache will expire in &lt;5 minutes</li> <li>If yes, creates new cache or finds existing one</li> <li>API call uses fresh cache</li> <li>No expiration errors!</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#manual-control","title":"Manual Control","text":"<p>Disable automatic renewal if you want manual control:</p> <pre><code>strategy = GeminiCachedStrategy(\n    ...,\n    auto_renew=False,  # Disable automatic renewal\n)\n\n# Manually renew cache when needed\nif strategy._is_cache_expired():\n    await strategy.prepare()  # Force cache renewal\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#cost-implications","title":"Cost Implications","text":"<p>Automatic renewal:</p> <ul> <li>\u2705 Zero downtime (no expiration errors)</li> <li>\u2705 Optimal cache reuse (always uses fresh cache)</li> <li>\u26a0\ufe0f  May create new cache if old one expired</li> </ul> <p>Best practice: Set TTL to slightly longer than expected run time to maximize reuse.</p> <pre><code>(end of section)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#migration-guide-v01x-v020","title":"Migration Guide (v0.1.x \u2192 v0.2.0)","text":"<p>File: <code>docs/MIGRATION_V0_2.md</code> (new)</p> <pre><code># Migration Guide: v0.1.x \u2192 v0.2.0\n\n## Overview\n\nVersion 0.2.0 adds critical fixes for production usage, particularly around shared strategies and Gemini caching. Most changes are backward compatible.\n\n## Breaking Changes\n\n### 1. GeminiCachedStrategy cleanup() Behavior\n\n**Before (v0.1):**\n```python\nstrategy = GeminiCachedStrategy(...)\nasync with ParallelBatchProcessor(...) as processor:\n    # ...\n    pass\n# cleanup() deletes cache\n</code></pre> <p>After (v0.2):</p> <pre><code>strategy = GeminiCachedStrategy(...)\nasync with ParallelBatchProcessor(...) as processor:\n    # ...\n    pass\n# cleanup() preserves cache for reuse\n\n# Explicitly delete if needed\nawait strategy.delete_cache()\n</code></pre> <p>Migration: If you relied on automatic cache deletion, call <code>await strategy.delete_cache()</code> explicitly.</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#new-features","title":"New Features","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#shared-strategy-optimization","title":"Shared Strategy Optimization","text":"<p>You can now share strategy instances across work items without duplicate <code>prepare()</code> calls:</p> <pre><code># Create one strategy\nstrategy = GeminiCachedStrategy(...)\n\n# Share across all work items\nfor item in items:\n    await processor.add_work(\n        LLMWorkItem(strategy=strategy, ...)  # Reuse same instance\n    )\n\n# prepare() is called only once!\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#cached-token-tracking","title":"Cached Token Tracking","text":"<p><code>BatchResult</code> now includes cached token metrics:</p> <pre><code>result = await processor.process_all()\n\nprint(f\"Cache hit rate: {result.cache_hit_rate():.1f}%\")\nprint(f\"Effective cost: {result.effective_input_tokens()} tokens\")\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#automatic-cache-renewal","title":"Automatic Cache Renewal","text":"<p>Long-running pipelines automatically renew expired caches:</p> <pre><code>strategy = GeminiCachedStrategy(\n    cache_ttl_seconds=3600,\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n    auto_renew=True,  # Default\n)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#api-compatibility","title":"API Compatibility","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#google-genai-v146","title":"google-genai v1.46+","text":"<p>Both old and new google-genai APIs are supported:</p> <pre><code># Recommended: Install latest\npip install 'batch-llm[gemini]'  # Gets google-genai&gt;=1.46\n\n# Legacy support (will be removed in v0.3)\npip install 'batch-llm[gemini]' 'google-genai&lt;1.46'\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#recommended-actions","title":"Recommended Actions","text":"<ol> <li>Update to google-genai v1.46+ for best support</li> <li>Review cache cleanup logic - add explicit <code>delete_cache()</code> calls if needed</li> <li>Enable shared strategies for caching cost optimization</li> <li>Monitor cache metrics using <code>result.cache_hit_rate()</code></li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#support","title":"Support","text":"<p>See docs/GEMINI_INTEGRATION.md for detailed examples and best practices.</p> <pre><code>(end of document)\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#testing-strategy","title":"Testing Strategy","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#test-coverage-goals","title":"Test Coverage Goals","text":"<ul> <li>Unit tests: 90%+ coverage for new code</li> <li>Integration tests: Key workflows (shared strategies, cache renewal)</li> <li>Mock tests: No API calls required for core tests</li> <li>Manual tests: Real API testing with Gemini (documented in examples)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_shared_strategies.py        # Issue #1 tests\n\u251c\u2500\u2500 test_gemini_api_versions.py      # Issue #2 tests\n\u251c\u2500\u2500 test_token_tracking.py           # Issue #4 tests\n\u251c\u2500\u2500 test_cache_lifecycle.py          # Issue #5 tests\n\u251c\u2500\u2500 test_cache_expiration.py         # Issue #7 tests\n\u2514\u2500\u2500 test_backward_compatibility.py   # Ensure v0.1 code still works\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#running-tests","title":"Running Tests","text":"<pre><code># All tests\nuv run pytest\n\n# Specific issue tests\nuv run pytest tests/test_shared_strategies.py -v\n\n# Coverage report\nuv run pytest --cov=batch_llm --cov-report=html\n</code></pre>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#documentation-updates_5","title":"Documentation Updates","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#files-to-update","title":"Files to Update","text":"<ol> <li>README.md</li> <li>Add shared strategies section</li> <li>Add cache metrics examples</li> <li> <p>Update quickstart examples</p> </li> <li> <p>docs/API.md</p> </li> <li>Document new BatchResult fields</li> <li>Document GeminiCachedStrategy parameters</li> <li> <p>Add cache lifecycle methods</p> </li> <li> <p>docs/GEMINI_INTEGRATION.md</p> </li> <li>Add API version compatibility section</li> <li>Add cache lifecycle best practices</li> <li> <p>Add long-running pipeline patterns</p> </li> <li> <p>docs/MIGRATION_V0_2.md (new)</p> </li> <li> <p>Migration guide for v0.1 \u2192 v0.2</p> </li> <li> <p>CLAUDE.md</p> </li> <li>Update version history</li> <li>Add v0.2.0 features and patterns</li> <li> <p>Update known limitations</p> </li> <li> <p>CHANGELOG.md</p> </li> <li>Document all changes, fixes, improvements</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#release-checklist","title":"Release Checklist","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#pre-release","title":"Pre-Release","text":"<ul> <li>[ ] All tests passing (uv run pytest)</li> <li>[ ] Linting passing (uv run ruff check src/ tests/)</li> <li>[ ] Type checking passing (uv run mypy src/batch_llm/)</li> <li>[ ] Documentation updated</li> <li>[ ] Examples updated/tested</li> <li>[ ] Migration guide written</li> <li>[ ] CHANGELOG.md updated</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#release","title":"Release","text":"<ul> <li>[ ] Update version in pyproject.toml (0.1.0 \u2192 0.2.0)</li> <li>[ ] Create git tag (v0.2.0)</li> <li>[ ] Build package (uv build)</li> <li>[ ] Test on TestPyPI</li> <li>[ ] Publish to PyPI</li> <li>[ ] Create GitHub release with notes</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#post-release","title":"Post-Release","text":"<ul> <li>[ ] Verify installation (pip install batch-llm==0.2.0)</li> <li>[ ] Test examples with fresh install</li> <li>[ ] Update documentation site (if applicable)</li> <li>[ ] Announce release</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#timeline-estimate","title":"Timeline Estimate","text":"Phase Duration Tasks Week 1, Days 1-2 2 days Implement Issue #1, #2, #4 with tests Week 1, Days 3-4 2 days Implement Issue #5, #7 with tests Week 1, Day 5 1 day Documentation updates, migration guide Week 2, Days 1-2 2 days Integration testing, bug fixes Week 2, Day 3 1 day Final review, release preparation Total ~7 days Full implementation + testing + docs"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#success-criteria","title":"Success Criteria","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#functional","title":"Functional","text":"<ul> <li>[ ] Shared strategies only call prepare() once</li> <li>[ ] Works with both google-genai v1.45 and v1.46+</li> <li>[ ] Cached tokens properly tracked in BatchResult</li> <li>[ ] Cache lifecycle clearly documented</li> <li>[ ] Cache expiration automatically handled</li> <li>[ ] All tests passing (90%+ coverage)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#non-functional","title":"Non-Functional","text":"<ul> <li>[ ] Backward compatible with v0.1 code</li> <li>[ ] No performance regression</li> <li>[ ] Clear migration path documented</li> <li>[ ] Production-ready (tested with real Gemini API)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#future-work-v030","title":"Future Work (v0.3.0)","text":"<p>Issues deferred to v0.3.0:</p> <ul> <li>Issue #3: Rich response metadata (safety ratings, finish reason)</li> <li>Issue #6: Cache tagging/metadata for precise matching</li> <li>Issue #8: Per-work-item state for advanced retry strategies</li> </ul> <p>See IMPLEMENTATION_PLAN_V0_3.md (to be created).</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#questions-decisions-needed","title":"Questions / Decisions Needed","text":"<ol> <li>Backward compatibility: Should we maintain v0.1 cleanup() behavior with a flag?</li> <li> <p>Recommendation: No, new behavior is better. Document migration.</p> </li> <li> <p>API version support: How long to support google-genai &lt;1.46?</p> </li> <li> <p>Recommendation: Support in v0.2, deprecate with warning, remove in v0.3.</p> </li> <li> <p>Cache matching: Should we add basic tagging in v0.2 or defer to v0.3?</p> </li> <li>Recommendation: Defer to v0.3. v0.2 is already substantial.</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#appendix-code-locations","title":"Appendix: Code Locations","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#files-to-modify","title":"Files to Modify","text":"<ul> <li><code>src/batch_llm/base.py</code> - BatchResult, ProcessingStats, cleanup()</li> <li><code>src/batch_llm/llm_strategies.py</code> - GeminiCachedStrategy enhancements</li> <li><code>src/batch_llm/parallel.py</code> - Strategy preparation tracking</li> <li><code>src/batch_llm/classifiers/gemini.py</code> - Cache expiration detection</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#files-to-create","title":"Files to Create","text":"<ul> <li><code>tests/test_shared_strategies.py</code></li> <li><code>tests/test_gemini_api_versions.py</code></li> <li><code>tests/test_token_tracking.py</code></li> <li><code>tests/test_cache_lifecycle.py</code></li> <li><code>tests/test_cache_expiration.py</code></li> <li><code>tests/test_backward_compatibility.py</code></li> <li><code>docs/MIGRATION_V0_2.md</code></li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_2/#files-to-update_1","title":"Files to Update","text":"<ul> <li><code>README.md</code></li> <li><code>docs/API.md</code></li> <li><code>docs/GEMINI_INTEGRATION.md</code></li> <li><code>CLAUDE.md</code></li> <li><code>CHANGELOG.md</code></li> <li><code>pyproject.toml</code> (version bump)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/","title":"Implementation Plan: v0.3.0","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#overview","title":"Overview","text":"<p>Version 0.3.0 focuses on remaining production feedback items from BATCH_LLM_FEEDBACK.md that were deferred from v0.2.0, plus additional enhancements based on user needs.</p> <p>Target Date: TBD Status: Planning</p>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#items-from-batch_llm_feedbackmd","title":"Items from BATCH_LLM_FEEDBACK.md","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#already-implemented-in-v020","title":"\u2705 Already Implemented in v0.2.0","text":"<ol> <li>Issue #1: Multiple prepare() calls - \u2705 Fixed with framework-level strategy tracking</li> <li>Issue #2: API version incompatibility - \u2705 Fixed with auto-detection for google-genai v1.46+</li> <li>Issue #4: Cached token tracking - \u2705 Added <code>total_cached_tokens</code>, <code>cache_hit_rate()</code>, <code>effective_input_tokens()</code></li> <li>Issue #5: Cache lifecycle unclear - \u2705 Clarified with cleanup() preserving caches, added delete_cache()</li> <li>Issue #7: Cache expiration handling - \u2705 Added auto-renewal with configurable buffer</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#deferred-to-v030","title":"\ud83d\udd1c Deferred to v0.3.0","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#issue-3-missing-safety-ratings-in-geminicachedstrategy","title":"Issue #3: Missing Safety Ratings in GeminiCachedStrategy","text":"<p>Priority: Medium Effort: Medium Breaking: No (opt-in feature)</p> <p>Problem:</p> <ul> <li>Users cannot access Gemini safety ratings (harassment, hate speech, etc.)</li> <li>Current <code>response_parser</code> only gets parsed response, not raw response object</li> <li>Critical for content filtering use cases</li> </ul> <p>Proposed Solution:</p> <p>Add optional <code>include_metadata</code> parameter and <code>GeminiResponse</code> wrapper:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Generic, TypeVar, Any\n\nTOutput = TypeVar('TOutput')\n\n@dataclass\nclass GeminiResponse(Generic[TOutput]):\n    \"\"\"Container for parsed output and raw response metadata.\"\"\"\n    output: TOutput\n    safety_ratings: dict[str, str] | None\n    finish_reason: str | None\n    token_usage: dict[str, int]\n    raw_response: Any  # Full response object\n\nclass GeminiCachedStrategy:\n    def __init__(\n        self,\n        # ... existing params ...\n        include_metadata: bool = False,  # NEW: Opt-in for metadata\n    ):\n        self.include_metadata = include_metadata\n\n    async def execute(self, ...) -&gt; tuple[TOutput | GeminiResponse[TOutput], dict[str, int]]:\n        response = await self.client.aio.models.generate_content(...)\n\n        output = self.response_parser(response)\n\n        if self.include_metadata:\n            safety_ratings = self._extract_safety_ratings(response)\n            return GeminiResponse(\n                output=output,\n                safety_ratings=safety_ratings,\n                finish_reason=response.candidates[0].finish_reason,\n                token_usage=token_usage,\n                raw_response=response,\n            ), token_usage\n\n        return output, token_usage\n\n    def _extract_safety_ratings(self, response) -&gt; dict[str, str]:\n        \"\"\"Extract safety ratings from Gemini response.\"\"\"\n        ratings = {}\n        if hasattr(response, 'candidates') and response.candidates:\n            candidate = response.candidates[0]\n            if hasattr(candidate, 'safety_ratings'):\n                for rating in candidate.safety_ratings:\n                    ratings[rating.category] = rating.probability\n        return ratings\n</code></pre> <p>Usage:</p> <pre><code>strategy = GeminiCachedStrategy(\n    response_parser=parse_metadata,\n    include_metadata=True,  # Enable metadata access\n)\n\n# In post-processor:\nif isinstance(result.output, GeminiResponse):\n    parsed_data = result.output.output\n    safety = result.output.safety_ratings\n    if safety.get(\"HARM_CATEGORY_HATE_SPEECH\") == \"HIGH\":\n        # Filter content\n        pass\n</code></pre> <p>Implementation Steps:</p> <ol> <li>Add <code>GeminiResponse</code> dataclass to <code>llm_strategies.py</code></li> <li>Add <code>include_metadata</code> parameter to <code>GeminiCachedStrategy.__init__</code></li> <li>Add <code>_extract_safety_ratings()</code> helper method</li> <li>Update <code>execute()</code> to return <code>GeminiResponse</code> when enabled</li> <li>Add tests for metadata extraction</li> <li>Update documentation with safety ratings example</li> </ol> <p>Test Coverage:</p> <ul> <li>Test metadata extraction with mocked Gemini response</li> <li>Test safety ratings parsing</li> <li>Test finish_reason extraction</li> <li>Test backward compatibility (include_metadata=False)</li> <li>Test with real Gemini API (integration test)</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#issue-6-cache-matching-precision","title":"Issue #6: Cache Matching Precision","text":"<p>Priority: Low Effort: Medium Breaking: No (enhancement)</p> <p>Problem:</p> <ul> <li>Current cache matching only checks model name (<code>.endswith()</code>)</li> <li>Will reuse cache even if prompt/content changed</li> <li>Can lead to inconsistent LLM instructions</li> </ul> <p>Current Behavior:</p> <pre><code># Finds first cache for model, ignores prompt content\nfor cache in caches:\n    if cache.model.endswith(self.model_name):\n        return cache  # May have different prompt!\n</code></pre> <p>Proposed Solution:</p> <p>Add cache tagging/metadata support:</p> <pre><code>class GeminiCachedStrategy:\n    def __init__(\n        self,\n        # ... existing params ...\n        cache_tags: dict[str, str] | None = None,  # NEW: Cache tags for matching\n    ):\n        self.cache_tags = cache_tags or {}\n\n    async def _create_cache(self):\n        \"\"\"Create new cache with tags.\"\"\"\n        # Check if API supports metadata\n        try:\n            config = CreateCachedContentConfig(\n                contents=self.cached_content,\n                ttl=f\"{self.cache_ttl_seconds}s\",\n                metadata=self.cache_tags,  # Add tags\n            )\n        except TypeError:\n            # Fallback for older API without metadata support\n            config = CreateCachedContentConfig(\n                contents=self.cached_content,\n                ttl=f\"{self.cache_ttl_seconds}s\",\n            )\n\n        return await self.client.aio.caches.create(\n            model=self.model,\n            config=config,\n        )\n\n    async def _find_existing_cache(self):\n        \"\"\"Find cache matching model AND tags.\"\"\"\n        caches = await self.client.aio.caches.list()\n\n        for cache in caches:\n            # Match model name\n            if not cache.model.endswith(self.model):\n                continue\n\n            # Match tags (if provided)\n            if self.cache_tags:\n                cache_metadata = getattr(cache, 'metadata', {})\n                if not all(\n                    cache_metadata.get(k) == v\n                    for k, v in self.cache_tags.items()\n                ):\n                    continue  # Tags don't match\n\n            return cache\n\n        return None\n</code></pre> <p>Usage:</p> <pre><code>strategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    cache_tags={\n        \"prompt_version\": \"v2\",\n        \"purpose\": \"enrichment\",\n        \"dataset\": \"openlibrary\",\n    },\n)\n</code></pre> <p>Implementation Steps:</p> <ol> <li>Add <code>cache_tags</code> parameter to <code>GeminiCachedStrategy.__init__</code></li> <li>Update <code>_create_cache()</code> to include metadata (with fallback)</li> <li>Update <code>_find_existing_cache()</code> to match on tags</li> <li>Add tests for tag matching</li> <li>Document cache tagging pattern</li> <li>Add migration note about tag matching</li> </ol> <p>Considerations:</p> <ul> <li>Check if google-genai API supports cache metadata (may be newer feature)</li> <li>Provide graceful fallback if metadata not supported</li> <li>Document that cache matching is best-effort</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#issue-8-per-work-item-state-for-advanced-retry-strategies","title":"Issue #8: Per-Work-Item State for Advanced Retry Strategies","text":"<p>Priority: High Effort: High Breaking: No (additive API change)</p> <p>Problem:</p> <ul> <li>Cannot implement sophisticated multi-stage retry strategies</li> <li>No way to persist state between retry attempts for same work item</li> <li>Shared strategies mean instance variables cause state collision</li> </ul> <p>Use Case:</p> <p>Multi-stage validation recovery:</p> <ol> <li>Stage 1: Full prompt at temp 0.0</li> <li>Stage 2: Partial recovery at temp 0.0 (fix only failed fields - 81% cheaper)</li> <li>Stage 3: Full prompt at temp 0.25</li> <li>Stage 4: Partial recovery at temp 0.25</li> </ol> <p>Different errors need different handling:</p> <ul> <li>ValidationError \u2192 advance to next stage</li> <li>Network error \u2192 retry same stage</li> <li>Limit total prompts to prevent runaway costs</li> </ul> <p>Proposed Solution:</p> <p>Add <code>RetryState</code> parameter to strategy methods:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass\nclass RetryState:\n    \"\"\"Mutable state that persists across retry attempts for a work item.\"\"\"\n    data: dict[str, Any] = field(default_factory=dict)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get value from state.\"\"\"\n        return self.data.get(key, default)\n\n    def set(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set value in state.\"\"\"\n        self.data[key] = value\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all state.\"\"\"\n        self.data.clear()\n\n# Update LLMCallStrategy protocol\nclass LLMCallStrategy(Protocol[TOutput]):\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,  # NEW: Optional state\n    ) -&gt; tuple[TOutput, dict[str, int]]:\n        ...\n\n    async def on_error(\n        self,\n        exception: Exception,\n        attempt: int,\n        state: RetryState | None = None,  # NEW: Optional state\n    ) -&gt; None:\n        ...\n</code></pre> <p>Framework Implementation:</p> <pre><code>class ParallelBatchProcessor:\n    def __init__(self, ...):\n        # ... existing init ...\n        self._retry_states: dict[str, RetryState] = {}  # item_id -&gt; state\n        self._retry_states_lock = asyncio.Lock()\n\n    async def _get_retry_state(self, item_id: str) -&gt; RetryState:\n        \"\"\"Get or create retry state for work item.\"\"\"\n        async with self._retry_states_lock:\n            if item_id not in self._retry_states:\n                self._retry_states[item_id] = RetryState()\n            return self._retry_states[item_id]\n\n    async def _clear_retry_state(self, item_id: str) -&gt; None:\n        \"\"\"Clear retry state after completion.\"\"\"\n        async with self._retry_states_lock:\n            self._retry_states.pop(item_id, None)\n\n    async def _process_item_with_retries(self, work_item):\n        # Get state for this work item\n        state = await self._get_retry_state(work_item.item_id)\n\n        try:\n            # ... retry loop ...\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    # Pass state to execute()\n                    output, token_usage = await strategy.execute(\n                        prompt=work_item.prompt,\n                        attempt=attempt,\n                        timeout=timeout,\n                        state=state,  # NEW\n                    )\n                    # Success - clear state\n                    await self._clear_retry_state(work_item.item_id)\n                    return success_result\n\n                except Exception as e:\n                    # Pass state to on_error()\n                    await strategy.on_error(e, attempt, state=state)  # NEW\n                    # ... retry logic ...\n\n        finally:\n            # Always clear state when done (success or failure)\n            await self._clear_retry_state(work_item.item_id)\n</code></pre> <p>Usage Example:</p> <pre><code>class MultiStageStrategy(LLMCallStrategy[Output]):\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,\n    ):\n        # Initialize state on first attempt\n        if state is None:\n            state = RetryState()\n\n        if attempt == 1:\n            state.set(\"stage\", 1)\n            state.set(\"total_prompts\", 0)\n\n        stage = state.get(\"stage\", 1)\n        total_prompts = state.get(\"total_prompts\", 0)\n\n        # Enforce total prompt limit\n        if total_prompts &gt;= 5:\n            raise ValueError(f\"Exceeded max prompts ({total_prompts}/5)\")\n\n        try:\n            if stage in [1, 3]:\n                # Full prompt stages\n                temp = 0.0 if stage == 1 else 0.25\n                result = await self._call_llm(prompt, temp)\n                state.set(\"total_prompts\", total_prompts + 1)\n                return result, tokens\n\n            elif stage in [2, 4]:\n                # Partial recovery stages (81% cheaper)\n                temp = 0.0 if stage == 2 else 0.25\n                last_error = state.get(\"last_validation_error\")\n                partial_data = state.get(\"partial_data\")\n                result = await self._partial_recovery(last_error, partial_data, temp)\n                state.set(\"total_prompts\", total_prompts + 1)\n                return result, tokens\n\n        except ValidationError as e:\n            # Validation error: advance to next stage\n            state.set(\"stage\", stage + 1)\n            state.set(\"last_validation_error\", str(e))\n            state.set(\"partial_data\", getattr(e, 'partial_data', None))\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Framework will retry with new stage\n\n        except (ConnectionError, TimeoutError):\n            # Network error: retry same stage\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Framework will retry same stage\n\n    async def on_error(\n        self,\n        exception: Exception,\n        attempt: int,\n        state: RetryState | None = None,\n    ):\n        if state:\n            logger.info(\n                f\"Attempt {attempt}, Stage {state.get('stage')}, \"\n                f\"Total prompts: {state.get('total_prompts')}\"\n            )\n</code></pre> <p>Implementation Steps:</p> <ol> <li>Create <code>RetryState</code> class in <code>batch_llm/core/retry_state.py</code></li> <li>Update <code>LLMCallStrategy</code> protocol with optional <code>state</code> parameter</li> <li>Add <code>_retry_states</code> dict to <code>ParallelBatchProcessor</code></li> <li>Add <code>_get_retry_state()</code> and <code>_clear_retry_state()</code> methods</li> <li>Update <code>_process_item_with_retries()</code> to pass state</li> <li>Update all built-in strategies to accept (but ignore) state parameter</li> <li>Add comprehensive tests for state management</li> <li>Add example multi-stage strategy to examples/</li> <li>Document retry state pattern in README</li> <li>Add migration guide section</li> </ol> <p>Test Coverage:</p> <ul> <li>Test state creation and retrieval</li> <li>Test state persistence across retries</li> <li>Test state clearing on success</li> <li>Test state clearing on final failure</li> <li>Test concurrent work items have isolated state</li> <li>Test multi-stage strategy example</li> <li>Test backward compatibility (strategies without state parameter)</li> </ul> <p>Backward Compatibility:</p> <ul> <li><code>state</code> parameter is optional with default <code>None</code></li> <li>Existing strategies work unchanged (they don't use state)</li> <li>No breaking changes to existing APIs</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#additional-enhancements","title":"Additional Enhancements","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#enhancement-1-prepareoncemixin-for-user-strategies","title":"Enhancement #1: PrepareOnceMixin for User Strategies","text":"<p>Priority: Low Effort: Low Breaking: No</p> <p>Rationale: While v0.2.0 fixed framework-level prepare() tracking, users writing custom strategies still need to implement idempotency manually. A mixin makes this easier.</p> <p>Proposed Solution:</p> <pre><code># batch_llm/mixins/prepare_once.py\nimport asyncio\nfrom typing import Any\n\nclass PrepareOnceMixin:\n    \"\"\"Mixin to ensure prepare() is called only once, even with concurrent calls.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self._prepare_once_lock = asyncio.Lock()\n        self._prepare_once_done = False\n\n    async def prepare(self) -&gt; None:\n        \"\"\"Call _prepare_once() exactly once, with thread-safe initialization.\"\"\"\n        async with self._prepare_once_lock:\n            if self._prepare_once_done:\n                return\n            await self._prepare_once()\n            self._prepare_once_done = True\n\n    async def _prepare_once(self) -&gt; None:\n        \"\"\"Override this method with your expensive initialization.\n\n        This will be called exactly once, even with concurrent prepare() calls.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement _prepare_once()\")\n</code></pre> <p>Usage:</p> <pre><code>from async_batch_llm.mixins import PrepareOnceMixin\nfrom async_batch_llm.llm_strategies import LLMCallStrategy\n\nclass MyCustomStrategy(PrepareOnceMixin, LLMCallStrategy[Output]):\n    async def _prepare_once(self) -&gt; None:\n        \"\"\"Expensive initialization - called exactly once.\"\"\"\n        self.connection = await create_connection()\n        self.cache = await create_cache()\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use self.connection and self.cache\n        pass\n</code></pre> <p>Note: With v0.2.0's framework-level tracking, this mixin is less critical but still useful for:</p> <ul> <li>Documentation/clarity of intent</li> <li>Extra safety layer</li> <li>Use outside of ParallelBatchProcessor</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#enhancement-2-improved-error-classifier-extensibility","title":"Enhancement #2: Improved Error Classifier Extensibility","text":"<p>Priority: Low Effort: Low Breaking: No</p> <p>Problem: Current pattern for extending error classifiers requires careful ordering:</p> <pre><code>class MyClassifier(GeminiErrorClassifier):\n    def classify(self, exception):\n        # MUST check custom cases BEFORE super().classify()\n        if \"cache expired\" in str(exception):\n            return ErrorInfo(...)\n        return super().classify(exception)  # AFTER custom checks\n</code></pre> <p>Proposed Solution:</p> <p>Add hooks for pre/post classification:</p> <pre><code>class ExtensibleErrorClassifier:\n    \"\"\"Base classifier with extension hooks.\"\"\"\n\n    def classify(self, exception: Exception) -&gt; ErrorInfo:\n        \"\"\"Final classification with extension hooks.\"\"\"\n        # Pre-hook: Check custom patterns first\n        pre_result = self.classify_custom(exception)\n        if pre_result is not None:\n            return pre_result\n\n        # Default classification\n        result = self.classify_default(exception)\n\n        # Post-hook: Modify default result if needed\n        return self.post_classify(exception, result)\n\n    def classify_custom(self, exception: Exception) -&gt; ErrorInfo | None:\n        \"\"\"Override to add custom classification logic.\n\n        Return ErrorInfo to override default classification.\n        Return None to fall through to default classification.\n        \"\"\"\n        return None\n\n    def classify_default(self, exception: Exception) -&gt; ErrorInfo:\n        \"\"\"Default classification logic. Override in subclasses.\"\"\"\n        raise NotImplementedError()\n\n    def post_classify(self, exception: Exception, info: ErrorInfo) -&gt; ErrorInfo:\n        \"\"\"Override to modify classification after default logic.\n\n        Useful for adjusting retry behavior based on context.\n        \"\"\"\n        return info\n</code></pre> <p>Usage:</p> <pre><code>class MyCachedClassifier(GeminiErrorClassifier):\n    def classify_custom(self, exception):\n        \"\"\"Custom checks run FIRST.\"\"\"\n        if \"cache expired\" in str(exception):\n            return ErrorInfo(is_retryable=True, error_category=\"cache_expired\")\n        return None  # Fall through to default Gemini classification\n\n    def post_classify(self, exception, info):\n        \"\"\"Modify default results if needed.\"\"\"\n        # Example: Make all validation errors use higher temp on retry\n        if info.error_category == \"validation_error\":\n            info.suggested_temperature = 0.5\n        return info\n</code></pre> <p>Implementation:</p> <ul> <li>Add to <code>strategies/errors.py</code></li> <li>Update <code>GeminiErrorClassifier</code> and <code>DefaultErrorClassifier</code> to use pattern</li> <li>Add documentation example</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#release-checklist","title":"Release Checklist","text":""},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#phase-1-issue-8-retry-state-high-priority","title":"Phase 1: Issue #8 - Retry State (High Priority)","text":"<ul> <li>[ ] Implement <code>RetryState</code> class</li> <li>[ ] Update <code>LLMCallStrategy</code> protocol</li> <li>[ ] Update <code>ParallelBatchProcessor</code> with state management</li> <li>[ ] Update built-in strategies for compatibility</li> <li>[ ] Add comprehensive tests (15+ tests)</li> <li>[ ] Add multi-stage example to <code>examples/</code></li> <li>[ ] Update README with retry state documentation</li> <li>[ ] Update API.md with RetryState API</li> <li>[ ] Add to MIGRATION_V0_3.md</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#phase-2-issue-3-safety-ratings-medium-priority","title":"Phase 2: Issue #3 - Safety Ratings (Medium Priority)","text":"<ul> <li>[ ] Implement <code>GeminiResponse</code> dataclass</li> <li>[ ] Add <code>include_metadata</code> parameter</li> <li>[ ] Add <code>_extract_safety_ratings()</code> method</li> <li>[ ] Update <code>execute()</code> to return wrapped response</li> <li>[ ] Add tests for metadata extraction (8+ tests)</li> <li>[ ] Add safety ratings example to examples/</li> <li>[ ] Update README with safety ratings section</li> <li>[ ] Update GEMINI_INTEGRATION.md with metadata docs</li> <li>[ ] Add to MIGRATION_V0_3.md</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#phase-3-issue-6-cache-tagging-low-priority","title":"Phase 3: Issue #6 - Cache Tagging (Low Priority)","text":"<ul> <li>[ ] Add <code>cache_tags</code> parameter</li> <li>[ ] Update <code>_create_cache()</code> with metadata</li> <li>[ ] Update <code>_find_existing_cache()</code> with tag matching</li> <li>[ ] Add graceful fallback for older API</li> <li>[ ] Add tests for tag matching (6+ tests)</li> <li>[ ] Document cache tagging pattern</li> <li>[ ] Add example to GEMINI_INTEGRATION.md</li> <li>[ ] Add to MIGRATION_V0_3.md</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#phase-4-enhancements-optional","title":"Phase 4: Enhancements (Optional)","text":"<ul> <li>[ ] Implement <code>PrepareOnceMixin</code></li> <li>[ ] Add tests for PrepareOnceMixin</li> <li>[ ] Document mixin usage</li> <li>[ ] Consider ExtensibleErrorClassifier pattern</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#phase-5-documentation-release","title":"Phase 5: Documentation &amp; Release","text":"<ul> <li>[ ] Update CHANGELOG.md with v0.3.0 changes</li> <li>[ ] Create MIGRATION_V0_3.md</li> <li>[ ] Update README.md with new features</li> <li>[ ] Update API.md with new APIs</li> <li>[ ] Run full test suite (target: 117+ tests, 80%+ coverage)</li> <li>[ ] Run linters and type checkers</li> <li>[ ] Update version in pyproject.toml to 0.3.0</li> <li>[ ] Create git tag v0.3.0</li> <li>[ ] Build and publish to PyPI</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#breaking-changes","title":"Breaking Changes","text":"<p>None planned. All changes are additive and backward compatible:</p> <ul> <li><code>state</code> parameter is optional (defaults to <code>None</code>)</li> <li><code>include_metadata</code> is opt-in (defaults to <code>False</code>)</li> <li><code>cache_tags</code> is optional (defaults to empty dict)</li> <li>Existing strategies and code work unchanged</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#success-criteria","title":"Success Criteria","text":"<ol> <li>Retry State:</li> <li>Users can implement multi-stage retry strategies</li> <li>State is isolated per work item (no collisions)</li> <li> <p>Example demonstrates 81% cost savings with partial recovery</p> </li> <li> <p>Safety Ratings:</p> </li> <li>Users can access Gemini safety ratings</li> <li>Opt-in feature doesn't affect existing code</li> <li> <p>Clear example for content filtering</p> </li> <li> <p>Cache Tagging:</p> </li> <li>Users can tag caches for precise matching</li> <li>Graceful fallback if API doesn't support metadata</li> <li> <p>Prevents accidental cache reuse with different prompts</p> </li> <li> <p>Quality:</p> </li> <li>All tests passing (117+ tests)</li> <li>Coverage maintained or improved (80%+)</li> <li>No regressions in existing functionality</li> <li>Clear migration guide for upgrading</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#timeline-estimate","title":"Timeline Estimate","text":"<ul> <li>Issue #8 (Retry State): 2-3 days</li> <li>Issue #3 (Safety Ratings): 1-2 days</li> <li>Issue #6 (Cache Tagging): 1 day</li> <li>Documentation &amp; Testing: 1-2 days</li> <li>Total: 5-8 days</li> </ul>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#open-questions","title":"Open Questions","text":"<ol> <li>Retry State Cleanup: Should state be cleared on final failure, or preserved for post-mortem analysis?</li> <li> <p>Recommendation: Clear on both success and failure, but log state to logger.debug() before clearing</p> </li> <li> <p>Safety Ratings API: Does google-genai consistently provide safety_ratings, or only sometimes?</p> </li> <li> <p>Action: Test with various Gemini models and prompts</p> </li> <li> <p>Cache Metadata Support: What version of google-genai added metadata support?</p> </li> <li> <p>Action: Check google-genai changelog and add version check if needed</p> </li> <li> <p>State Serialization: Should RetryState support JSON serialization for debugging?</p> </li> <li>Recommendation: Yes, add <code>to_dict()</code> and <code>from_dict()</code> methods</li> </ol>"},{"location":"archive/IMPLEMENTATION_PLAN_V0_3/#post-v030-backlog","title":"Post-v0.3.0 Backlog","text":"<p>Items to consider for future versions:</p> <ul> <li>Strategy Lifecycle Management: Explicit <code>register_strategy()</code> pattern (Issue from feedback)</li> <li>Prometheus Metrics: Built-in Prometheus endpoint (mentioned in features)</li> <li>Dynamic Worker Scaling: Adjust workers based on load (mentioned in future enhancements)</li> <li>Persistent Queue: Redis/DB-backed queue (mentioned in limitations)</li> <li>Batch API Support: True batch API for 50% cost savings (mentioned in limitations)</li> <li>Multi-Process Support: Distributed locks for multi-process scenarios (mentioned in limitations)</li> <li>More Error Classifiers: OpenAI, Anthropic, etc. (mentioned in contributing)</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/","title":"batch-llm Improvement Plan","text":"<p>Created: 2025-01-10 Based on: Package Review (PACKAGE_REVIEW_2025_01_10.md) Current Version: 0.3.0 Target Version: 0.3.1 (patches), 0.4.0 (features)</p>"},{"location":"archive/IMPROVEMENT_PLAN/#overview","title":"Overview","text":"<p>This document outlines a prioritized plan for improving the batch-llm package based on a comprehensive review that identified 27 opportunities for improvement across code quality, testing, documentation, performance, and user experience.</p> <p>Current Status: Production-ready (8/10 score) Goal: Increase to 9/10 with focused improvements</p>"},{"location":"archive/IMPROVEMENT_PLAN/#phase-1-critical-fixes-week-1","title":"Phase 1: Critical Fixes (Week 1)","text":"<p>Target: v0.3.1 patch release Goal: Fix critical issues that affect correctness and user experience</p>"},{"location":"archive/IMPROVEMENT_PLAN/#11-fix-version-mismatch-critical","title":"1.1 Fix Version Mismatch \u26a0\ufe0f CRITICAL","text":"<p>Priority: P0 - BLOCKER Effort: 15 minutes Owner: TBD</p> <p>Problem:</p> <ul> <li><code>pyproject.toml</code> declares <code>version = \"0.2.0\"</code></li> <li><code>src/batch_llm/__init__.py</code> declares <code>__version__ = \"0.1.0\"</code></li> <li>Users checking <code>batch_llm.__version__</code> get wrong information</li> </ul> <p>Implementation:</p> <ol> <li>Update <code>src/batch_llm/__init__.py</code>:</li> </ol> <pre><code># Remove hardcoded version\n# OLD: __version__ = \"0.1.0\"\n\n# NEW: Use importlib.metadata\nfrom importlib.metadata import version, PackageNotFoundError\n\ntry:\n    __version__ = version(\"batch-llm\")\nexcept PackageNotFoundError:\n    # Package not installed (e.g., running from source)\n    __version__ = \"0.0.0+dev\"\n</code></pre> <ol> <li>Update <code>pyproject.toml</code> to correct version:</li> </ol> <pre><code>version = \"0.3.0\"  # Match actual release\n</code></pre> <ol> <li>Add test:</li> </ol> <pre><code>def test_version_matches_package():\n    \"\"\"Verify __version__ matches package metadata.\"\"\"\n    from async_batch_llm import __version__\n    from importlib.metadata import version\n    assert __version__ == version(\"batch-llm\")\n</code></pre> <p>Validation:</p> <ul> <li>Run <code>python -c \"import async_batch_llm; print(batch_llm.__version__)\"</code></li> <li>Should print \"0.3.0\"</li> <li>Test passes in CI</li> </ul> <p>Related Issues: None Documentation: Update CHANGELOG.md with fix</p>"},{"location":"archive/IMPROVEMENT_PLAN/#12-add-shared-strategy-lifecycle-tests","title":"1.2 Add Shared Strategy Lifecycle Tests","text":"<p>Priority: P0 - CRITICAL Effort: 2-3 hours Owner: TBD</p> <p>Problem:</p> <ul> <li>v0.2.0 shared strategy optimization (70-90% cost savings) has no tests</li> <li>Critical feature could break silently</li> <li><code>_ensure_strategy_prepared()</code> logic at <code>parallel.py:162-194</code> untested</li> </ul> <p>Implementation:</p> <p>Create <code>tests/test_shared_strategy_lifecycle.py</code>:</p> <pre><code>\"\"\"Tests for shared strategy lifecycle (v0.2.0 feature).\"\"\"\n\nimport asyncio\nimport pytest\nfrom async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom async_batch_llm.llm_strategies import LLMCallStrategy\n\n\n@pytest.mark.asyncio\nasync def test_shared_strategy_prepare_called_once():\n    \"\"\"Verify prepare() called exactly once for shared strategy.\"\"\"\n    prepare_count = 0\n    cleanup_count = 0\n    execute_count = 0\n\n    class CountingStrategy(LLMCallStrategy[str]):\n        async def prepare(self):\n            nonlocal prepare_count\n            prepare_count += 1\n            await asyncio.sleep(0.01)  # Simulate slow prepare\n\n        async def cleanup(self):\n            nonlocal cleanup_count\n            cleanup_count += 1\n\n        async def execute(self, prompt, attempt, timeout, state):\n            nonlocal execute_count\n            execute_count += 1\n            return f\"Result: {prompt}\", {\"input_tokens\": 10, \"output_tokens\": 5}\n\n    strategy = CountingStrategy()  # One instance\n    config = ProcessorConfig(max_workers=5, timeout_per_item=10.0)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        # Add 100 items all sharing the same strategy\n        for i in range(100):\n            await processor.add_work(LLMWorkItem(\n                item_id=f\"item_{i}\",\n                strategy=strategy,  # SHARED\n                prompt=f\"Test {i}\"\n            ))\n\n        result = await processor.process_all()\n\n    # Critical assertions\n    assert result.total_items == 100\n    assert result.succeeded == 100\n    assert prepare_count == 1, \"prepare() should be called exactly once\"\n    assert execute_count == 100, \"execute() should be called per item\"\n    assert cleanup_count == 100, \"cleanup() should be called per item\"\n\n\n@pytest.mark.asyncio\nasync def test_shared_strategy_concurrent_prepare():\n    \"\"\"Verify no race condition with concurrent workers.\"\"\"\n    prepare_count = 0\n    prepare_lock = asyncio.Lock()\n\n    class ConcurrentStrategy(LLMCallStrategy[str]):\n        async def prepare(self):\n            nonlocal prepare_count\n            async with prepare_lock:\n                prepare_count += 1\n            await asyncio.sleep(0.1)  # Long prepare to stress test\n\n        async def execute(self, prompt, attempt, timeout, state):\n            return \"Result\", {\"input_tokens\": 10, \"output_tokens\": 5}\n\n    strategy = ConcurrentStrategy()\n    config = ProcessorConfig(max_workers=20, timeout_per_item=10.0)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        for i in range(100):\n            await processor.add_work(LLMWorkItem(\n                item_id=f\"item_{i}\",\n                strategy=strategy,\n                prompt=f\"Test {i}\"\n            ))\n\n        result = await processor.process_all()\n\n    assert result.succeeded == 100\n    assert prepare_count == 1, \"Race condition: prepare() called multiple times\"\n\n\n@pytest.mark.asyncio\nasync def test_different_strategies_prepare_separately():\n    \"\"\"Verify different strategy instances each get prepare() called.\"\"\"\n    prepare_counts = {}\n\n    class TrackedStrategy(LLMCallStrategy[str]):\n        def __init__(self, name: str):\n            self.name = name\n\n        async def prepare(self):\n            prepare_counts[self.name] = prepare_counts.get(self.name, 0) + 1\n\n        async def execute(self, prompt, attempt, timeout, state):\n            return f\"{self.name}: {prompt}\", {\"input_tokens\": 10}\n\n    strategy_a = TrackedStrategy(\"A\")\n    strategy_b = TrackedStrategy(\"B\")\n    strategy_c = TrackedStrategy(\"C\")\n\n    config = ProcessorConfig(max_workers=5, timeout_per_item=10.0)\n\n    async with ParallelBatchProcessor[str, str, None](config=config) as processor:\n        # Add items with different strategies\n        for i in range(10):\n            await processor.add_work(LLMWorkItem(\n                item_id=f\"a_{i}\", strategy=strategy_a, prompt=\"Test\"\n            ))\n        for i in range(10):\n            await processor.add_work(LLMWorkItem(\n                item_id=f\"b_{i}\", strategy=strategy_b, prompt=\"Test\"\n            ))\n        for i in range(10):\n            await processor.add_work(LLMWorkItem(\n                item_id=f\"c_{i}\", strategy=strategy_c, prompt=\"Test\"\n            ))\n\n        result = await processor.process_all()\n\n    assert result.total_items == 30\n    assert result.succeeded == 30\n    assert prepare_counts[\"A\"] == 1\n    assert prepare_counts[\"B\"] == 1\n    assert prepare_counts[\"C\"] == 1\n</code></pre> <p>Validation:</p> <ul> <li>All tests pass with 100% success rate</li> <li>Run with <code>pytest tests/test_shared_strategy_lifecycle.py -v</code></li> <li>Add to CI pipeline</li> </ul> <p>Related Issues: #1 (from v0.2.0) Documentation: Update README with test results</p>"},{"location":"archive/IMPROVEMENT_PLAN/#13-document-shared-strategy-cost-optimization","title":"1.3 Document Shared Strategy Cost Optimization","text":"<p>Priority: P0 - CRITICAL Effort: 1-2 hours Owner: TBD</p> <p>Problem:</p> <ul> <li>Users might create new strategy instances per item</li> <li>Miss 70-90% cost savings with Gemini caching</li> <li>Not prominently documented in README</li> </ul> <p>Implementation:</p>"},{"location":"archive/IMPROVEMENT_PLAN/#131-update-readmemd","title":"1.3.1 Update README.md","text":"<p>Add prominent section in \"Quick Start\" or \"Features\":</p> <pre><code>## \ud83d\udcb0 Critical: Shared Strategies for Cost Optimization\n\n**IMPORTANT:** When using `GeminiCachedStrategy`, reuse the SAME strategy instance across all work items to share the cache. This provides **70-90% cost savings**.\n\n### \u274c Wrong: New Strategy Per Item (Expensive!)\n\n```python\n# This creates a NEW cache for each item - wastes money!\nfor document in documents:\n    strategy = GeminiCachedStrategy(...)  # NEW INSTANCE per loop\n    work_item = LLMWorkItem(\n        item_id=document.id,\n        strategy=strategy,  # Different strategy each time\n        prompt=format_prompt(document)\n    )\n    await processor.add_work(work_item)\n</code></pre> <p>Cost: 100 items \u00d7 $0.10 per item = $10.00</p>"},{"location":"archive/IMPROVEMENT_PLAN/#right-reuse-strategy-70-90-savings","title":"\u2705 Right: Reuse Strategy (70-90% Savings!)","text":"<pre><code># Create ONE strategy, reuse for all items\nstrategy = GeminiCachedStrategy(...)  # ONE INSTANCE\n\nfor document in documents:\n    work_item = LLMWorkItem(\n        item_id=document.id,\n        strategy=strategy,  # REUSE same strategy\n        prompt=format_prompt(document)\n    )\n    await processor.add_work(work_item)\n</code></pre> <p>Cost: 100 items \u00d7 $0.03 per item (with caching) = $3.00 \u2705</p> <p>Savings: $7.00 (70%) on this batch alone!</p>"},{"location":"archive/IMPROVEMENT_PLAN/#how-it-works","title":"How It Works","text":"<p>When you reuse a strategy:</p> <ol> <li>Framework calls <code>prepare()</code> once (creates one cache)</li> <li>All work items share that cache</li> <li>Cached tokens get 90% discount from Gemini</li> <li>Overall cost reduction: 70-90%</li> </ol> <p>See the Shared Strategies documentation for details.</p> <pre><code>(end of markdown code block)\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#132-update-geminicachedstrategy-docstring","title":"1.3.2 Update GeminiCachedStrategy Docstring","text":"<p>Add to <code>src/batch_llm/llm_strategies.py</code>:</p> <pre><code>class GeminiCachedStrategy(LLMCallStrategy[TOutput]):\n    \"\"\"\n    Gemini strategy with automatic context caching (v0.2.0).\n\n    **CRITICAL FOR COST OPTIMIZATION:**\n    Create ONE instance and reuse it across all work items to share the cache.\n    This provides 70-90% cost savings compared to creating new instances per item.\n\n    Example (CORRECT usage):\n        &gt;&gt;&gt; # Create one strategy\n        &gt;&gt;&gt; strategy = GeminiCachedStrategy(\n        ...     model=\"gemini-2.0-flash\",\n        ...     client=client,\n        ...     response_parser=lambda r: str(r.text),\n        ...     cached_content=[system_instruction, context_docs],\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Reuse for all items\n        &gt;&gt;&gt; for doc in documents:\n        ...     work_item = LLMWorkItem(strategy=strategy, ...)  # REUSE\n        ...     await processor.add_work(work_item)\n\n    Wrong usage (creates new cache per item):\n        &gt;&gt;&gt; for doc in documents:\n        ...     strategy = GeminiCachedStrategy(...)  # NEW instance - expensive!\n        ...     work_item = LLMWorkItem(strategy=strategy, ...)\n\n    Cost comparison (100 items with 500 cached tokens):\n    - Wrong: $10.00 (no caching benefit)\n    - Right: $3.00 (70% savings)\n    \"\"\"\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#133-add-faq-entry","title":"1.3.3 Add FAQ Entry","text":"<p>Add to README FAQ:</p> <pre><code>### Why are my costs so high with GeminiCachedStrategy?\n\nYou're probably creating a new strategy instance for each work item:\n\n```python\n# \u274c WRONG - creates new cache per item\nfor item in items:\n    strategy = GeminiCachedStrategy(...)  # NEW instance\n    await processor.add_work(LLMWorkItem(strategy=strategy, ...))\n</code></pre> <p>Instead, create ONE strategy and reuse it:</p> <pre><code># \u2705 RIGHT - shares cache across all items (70-90% savings)\nstrategy = GeminiCachedStrategy(...)  # ONE instance\nfor item in items:\n    await processor.add_work(LLMWorkItem(strategy=strategy, ...))  # REUSE\n</code></pre> <p>See the Cost Optimization section for details.</p> <pre><code>(end of markdown code block)\n</code></pre> <p>Validation:</p> <ul> <li>README section is visible and prominent</li> <li>Docstring shows up in IDE hover</li> <li>FAQ addresses common mistake</li> </ul> <p>Related Issues: User confusion from production usage Documentation: Update CHANGELOG.md noting documentation improvements</p>"},{"location":"archive/IMPROVEMENT_PLAN/#phase-2-high-impact-improvements-sprint-1","title":"Phase 2: High-Impact Improvements (Sprint 1)","text":"<p>Target: v0.3.1 or v0.3.2 Goal: Improve code quality, type safety, and test coverage</p>"},{"location":"archive/IMPROVEMENT_PLAN/#21-reduce-type-ignores-21-10","title":"2.1 Reduce Type Ignores (21 \u2192 &lt;10)","text":"<p>Priority: P1 - HIGH Effort: 4-6 hours Owner: TBD</p> <p>Problem: 21 <code># type: ignore</code> comments mask potential type safety issues</p> <p>Implementation:</p>"},{"location":"archive/IMPROVEMENT_PLAN/#step-1-fix-tokenusage-default-factory","title":"Step 1: Fix TokenUsage Default Factory","text":"<p>File: <code>src/batch_llm/base.py</code></p> <pre><code># Before\ntoken_usage: TokenUsage = field(default_factory=dict)  # type: ignore[assignment]\n\n# After\ndef _empty_token_usage() -&gt; TokenUsage:\n    \"\"\"Create an empty token usage dict with proper types.\"\"\"\n    return {\n        \"input_tokens\": 0,\n        \"output_tokens\": 0,\n        \"total_tokens\": 0,\n        \"cached_input_tokens\": 0,\n    }\n\n@dataclass\nclass WorkItemResult(Generic[TOutput, TContext]):\n    token_usage: TokenUsage = field(default_factory=_empty_token_usage)\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#step-2-fix-optional-import-type-ignores","title":"Step 2: Fix Optional Import Type Ignores","text":"<p>File: <code>src/batch_llm/llm_strategies.py</code></p> <pre><code># Before\ntry:\n    from pydantic_ai import Agent\nexcept ImportError:\n    Agent = Any  # type: ignore[misc,assignment]\n\n# After\ntry:\n    from pydantic_ai import Agent as PydanticAgent\n    Agent = PydanticAgent\nexcept ImportError:\n    from typing import TYPE_CHECKING\n    if TYPE_CHECKING:\n        from pydantic_ai import Agent\n    else:\n        Agent = Any  # Only ignore when not type checking\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#step-3-fix-google-api-type-stubs","title":"Step 3: Fix Google API Type Stubs","text":"<p>File: <code>src/batch_llm/llm_strategies.py</code></p> <p>For incomplete third-party type stubs, use <code>cast()</code>:</p> <pre><code># Before\nself._cache = await self.client.aio.caches.create(...)  # type: ignore[call-arg]\n\n# After\nfrom typing import cast\nself._cache = cast(\n    \"CachedContent\",\n    await self.client.aio.caches.create(...)\n)\n</code></pre> <p>Target: Reduce from 21 to &lt;10 type ignores Validation: Run <code>mypy src/batch_llm/ --ignore-missing-imports</code></p>"},{"location":"archive/IMPROVEMENT_PLAN/#22-fix-line-length-violations","title":"2.2 Fix Line Length Violations","text":"<p>Priority: P1 - HIGH Effort: 1 hour Owner: TBD</p> <p>Files: <code>base.py</code>, <code>parallel.py</code>, others</p> <p>Implementation:</p> <p>Run ruff and fix all E501 violations:</p> <pre><code>uv run ruff check src/ tests/ --select E501 --fix\n</code></pre> <p>Manual fixes for complex cases:</p> <pre><code># Before (113 chars)\nf\"item_id must be a non-empty string (got {type(self.item_id).__name__}: {repr(self.item_id)}). \"\n\n# After\nraise ValueError(\n    f\"item_id must be a non-empty string \"\n    f\"(got {type(self.item_id).__name__}: {repr(self.item_id)}). \"\n    f\"Provide a unique string identifier for this work item.\"\n)\n</code></pre> <p>Validation: <code>ruff check src/ tests/</code> shows 0 E501 violations</p>"},{"location":"archive/IMPROVEMENT_PLAN/#23-add-resource-cleanup-for-failed-prepare","title":"2.3 Add Resource Cleanup for Failed prepare()","text":"<p>Priority: P1 - HIGH Effort: 1 hour Owner: TBD</p> <p>File: <code>src/batch_llm/parallel.py:625-730</code></p> <p>Implementation:</p> <pre><code>async def _process_item_with_retries(\n    self, work_item: LLMWorkItem[TInput, TOutput, TContext], worker_id: int\n) -&gt; WorkItemResult[TOutput, TContext]:\n    \"\"\"Wrapper that applies retry logic and strategy lifecycle.\"\"\"\n    strategy = self._get_strategy(work_item)\n    retry_state = RetryState()\n\n    # Track if prepare succeeded for cleanup decision\n    prepare_succeeded = False\n\n    try:\n        # Ensure strategy is prepared\n        await self._ensure_strategy_prepared(strategy)\n        prepare_succeeded = True\n\n        # Process with retries\n        for attempt in range(1, self.config.retry.max_attempts + 1):\n            try:\n                return await self._process_item(\n                    work_item, worker_id, attempt_number=attempt,\n                    strategy=strategy, retry_state=retry_state\n                )\n            except Exception as e:\n                # ... existing retry logic ...\n\n    except Exception as e:\n        # ... existing error handling ...\n        raise\n\n    finally:\n        # Always cleanup, even if prepare failed\n        if strategy is not None:\n            try:\n                await strategy.cleanup()\n            except Exception as cleanup_error:\n                logger.warning(\n                    f\"Cleanup failed for {work_item.item_id}: {cleanup_error}\"\n                )\n</code></pre> <p>Validation: Test that cleanup() called even when prepare() fails</p>"},{"location":"archive/IMPROVEMENT_PLAN/#24-document-worker-count-selection","title":"2.4 Document Worker Count Selection","text":"<p>Priority: P1 - HIGH Effort: 1 hour Owner: TBD</p> <p>Implementation:</p> <p>Add section to README:</p> <pre><code>## Configuration Guide\n\n### Choosing max_workers\n\nThe optimal worker count depends on your use case:\n\n#### 1. Rate-Limited APIs (OpenAI, Anthropic, Gemini)\n\n**Recommended:** 5-10 workers\n\n```python\nconfig = ProcessorConfig(max_workers=5)\n</code></pre> <p>Why: Too many workers hit rate limits immediately. Start with 5, increase gradually while monitoring <code>rate_limit_count</code> in metrics.</p> <p>With proactive rate limiting:</p> <pre><code>config = ProcessorConfig(\n    max_workers=4,  # Conservative: 300 req/min \u00f7 60 \u00d7 0.8 buffer\n    max_requests_per_minute=300,  # Set based on your API tier\n)\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#2-unlimited-apis-local-models-self-hosted","title":"2. Unlimited APIs (Local Models, Self-Hosted)","text":"<p>Recommended: <code>min(cpu_count() * 2, 20)</code></p> <pre><code>import os\nconfig = ProcessorConfig(max_workers=min(os.cpu_count() * 2, 20))\n</code></pre> <p>Why: CPU-bound operations benefit from more workers up to 2\u00d7 CPU count. Cap at 20 to avoid diminishing returns.</p>"},{"location":"archive/IMPROVEMENT_PLAN/#3-testing-and-debugging","title":"3. Testing and Debugging","text":"<p>Recommended: 2 workers</p> <pre><code>config = ProcessorConfig(max_workers=2)\n</code></pre> <p>Why: Easier to debug with less concurrency. Logs are easier to follow.</p>"},{"location":"archive/IMPROVEMENT_PLAN/#4-monitoring-and-tuning","title":"4. Monitoring and Tuning","text":"<p>Monitor these metrics to optimize:</p> <pre><code>result = await processor.process_all()\nstats = await processor.get_stats()\n\nprint(f\"Rate limits hit: {stats['rate_limit_count']}\")\nprint(f\"Items/sec: {result.total_items / stats['duration']:.2f}\")\n</code></pre> <ul> <li>If rate_limit_count &gt; 0: Reduce workers or enable proactive rate limiting</li> <li>If items/sec is low: Check if workers are idle (increase count)</li> <li>If errors spike: Check logs for timeout or validation issues</li> </ul> <pre><code>(end of markdown code block)\n</code></pre> <p>Validation: User feedback on clarity</p>"},{"location":"archive/IMPROVEMENT_PLAN/#25-add-tests-for-cache-tag-matching-v030","title":"2.5 Add Tests for Cache Tag Matching (v0.3.0)","text":"<p>Priority: P1 - HIGH Effort: 2 hours Owner: TBD</p> <p>Implementation:</p> <p>Add to <code>tests/test_v0_3_features.py</code> or create <code>tests/test_cache_tags.py</code>:</p> <pre><code>@pytest.mark.asyncio\nasync def test_cache_tags_matching_real_caches():\n    \"\"\"Test that cache tags correctly filter cache matches.\"\"\"\n    try:\n        from async_batch_llm.llm_strategies import GeminiCachedStrategy\n        import google.genai as genai\n        from google.genai.types import Content\n    except ImportError:\n        pytest.skip(\"google-genai not installed\")\n\n    # This test uses actual cache creation logic (no API calls)\n    # Tests the matching algorithm in _find_or_create_cache()\n\n    # Mock cache objects\n    class MockCache:\n        def __init__(self, model: str, metadata: dict):\n            self.model = model\n            self.metadata = metadata or {}\n            self.name = f\"cache_{model}_{id(self)}\"\n\n    # Test: Matching tags should reuse cache\n    strategy_a = GeminiCachedStrategy(\n        model=\"gemini-2.0-flash\",\n        client=None,  # Won't actually call API\n        response_parser=lambda x: str(x),\n        cached_content=[],\n        cache_tags={\"customer\": \"acme\", \"version\": \"v1\"}\n    )\n\n    # Simulate existing cache with matching tags\n    existing_cache = MockCache(\n        model=\"gemini-2.0-flash\",\n        metadata={\"customer\": \"acme\", \"version\": \"v1\"}\n    )\n\n    # Test matching logic\n    tags_match = all(\n        existing_cache.metadata.get(k) == v\n        for k, v in strategy_a.cache_tags.items()\n    )\n    assert tags_match, \"Tags should match\"\n\n    # Test: Different tags should NOT reuse cache\n    existing_cache_b = MockCache(\n        model=\"gemini-2.0-flash\",\n        metadata={\"customer\": \"globex\", \"version\": \"v1\"}\n    )\n\n    tags_match_b = all(\n        existing_cache_b.metadata.get(k) == v\n        for k, v in strategy_a.cache_tags.items()\n    )\n    assert not tags_match_b, \"Tags should NOT match\"\n</code></pre> <p>Validation: Tests pass, cover all tag matching scenarios</p>"},{"location":"archive/IMPROVEMENT_PLAN/#phase-3-performance-polish-sprint-2","title":"Phase 3: Performance &amp; Polish (Sprint 2)","text":"<p>Target: v0.4.0 Goal: Optimize performance and improve reliability</p>"},{"location":"archive/IMPROVEMENT_PLAN/#31-optimize-slow-start-calculations","title":"3.1 Optimize Slow-Start Calculations","text":"<p>Priority: P2 - MEDIUM Effort: 2-3 hours Impact: 5-10% performance improvement</p> <p>File: <code>src/batch_llm/parallel.py:300-314</code></p> <p>Implementation:</p> <pre><code>class ParallelBatchProcessor:\n    def __init__(self, config: ProcessorConfig, ...):\n        # ... existing init ...\n\n        # Pre-calculate slow-start delays (v0.4.0 optimization)\n        self._slow_start_delays: list[float] = []\n        if self.rate_limit_strategy.slow_start_items &gt; 0:\n            for i in range(self.rate_limit_strategy.slow_start_items):\n                _, delay = self.rate_limit_strategy.should_apply_slow_start(i)\n                self._slow_start_delays.append(delay)\n\n    async def _worker(self, worker_id: int) -&gt; None:\n        \"\"\"Worker that processes items from queue.\"\"\"\n        while True:\n            # ... get work_item ...\n\n            # Optimized slow-start (v0.4.0)\n            should_delay = False\n            delay = 0.0\n\n            async with self._rate_limit_lock:\n                if self._slow_start_active:\n                    idx = self._items_since_resume\n                    if idx &lt; len(self._slow_start_delays):\n                        delay = self._slow_start_delays[idx]\n                        should_delay = True\n                        self._items_since_resume += 1\n                    else:\n                        self._slow_start_active = False\n\n            if should_delay:\n                await asyncio.sleep(delay)\n\n            # ... process item ...\n</code></pre> <p>Validation:</p> <ul> <li>Benchmark with 20 workers, 1000 items</li> <li>Measure lock contention reduction</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#32-add-debug-logging-to-token-extraction","title":"3.2 Add Debug Logging to Token Extraction","text":"<p>Priority: P2 - MEDIUM Effort: 30 minutes</p> <p>File: <code>src/batch_llm/parallel.py:550-608</code></p> <p>Implementation:</p> <pre><code>def _extract_token_usage(self, exception: Exception) -&gt; TokenUsage:\n    \"\"\"Extract token usage from exception chain.\"\"\"\n    try:\n        # ... existing extraction logic ...\n    except Exception as e:\n        logger.debug(\n            f\"Failed to extract token usage from {type(exception).__name__}: {e}. \"\n            \"Returning 0 tokens. This is normal for non-LLM exceptions.\"\n        )\n\n    return {\"input_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0}\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#33-add-validation-for-cache-parameters","title":"3.3 Add Validation for Cache Parameters","text":"<p>Priority: P2 - MEDIUM Effort: 30 minutes</p> <p>File: <code>src/batch_llm/llm_strategies.py:356-406</code></p> <p>Implementation:</p> <pre><code>def __init__(\n    self,\n    model: str,\n    # ... other params ...\n    cache_ttl_seconds: int = 3600,\n    cache_renewal_buffer_seconds: int = 300,\n    # ... other params ...\n):\n    # Validate buffer &lt; ttl\n    if cache_renewal_buffer_seconds &gt;= cache_ttl_seconds:\n        raise ValueError(\n            f\"cache_renewal_buffer_seconds ({cache_renewal_buffer_seconds}) \"\n            f\"must be less than cache_ttl_seconds ({cache_ttl_seconds}). \"\n            f\"Typical value: 5-10 minutes (300-600 seconds).\"\n        )\n\n    # ... rest of init ...\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#34-improve-error-context","title":"3.4 Improve Error Context","text":"<p>Priority: P2 - MEDIUM Effort: 1 hour</p> <p>File: <code>src/batch_llm/strategies/errors.py</code></p> <p>Implementation:</p> <pre><code>class FrameworkTimeoutError(TimeoutError):\n    \"\"\"Raised when framework-level timeout is exceeded.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        item_id: str | None = None,\n        elapsed: float | None = None,\n        timeout_limit: float | None = None,\n    ):\n        super().__init__(message)\n        self.item_id = item_id\n        self.elapsed = elapsed\n        self.timeout_limit = timeout_limit\n\n# Usage in parallel.py:\nraise FrameworkTimeoutError(\n    f\"Framework timeout after {elapsed:.1f}s (limit: {timeout:.1f}s)\",\n    item_id=work_item.item_id,\n    elapsed=elapsed,\n    timeout_limit=timeout,\n) from timeout_exc\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#35-improve-retrystate-api-consistency","title":"3.5 Improve RetryState API Consistency","text":"<p>Priority: P2 - MEDIUM Effort: 30 minutes</p> <p>File: <code>src/batch_llm/base.py:24-100</code></p> <p>Implementation:</p> <pre><code>def delete(self, key: str, raise_if_missing: bool = False) -&gt; None:\n    \"\"\"\n    Delete a value from the state.\n\n    Args:\n        key: State key to delete\n        raise_if_missing: If True, raise KeyError if key doesn't exist.\n                         If False, silently ignore missing keys.\n\n    Raises:\n        KeyError: If key doesn't exist and raise_if_missing=True\n    \"\"\"\n    if raise_if_missing:\n        del self.data[key]\n    else:\n        self.data.pop(key, None)\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#phase-4-documentation-testing-completed","title":"Phase 4: Documentation &amp; Testing \u2705 COMPLETED","text":"<p>Target: v0.4.x (achieved in v0.3.x) Goal: Comprehensive documentation and test coverage</p> <p>Status: All tasks completed (2025-01-10)</p>"},{"location":"archive/IMPROVEMENT_PLAN/#41-add-missing-docstrings","title":"4.1 Add Missing Docstrings \u2705","text":"<p>Status: COMPLETED Priority: P3 - LOW Effort: 2-3 hours</p> <p>Files: Multiple</p> <p>Completion Notes:</p> <ul> <li>Added docstrings to all missing methods identified by <code>pydocstyle</code></li> <li>Methods documented: <code>MetricsObserver.reset()</code>, <code>ProcessorConfig.validate()</code>, <code>RetryState.__contains__()</code></li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#42-create-migration-guides","title":"4.2 Create Migration Guides \u2705","text":"<p>Status: COMPLETED (Commit: 791a1e0) Priority: P2 - MEDIUM Effort: 2-3 hours</p> <p>Completion Notes:</p> <ul> <li>Created <code>docs/MIGRATION.md</code> as comprehensive migration guide index</li> <li>Links all existing migration guides: v0.0\u2192v0.1, v0.1\u2192v0.2, v0.2\u2192v0.3</li> <li>Added \"Migration Path Finder\" for users upgrading from any version</li> <li>Updated README.md to link to new migration index</li> <li>Includes version history table and deprecation policy</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#43-add-error-handling-to-examples","title":"4.3 Add Error Handling to Examples \u2705","text":"<p>Status: COMPLETED Priority: P3 - LOW Effort: 1 hour</p> <p>Files: <code>examples/*.py</code></p> <p>Completion Notes:</p> <ul> <li>Added API key checks to all example files</li> <li>Examples now provide helpful error messages when API keys are missing</li> <li>Includes links to where users can get API keys</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#44-add-integration-tests","title":"4.4 Add Integration Tests \u2705","text":"<p>Status: COMPLETED (Commit: 94d65f6) Priority: P2 - MEDIUM Effort: 4-6 hours</p> <p>File: <code>tests/test_integration.py</code></p> <p>Completion Notes:</p> <ul> <li>Created 7 integration tests with real API calls</li> <li>Tests for: Gemini basic generation, cached strategy, safety ratings, validation retries</li> <li>Tests automatically skip when API keys are not present</li> <li>Placeholder tests for future OpenAI/Anthropic implementations</li> <li>Added <code>integration</code> marker to pyproject.toml</li> <li>Tests deselected by default, run with: <code>pytest -m integration -v</code></li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#45-add-performance-benchmarks","title":"4.5 Add Performance Benchmarks \u2705","text":"<p>Status: COMPLETED (Commit: 94d65f6) Priority: P3 - LOW Effort: 4-6 hours</p> <p>File: <code>tests/test_performance.py</code></p> <p>Completion Notes:</p> <ul> <li>Created 8 performance benchmark tests</li> <li>Benchmarks: throughput (single/scaling), memory usage, framework overhead, stats performance</li> <li>Tests compare shared vs unique strategy performance</li> <li>All benchmarks print detailed results and verify performance expectations</li> <li>Added <code>benchmark</code> marker to pyproject.toml</li> <li>Tests deselected by default, run with: <code>pytest -m benchmark -v -s</code></li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#46-add-worst-case-rate-limit-test","title":"4.6 Add Worst-Case Rate Limit Test \u2705","text":"<p>Status: COMPLETED Priority: P2 - MEDIUM Effort: 2 hours</p> <p>File: <code>tests/test_worst_case_rate_limit.py</code></p> <p>Completion Notes:</p> <ul> <li>Created comprehensive worst-case rate limit tests</li> <li>Tests all workers hitting rate limit simultaneously using barriers</li> <li>Verifies coordination and no deadlocks occur</li> <li>Includes tests for cache expiration thread safety with multiple workers</li> <li>Tests run with standard pytest, no special marker needed</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#phase-5-user-experience-future","title":"Phase 5: User Experience (Future)","text":"<p>Target: v0.5.0 Goal: Convenience features and polish</p>"},{"location":"archive/IMPROVEMENT_PLAN/#51-add-built-in-progress-bar","title":"5.1 Add Built-in Progress Bar","text":"<p>Priority: P3 - MEDIUM Effort: 4-6 hours</p> <p>Implementation: Optional integration with <code>rich</code> or <code>tqdm</code></p> <pre><code>from async_batch_llm.progress import RichProgressBar\n\nprocessor = ParallelBatchProcessor(\n    config=config,\n    progress_callback=RichProgressBar(),  # Optional convenience\n)\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#52-improve-dry-run-documentation","title":"5.2 Improve Dry-Run Documentation","text":"<p>Priority: P3 - LOW Effort: 1 hour</p> <p>Add example to README showing dry-run testing</p>"},{"location":"archive/IMPROVEMENT_PLAN/#53-add-input-validation","title":"5.3 Add Input Validation","text":"<p>Priority: P2 - MEDIUM Effort: 2 hours</p> <p>File: <code>src/batch_llm/base.py:145-157</code></p> <pre><code>def __post_init__(self):\n    \"\"\"Validate work item fields.\"\"\"\n    if not self.item_id or not isinstance(self.item_id, str):\n        raise ValueError(...)\n\n    if self.strategy is None:\n        raise ValueError(\n            f\"strategy must not be None for {self.item_id}. \"\n            f\"Provide an LLMCallStrategy instance.\"\n        )\n\n    if len(self.prompt) &gt; 1_000_000:  # 1MB limit\n        logger.warning(\n            f\"Very large prompt for {self.item_id}: {len(self.prompt)} chars. \"\n            f\"May exceed model context limits.\"\n        )\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#code-quality-improvements","title":"Code Quality Improvements","text":""},{"location":"archive/IMPROVEMENT_PLAN/#61-consistent-error-message-truncation","title":"6.1 Consistent Error Message Truncation","text":"<p>Priority: P3 - LOW Effort: 30 minutes</p> <p>Implementation:</p> <pre><code># Add constant\nERROR_MESSAGE_MAX_LENGTH = 200\n\n# Use throughout:\nerror = f\"{type(e).__name__}: {str(e)[:ERROR_MESSAGE_MAX_LENGTH]}\"\n</code></pre>"},{"location":"archive/IMPROVEMENT_PLAN/#62-document-thread-safety","title":"6.2 Document Thread Safety","text":"<p>Priority: P3 - LOW Effort: 1 hour</p> <p>Add comments documenting thread-safety assumptions in:</p> <ul> <li><code>_is_cache_expired()</code></li> <li><code>_progress_tasks</code> cleanup</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#tracking-and-metrics","title":"Tracking and Metrics","text":""},{"location":"archive/IMPROVEMENT_PLAN/#success-criteria","title":"Success Criteria","text":"<p>Phase 1 (Critical):</p> <ul> <li>\u2705 Version mismatch fixed</li> <li>\u2705 Shared strategy tests added (3+ tests)</li> <li>\u2705 Cost optimization documented prominently</li> </ul> <p>Phase 2 (High Impact):</p> <ul> <li>\u2705 Type ignores reduced to &lt;10</li> <li>\u2705 Line length violations: 0</li> <li>\u2705 Resource cleanup added</li> <li>\u2705 Worker count guide added</li> <li>\u2705 Cache tag tests added</li> </ul> <p>Phase 3 (Performance):</p> <ul> <li>\u2705 Slow-start optimized (5-10% improvement)</li> <li>\u2705 Debug logging added</li> <li>\u2705 Parameter validation added</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#progress-tracking","title":"Progress Tracking","text":"<p>Track progress in GitHub Issues:</p> <ul> <li>Tag issues with phase labels: <code>phase-1-critical</code>, <code>phase-2-high-impact</code>, etc.</li> <li>Tag issues with type labels: <code>bug</code>, <code>enhancement</code>, <code>documentation</code>, <code>testing</code></li> <li>Use milestones: <code>v0.3.1</code>, <code>v0.4.0</code>, etc.</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#review-cadence","title":"Review Cadence","text":"<ul> <li>Weekly: Review Phase 1 progress</li> <li>Bi-weekly: Review Phases 2-3 progress</li> <li>Monthly: Review backlog prioritization</li> </ul>"},{"location":"archive/IMPROVEMENT_PLAN/#appendix-full-issue-list","title":"Appendix: Full Issue List","text":""},{"location":"archive/IMPROVEMENT_PLAN/#by-priority","title":"By Priority","text":"<p>P0 - Critical (3 issues):</p> <ol> <li>Fix version mismatch</li> <li>Add shared strategy tests</li> <li>Document cost optimization</li> </ol> <p>P1 - High (5 issues): 4. Reduce type ignores 5. Fix line length violations 6. Add resource cleanup 7. Document worker count 8. Add cache tag tests</p> <p>P2 - Medium (11 issues): 9. Optimize slow-start 10. Add debug logging 11. Add parameter validation 12. Improve error context 13. Improve RetryState API 14. Add migration guides 15. Add integration tests 16. Add worst-case rate limit test 17. Add input validation 18. Others...</p> <p>P3 - Low (8 issues): 19. Add docstrings 20. Add example error handling 21. Performance benchmarks 22. Progress bar integration 23. Consistent error truncation 24. Others...</p>"},{"location":"archive/IMPROVEMENT_PLAN/#change-log","title":"Change Log","text":"<p>2025-01-10: Initial plan created based on package review</p>"},{"location":"archive/IMPROVEMENT_PLAN/#references","title":"References","text":"<ul> <li>Package Review: <code>PACKAGE_REVIEW_2025_01_10.md</code></li> <li>v0.2.0 Features: <code>CHANGELOG.md</code></li> <li>v0.3.0 Features: <code>docs/IMPLEMENTATION_PLAN_V0_3.md</code></li> <li>Architecture Notes: <code>CLAUDE.md</code></li> </ul>"},{"location":"archive/MIGRATION/","title":"Migration Guides","text":"<p>This directory contains migration guides for upgrading between major versions of batch-llm.</p>"},{"location":"archive/MIGRATION/#current-version-030","title":"Current Version: 0.3.0","text":"<p>Latest stable release: v0.3.0 (2025-01-10)</p>"},{"location":"archive/MIGRATION/#available-migration-guides","title":"Available Migration Guides","text":""},{"location":"archive/MIGRATION/#v02-v03-migration-guide","title":"\ud83d\udd04 v0.2 \u2192 v0.3 Migration Guide","text":"<p>Status: \u2705 Complete Breaking Changes: None (100% backward compatible) Release Date: 2025-01-10</p> <p>Key Features:</p> <ul> <li>RetryState for multi-stage retry patterns</li> <li>GeminiResponse for safety ratings access</li> <li>Cache tagging for precise cache matching</li> </ul> <p>Who should read this:</p> <ul> <li>Everyone upgrading from v0.2.x to v0.3.0</li> <li>Users wanting to adopt new retry state features</li> <li>Users needing content moderation with safety ratings</li> <li>Multi-tenant applications needing cache isolation</li> </ul> <p>Migration complexity: \ud83d\udfe2 Easy - All new features are opt-in</p>"},{"location":"archive/MIGRATION/#v01-v02-migration-guide","title":"\ud83d\udd04 v0.1 \u2192 v0.2 Migration Guide","text":"<p>Status: \u2705 Complete Breaking Changes: None (100% backward compatible) Release Date: 2024-10-22</p> <p>Key Features:</p> <ul> <li>Shared strategy lifecycle (70-90% cost savings)</li> <li>Automatic cache renewal for GeminiCachedStrategy</li> <li>Enhanced cache management and reuse</li> </ul> <p>Who should read this:</p> <ul> <li>Users upgrading from v0.1.x to v0.2.0</li> <li>Users of GeminiCachedStrategy wanting cost optimization</li> <li>Anyone using multiple work items with the same strategy</li> </ul> <p>Migration complexity: \ud83d\udfe2 Easy - All new features are opt-in</p>"},{"location":"archive/MIGRATION/#v00-v01-migration-guide-strategy-pattern","title":"\ud83d\udd04 v0.0 \u2192 v0.1 Migration Guide (Strategy Pattern)","text":"<p>Status: \u2705 Complete Breaking Changes: Yes - Major API refactor Release Date: 2024-10-15</p> <p>Key Changes:</p> <ul> <li>Replaced <code>agent=</code>, <code>agent_factory=</code>, <code>direct_call=</code> with unified <code>strategy=</code> parameter</li> <li>Introduced <code>LLMCallStrategy</code> abstract base class</li> <li>Framework-level timeout enforcement with <code>asyncio.wait_for()</code></li> <li>Built-in strategies: <code>PydanticAIStrategy</code>, <code>GeminiStrategy</code>, <code>GeminiCachedStrategy</code></li> </ul> <p>Who should read this:</p> <ul> <li>Users upgrading from v0.0.x to v0.1.0+</li> <li>Anyone using the old agent-based API</li> <li>Users migrating from PydanticAI agent parameters</li> </ul> <p>Migration complexity: \ud83d\udfe1 Medium - Requires code changes to use new strategy API</p> <p>Note: This file is named <code>MIGRATION_V3.md</code> for historical reasons (internal versioning), but refers to the v0.1.0 release.</p>"},{"location":"archive/MIGRATION/#quick-reference-what-changed-when","title":"Quick Reference: What Changed When?","text":"Version Release Date Breaking Changes Key Features v0.3.0 2025-01-10 None RetryState, GeminiResponse, Cache tags v0.2.0 2024-10-22 None Shared strategy lifecycle, Auto cache renewal v0.1.0 2024-10-15 Yes Strategy pattern refactor, Unified API v0.0.x 2024-10-01 N/A Initial releases (agent-based API)"},{"location":"archive/MIGRATION/#migration-path-finder","title":"Migration Path Finder","text":""},{"location":"archive/MIGRATION/#im-on-v00x-and-want-to-upgrade-to-latest","title":"\"I'm on v0.0.x and want to upgrade to latest\"","text":"<ol> <li>Read v0.0 \u2192 v0.1 Migration Guide (Required - breaking changes)</li> <li>Read v0.1 \u2192 v0.2 Migration Guide (Optional - no breaking changes)</li> <li>Read v0.2 \u2192 v0.3 Migration Guide (Optional - no breaking changes)</li> </ol> <p>Estimated effort: 2-4 hours (mostly for v0.1 strategy pattern refactor)</p>"},{"location":"archive/MIGRATION/#im-on-v01x-and-want-to-upgrade-to-latest","title":"\"I'm on v0.1.x and want to upgrade to latest\"","text":"<ol> <li>Read v0.1 \u2192 v0.2 Migration Guide (Optional - no breaking changes)</li> <li>Read v0.2 \u2192 v0.3 Migration Guide (Optional - no breaking changes)</li> </ol> <p>Estimated effort: 30 minutes - 1 hour (only if adopting new features)</p>"},{"location":"archive/MIGRATION/#im-on-v02x-and-want-to-upgrade-to-v030","title":"\"I'm on v0.2.x and want to upgrade to v0.3.0\"","text":"<ol> <li>Read v0.2 \u2192 v0.3 Migration Guide (Optional - no breaking changes)</li> </ol> <p>Estimated effort: 15-30 minutes (only if adopting new features)</p>"},{"location":"archive/MIGRATION/#deprecation-policy","title":"Deprecation Policy","text":"<p>batch-llm follows semantic versioning:</p> <ul> <li>Patch releases (0.3.x): Bug fixes only, no breaking changes, no deprecations</li> <li>Minor releases (0.x.0): New features, may deprecate old features with warnings, no breaking changes</li> <li>Major releases (x.0.0): May remove deprecated features, may have breaking changes</li> </ul> <p>Deprecation timeline:</p> <ol> <li>Feature marked as deprecated in minor release (with warning)</li> <li>Documented in CHANGELOG and migration guide</li> <li>Removed in next major release (minimum 6 months notice)</li> </ol>"},{"location":"archive/MIGRATION/#backward-compatibility-guarantee","title":"Backward Compatibility Guarantee","text":"<p>v0.1.0+: We maintain backward compatibility across minor versions:</p> <ul> <li>v0.1.x \u2192 v0.2.0: \u2705 No breaking changes</li> <li>v0.2.x \u2192 v0.3.0: \u2705 No breaking changes</li> <li>v0.3.x \u2192 v0.4.0: \u2705 No breaking changes expected</li> </ul> <p>v0.0.x: Legacy versions, no backward compatibility guarantee.</p>"},{"location":"archive/MIGRATION/#getting-help","title":"Getting Help","text":""},{"location":"archive/MIGRATION/#if-you-encounter-migration-issues","title":"If you encounter migration issues","text":"<ol> <li>Check the migration guide for your version in this directory</li> <li>Search GitHub Issues: https://github.com/anthropics/batch-llm/issues</li> <li>Open a new issue: Include:</li> <li>Source version (e.g., \"v0.1.0\")</li> <li>Target version (e.g., \"v0.3.0\")</li> <li>Error message or unexpected behavior</li> <li>Minimal code example</li> </ol>"},{"location":"archive/MIGRATION/#common-migration-questions","title":"Common Migration Questions","text":"<p>Q: Do I need to migrate immediately after a new release?</p> <p>A: No. Minor releases (0.x.0) are backward compatible. You can upgrade at your own pace. Only major releases (x.0.0) may require migration.</p> <p>Q: Can I skip versions when upgrading?</p> <p>A: Yes, if there are no breaking changes. However, we recommend reading all migration guides between your version and the target version to learn about new features.</p> <p>Q: What if I'm using a deprecated feature?</p> <p>A: Deprecated features will show warnings but continue to work until the next major release. Plan to migrate during the deprecation period.</p> <p>Q: How do I know if a release has breaking changes?</p> <p>A: Check the CHANGELOG.md file. Breaking changes are clearly marked with a warning symbol at the top of each release section.</p>"},{"location":"archive/MIGRATION/#version-history-reference","title":"Version History Reference","text":"<p>For complete details on each release, see CHANGELOG.md.</p> <p>For current development roadmap, see IMPROVEMENT_PLAN.md.</p>"},{"location":"archive/MIGRATION/#contributing-migration-guides","title":"Contributing Migration Guides","text":"<p>When releasing a new version with changes that affect users:</p> <ol> <li>Create <code>docs/MIGRATION_V0_X.md</code> following the existing template</li> <li>Update this index file with the new guide</li> <li>Mark breaking changes clearly with \u26a0\ufe0f symbols</li> <li>Provide before/after code examples</li> <li>Estimate migration complexity (Easy/Medium/Hard)</li> <li>Include links to relevant issues and documentation</li> </ol> <p>Template structure:</p> <ul> <li>Overview and key changes</li> <li>Breaking changes (if any)</li> <li>Step-by-step migration instructions</li> <li>Before/after code examples</li> <li>Testing migration checklist</li> <li>Common pitfalls and solutions</li> <li>Additional resources</li> </ul> <p>Note: Last updated: 2025-01-10</p>"},{"location":"archive/MIGRATION_V0_1/","title":"Migration Guide: v0.0.x \u2192 v0.1","text":"<p>This guide helps you migrate from batch-llm v0.0.x to v0.1, which introduces the new LLM call strategy pattern to provide greater flexibility and cleaner separation of concerns.</p>"},{"location":"archive/MIGRATION_V0_1/#overview-of-changes","title":"Overview of Changes","text":"<p>v0.1 Breaking Changes:</p> <ul> <li>Replaced <code>agent=</code> parameter with <code>strategy=</code> in <code>LLMWorkItem</code></li> <li>Removed <code>client=</code> parameter from <code>LLMWorkItem</code></li> <li>Introduced <code>LLMCallStrategy</code> abstract base class</li> <li>New built-in strategies: <code>PydanticAIStrategy</code>, <code>GeminiStrategy</code>, <code>GeminiCachedStrategy</code></li> <li>Improved timeout enforcement (now framework-level)</li> </ul> <p>Why the change?</p> <ul> <li>Flexibility: Support any LLM provider (OpenAI, Anthropic, LangChain, etc.) through custom strategies</li> <li>Clean separation: Strategy encapsulates all model-specific logic (caching, parsing, retries)</li> <li>Extensibility: Easy to create custom strategies with prepare/execute/cleanup lifecycle</li> <li>Consistency: Unified interface regardless of underlying LLM provider</li> </ul>"},{"location":"archive/MIGRATION_V0_1/#new-features-in-v01","title":"New Features in v0.1","text":"<p>In addition to the strategy pattern refactor, v0.1 adds powerful new capabilities:</p>"},{"location":"archive/MIGRATION_V0_1/#on_error-callback-for-intelligent-retry","title":"on_error() Callback for Intelligent Retry","text":"<p>Non-breaking addition - Strategies can now implement <code>on_error()</code> to handle exceptions intelligently:</p> <pre><code>from pydantic import ValidationError\nfrom async_batch_llm.llm_strategies import LLMCallStrategy\n\nclass SmartStrategy(LLMCallStrategy[Output]):\n    def __init__(self, client):\n        self.client = client\n        self.validation_failures = 0\n        self.network_errors = 0\n\n    async def on_error(self, exception: Exception, attempt: int) -&gt; None:\n        \"\"\"Track error types to make smart retry decisions.\"\"\"\n        if isinstance(exception, ValidationError):\n            self.validation_failures += 1\n        elif isinstance(exception, ConnectionError):\n            self.network_errors += 1\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use error counts to make intelligent decisions:\n        # - Only escalate model on validation errors\n        # - Retry same model on network errors\n        # - Build smarter retry prompts based on what failed\n        if self.validation_failures &gt; 0:\n            model = \"expensive-smart-model\"  # Quality issue\n        else:\n            model = \"cheap-fast-model\"  # Network issue, use same model\n\n        response = await self.client.generate(prompt, model=model)\n        return parsed_output, tokens\n</code></pre> <p>Benefits:</p> <ul> <li>Error-aware retry logic: Distinguish validation errors from network/rate limit errors</li> <li>Smart model escalation: Only use expensive models when LLM quality is the issue (60-80% cost savings)</li> <li>Smart retry prompts: Build targeted prompts based on which fields failed validation</li> <li>Error tracking: Count different error types for analytics and debugging</li> <li>Framework integration: Called automatically when <code>execute()</code> raises an exception</li> <li>Safe: Exceptions in <code>on_error()</code> are caught and logged (won't crash processing)</li> <li>Non-breaking: Default no-op implementation, opt-in behavior</li> </ul> <p>Use Cases:</p> <ol> <li>Cost Optimization - Only escalate to expensive models on validation errors:</li> </ol> <pre><code># Validation error \u2192 Use GPT-4 (quality issue)\n# Network error \u2192 Retry with GPT-3.5 (transient issue)\n# Result: 70% cost reduction\n</code></pre> <ol> <li>Better Retry Prompts - Tell LLM exactly what failed:</li> </ol> <pre><code># Parse validation error: email field invalid, name and age OK\n# Retry prompt: \"Previous attempt succeeded for name/age.\n#                Please fix the email field validation error.\"\n</code></pre> <ol> <li>Analytics - Track error patterns:</li> </ol> <pre><code># Monitor: 80% validation errors, 15% network, 5% rate limits\n# Action: Improve prompts to reduce validation errors\n</code></pre> <p>Examples:</p> <ul> <li><code>examples/example_smart_model_escalation.py</code> - Smart model escalation</li> <li><code>examples/example_gemini_smart_retry.py</code> - Smart retry with validation feedback</li> </ul>"},{"location":"archive/MIGRATION_V0_1/#quick-migration-patterns","title":"Quick Migration Patterns","text":""},{"location":"archive/MIGRATION_V0_1/#pattern-1-pydanticai-agent-most-common","title":"Pattern 1: PydanticAI Agent (Most Common)","text":"<p>v0.0.x Code:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom pydantic_ai import Agent\n\n# Old way - passing agent directly\nagent = Agent(\"gemini-2.5-flash\", result_type=MyOutput)\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    agent=agent,  # \u274c Removed in v0.1\n    prompt=\"Test prompt\",\n)\n</code></pre> <p>v0.1 Code:</p> <pre><code>from async_batch_llm import (\n    LLMWorkItem,\n    ParallelBatchProcessor,\n    ProcessorConfig,\n    PydanticAIStrategy,  # \u2705 New import\n)\nfrom pydantic_ai import Agent\n\n# New way - wrap agent in strategy\nagent = Agent(\"gemini-2.5-flash\", result_type=MyOutput)\nstrategy = PydanticAIStrategy(agent=agent)  # \u2705 Wrap in strategy\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,  # \u2705 Use strategy= instead of agent=\n    prompt=\"Test prompt\",\n)\n</code></pre> <p>Migration steps:</p> <ol> <li>Import <code>PydanticAIStrategy</code> from <code>batch_llm</code></li> <li>Wrap your agent: <code>strategy = PydanticAIStrategy(agent=agent)</code></li> <li>Replace <code>agent=agent</code> with <code>strategy=strategy</code></li> </ol>"},{"location":"archive/MIGRATION_V0_1/#pattern-2-direct-gemini-api-calls","title":"Pattern 2: Direct Gemini API Calls","text":"<p>v0.0.x Code:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom google import genai\n\nclient = genai.Client(api_key=API_KEY)\n\n# Old way - passing client directly\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    client=client,  # \u274c Removed in v0.1\n    prompt=\"Test prompt\",\n)\n</code></pre> <p>v0.1 Code:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom async_batch_llm.llm_strategies import GeminiStrategy  # \u2705 New import\nfrom google import genai\n\nclient = genai.Client(api_key=API_KEY)\n\n# Create response parser\ndef parse_response(response) -&gt; str:\n    return response.text\n\n# New way - use GeminiStrategy\nstrategy = GeminiStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n    config=genai.types.GenerateContentConfig(temperature=0.7),\n)\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,  # \u2705 Use strategy\n    prompt=\"Test prompt\",\n)\n</code></pre> <p>Migration steps:</p> <ol> <li>Import <code>GeminiStrategy</code> from <code>batch_llm.llm_strategies</code></li> <li>Create a response parser function</li> <li>Create strategy with model, client, parser, and optional config</li> <li>Replace <code>client=client</code> with <code>strategy=strategy</code></li> </ol>"},{"location":"archive/MIGRATION_V0_1/#pattern-3-gemini-with-context-caching","title":"Pattern 3: Gemini with Context Caching","text":"<p>v0.0.x Code:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom google import genai\n\nclient = genai.Client(api_key=API_KEY)\n\n# Old way - caching handled implicitly or manually\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    client=client,  # \u274c Removed\n    prompt=\"Question about cached context\",\n)\n</code></pre> <p>v0.1 Code:</p> <pre><code>from async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom async_batch_llm.llm_strategies import GeminiCachedStrategy  # \u2705 New import\nfrom google import genai\n\nclient = genai.Client(api_key=API_KEY)\n\n# Define content to cache (e.g., large documents, knowledge base)\ncached_content = [\n    genai.types.Content(\n        role=\"user\",\n        parts=[genai.types.Part(text=\"Large context to cache...\")]\n    ),\n]\n\ndef parse_response(response) -&gt; str:\n    return response.text\n\n# New way - use GeminiCachedStrategy\nstrategy = GeminiCachedStrategy(\n    model=\"gemini-2.5-flash\",\n    client=client,\n    response_parser=parse_response,\n    cached_content=cached_content,\n    cache_ttl_seconds=3600,  # Cache for 1 hour\n    cache_refresh_threshold=0.1,  # Refresh at 10% TTL\n)\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,  # \u2705 Use cached strategy\n    prompt=\"Question about cached context\",\n)\n</code></pre> <p>Migration steps:</p> <ol> <li>Import <code>GeminiCachedStrategy</code> from <code>batch_llm.llm_strategies</code></li> <li>Define your cached content as list of <code>Content</code> objects</li> <li>Create cached strategy with TTL and refresh settings</li> <li>Strategy automatically handles cache lifecycle (create, refresh, delete)</li> </ol>"},{"location":"archive/MIGRATION_V0_1/#complete-migration-example","title":"Complete Migration Example","text":"<p>Here's a complete example showing before and after:</p>"},{"location":"archive/MIGRATION_V0_1/#v00x-complete-example","title":"v0.0.x Complete Example","text":"<pre><code>import asyncio\nfrom async_batch_llm import LLMWorkItem, ParallelBatchProcessor, ProcessorConfig\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Output(BaseModel):\n    result: str\n\nasync def main():\n    # Create agent\n    agent = Agent(\"gemini-2.5-flash\", result_type=Output)\n\n    # Configure processor\n    config = ProcessorConfig(max_workers=5, timeout_per_item=30.0)\n\n    # Process items\n    async with ParallelBatchProcessor[str, Output, None](config=config) as processor:\n        # Add work items\n        for i in range(10):\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=f\"item_{i}\",\n                    agent=agent,  # \u274c Old way\n                    prompt=f\"Process item {i}\",\n                )\n            )\n\n        # Process all\n        result = await processor.process_all()\n\n    print(f\"Succeeded: {result.succeeded}/{result.total_items}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"archive/MIGRATION_V0_1/#v01-complete-example","title":"v0.1 Complete Example","text":"<pre><code>import asyncio\nfrom async_batch_llm import (\n    LLMWorkItem,\n    ParallelBatchProcessor,\n    ProcessorConfig,\n    PydanticAIStrategy,  # \u2705 New import\n)\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent\n\nclass Output(BaseModel):\n    result: str\n\nasync def main():\n    # Create agent\n    agent = Agent(\"gemini-2.5-flash\", result_type=Output)\n\n    # Wrap in strategy \u2705\n    strategy = PydanticAIStrategy(agent=agent)\n\n    # Configure processor\n    config = ProcessorConfig(max_workers=5, timeout_per_item=30.0)\n\n    # Process items\n    async with ParallelBatchProcessor[str, Output, None](config=config) as processor:\n        # Add work items\n        for i in range(10):\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=f\"item_{i}\",\n                    strategy=strategy,  # \u2705 New way\n                    prompt=f\"Process item {i}\",\n                )\n            )\n\n        # Process all\n        result = await processor.process_all()\n\n    print(f\"Succeeded: {result.succeeded}/{result.total_items}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Key differences:</p> <ol> <li>Import <code>PydanticAIStrategy</code></li> <li>Create strategy: <code>strategy = PydanticAIStrategy(agent=agent)</code></li> <li>Use <code>strategy=</code> instead of <code>agent=</code></li> </ol>"},{"location":"archive/MIGRATION_V0_1/#custom-strategies-new-in-v01","title":"Custom Strategies (New in v0.1)","text":"<p>One of the biggest benefits of v0.1 is the ability to create custom strategies for any LLM provider:</p>"},{"location":"archive/MIGRATION_V0_1/#example-openai-custom-strategy","title":"Example: OpenAI Custom Strategy","text":"<pre><code>from async_batch_llm.llm_strategies import LLMCallStrategy\nfrom openai import AsyncOpenAI\n\nclass OpenAIStrategy(LLMCallStrategy[str]):\n    \"\"\"Custom strategy for OpenAI API.\"\"\"\n\n    def __init__(self, client: AsyncOpenAI, model: str = \"gpt-4o-mini\"):\n        self.client = client\n        self.model = model\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[str, dict[str, int]]:\n        # Make API call\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n\n        # Extract output and token usage\n        output = response.choices[0].message.content or \"\"\n        tokens = {\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n        return output, tokens\n\n# Use it\nclient = AsyncOpenAI(api_key=API_KEY)\nstrategy = OpenAIStrategy(client=client, model=\"gpt-4o-mini\")\n\nwork_item = LLMWorkItem(\n    item_id=\"item_1\",\n    strategy=strategy,\n    prompt=\"Your prompt here\",\n)\n</code></pre> <p>See <code>examples/example_openai.py</code>, <code>examples/example_anthropic.py</code>, and <code>examples/example_langchain.py</code> for more examples.</p>"},{"location":"archive/MIGRATION_V0_1/#strategy-lifecycle-new-in-v01","title":"Strategy Lifecycle (New in v0.1)","text":"<p>Strategies support a lifecycle with three methods:</p> <pre><code>class LLMCallStrategy(ABC):\n    async def prepare(self) -&gt; None:\n        \"\"\"Called once before any retry attempts. Use for initialization.\"\"\"\n        pass\n\n    async def execute(\n        self, prompt: str, attempt: int, timeout: float\n    ) -&gt; tuple[TOutput, dict[str, int]]:\n        \"\"\"Called for each attempt (including retries). Must be implemented.\"\"\"\n        pass\n\n    async def cleanup(self) -&gt; None:\n        \"\"\"Called once after all attempts. Use for cleanup.\"\"\"\n        pass\n</code></pre> <p>Example with lifecycle:</p> <pre><code>class CachedStrategy(LLMCallStrategy[str]):\n    async def prepare(self):\n        # Initialize cache, open connections, etc.\n        self.cache = await create_cache()\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use the cache\n        return await self.cache.query(prompt)\n\n    async def cleanup(self):\n        # Clean up resources\n        await self.cache.delete()\n</code></pre>"},{"location":"archive/MIGRATION_V0_1/#timeout-enforcement-improvements","title":"Timeout Enforcement Improvements","text":"<p>v0.1 changes how timeouts work:</p> <ul> <li>v0.0.x: Each strategy was responsible for timeout enforcement</li> <li>v0.1: Framework enforces timeout with <code>asyncio.wait_for()</code> wrapper</li> </ul> <p>Impact on custom strategies:</p> <ul> <li>You no longer need to wrap your strategy execution in <code>asyncio.wait_for()</code></li> <li>The <code>timeout</code> parameter is still passed to <code>execute()</code> for informational purposes</li> <li>Framework handles timeout consistently across all strategies</li> </ul> <p>Example:</p> <pre><code># v0.0.x - Strategy had to handle timeout\nasync def execute(self, prompt: str, attempt: int, timeout: float):\n    return await asyncio.wait_for(\n        self.agent.run(prompt),\n        timeout=timeout,  # \u274c No longer needed\n    )\n\n# v0.1 - Framework handles timeout\nasync def execute(self, prompt: str, attempt: int, timeout: float):\n    # Framework wraps this in asyncio.wait_for()\n    return await self.agent.run(prompt)  # \u2705 Simpler\n</code></pre>"},{"location":"archive/MIGRATION_V0_1/#summary-checklist","title":"Summary Checklist","text":"<p>To migrate from v0.0.x to v0.1:</p> <ul> <li>[ ] For PydanticAI users:</li> <li>[ ] Import <code>PydanticAIStrategy</code> from <code>batch_llm</code></li> <li>[ ] Wrap agents: <code>strategy = PydanticAIStrategy(agent=agent)</code></li> <li> <p>[ ] Replace <code>agent=</code> with <code>strategy=</code></p> </li> <li> <p>[ ] For Gemini API users:</p> </li> <li>[ ] Import <code>GeminiStrategy</code> or <code>GeminiCachedStrategy</code> from <code>batch_llm.llm_strategies</code></li> <li>[ ] Create response parser function</li> <li>[ ] Create strategy with model, client, parser</li> <li> <p>[ ] Replace <code>client=</code> with <code>strategy=</code></p> </li> <li> <p>[ ] For custom implementations:</p> </li> <li>[ ] Implement <code>LLMCallStrategy</code> abstract base class</li> <li>[ ] Define <code>execute()</code> method (required)</li> <li>[ ] Optionally define <code>prepare()</code> and <code>cleanup()</code></li> <li> <p>[ ] Remove <code>asyncio.wait_for()</code> from execute (framework handles it)</p> </li> <li> <p>[ ] Testing:</p> </li> <li>[ ] Run your test suite</li> <li>[ ] Verify timeout behavior works as expected</li> <li>[ ] Check token usage is tracked correctly</li> </ul>"},{"location":"archive/MIGRATION_V0_1/#need-help","title":"Need Help?","text":"<ul> <li>API Documentation: See <code>docs/API.md</code> for complete API reference</li> <li>Examples: See <code>examples/</code> directory for working examples:</li> <li><code>example_llm_strategies.py</code> - All built-in strategies</li> <li><code>example_openai.py</code> - OpenAI integration</li> <li><code>example_anthropic.py</code> - Anthropic Claude integration</li> <li><code>example_langchain.py</code> - LangChain integration</li> <li>Issues: Report bugs at https://github.com/yourusername/batch-llm/issues</li> </ul>"},{"location":"archive/MIGRATION_V0_1/#benefits-of-v01","title":"Benefits of v0.1","text":"<p>Why upgrade?</p> <ol> <li>Universal Provider Support: Use any LLM provider (OpenAI, Anthropic, LangChain, etc.)</li> <li>Better Caching: First-class support for context caching (Gemini, etc.)</li> <li>Cleaner Code: Separation of concerns between framework and model logic</li> <li>More Reliable: Framework-level timeout enforcement</li> <li>Extensible: Easy to create custom strategies for new providers</li> <li>Resource Management: Proper lifecycle with prepare/cleanup hooks</li> </ol> <p>The migration is straightforward and the benefits are significant. Most codebases can be migrated in under an hour.</p>"},{"location":"archive/MIGRATION_V0_2/","title":"Migration Guide: v0.1.x \u2192 v0.2.0","text":""},{"location":"archive/MIGRATION_V0_2/#overview","title":"Overview","text":"<p>Version 0.2.0 adds critical fixes for production usage, particularly around shared strategies and Gemini caching. Most changes are backward compatible with minimal breaking changes.</p> <p>Key improvements:</p> <ul> <li>Shared strategy instances (prepare() called only once)</li> <li>Cached token tracking in BatchResult</li> <li>Automatic cache renewal for long pipelines</li> <li>google-genai v1.46+ compatibility</li> <li>Cache lifecycle clarification</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#breaking-changes","title":"Breaking Changes","text":""},{"location":"archive/MIGRATION_V0_2/#1-geminicachedstrategy-cleanup-behavior","title":"1. GeminiCachedStrategy cleanup() Behavior","text":"<p>Before (v0.1):</p> <pre><code>strategy = GeminiCachedStrategy(...)\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n# cleanup() automatically deletes cache\n</code></pre> <p>After (v0.2):</p> <pre><code>strategy = GeminiCachedStrategy(...)\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n# cleanup() preserves cache for reuse\n\n# Explicitly delete if needed\nawait strategy.delete_cache()\n</code></pre> <p>Why: Preserving caches between runs enables 70-90% cost savings when running multiple batches within the TTL window.</p> <p>Migration:</p> <ul> <li>If you relied on automatic cache deletion, call <code>await strategy.delete_cache()</code> explicitly</li> <li>For most production use cases, the new behavior is better (no code changes needed)</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#new-features","title":"New Features","text":""},{"location":"archive/MIGRATION_V0_2/#1-shared-strategy-optimization","title":"1. Shared Strategy Optimization","text":"<p>You can now share strategy instances across work items without duplicate <code>prepare()</code> calls:</p> <pre><code># Create one strategy\nstrategy = GeminiCachedStrategy(...)\n\n# Share across all work items\nfor item in items:\n    await processor.add_work(\n        LLMWorkItem(strategy=strategy, ...)  # Reuse same instance\n    )\n\n# prepare() is called only once!\n</code></pre> <p>Benefits:</p> <ul> <li>Single cache created and shared across all work items</li> <li>70-90% cost reduction with Gemini prompt caching</li> <li>Framework ensures thread-safe initialization</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#2-cached-token-tracking","title":"2. Cached Token Tracking","text":"<p><code>BatchResult</code> now includes cached token metrics:</p> <pre><code>result = await processor.process_all()\n\nprint(f\"Cache hit rate: {result.cache_hit_rate():.1f}%\")\nprint(f\"Effective cost: {result.effective_input_tokens()} tokens\")\n</code></pre> <p>New fields:</p> <ul> <li><code>BatchResult.total_cached_tokens</code> - Sum of cached input tokens</li> <li><code>BatchResult.cache_hit_rate()</code> - Percentage of input tokens cached</li> <li><code>BatchResult.effective_input_tokens()</code> - Actual cost after caching discount</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#3-automatic-cache-renewal","title":"3. Automatic Cache Renewal","text":"<p>Long-running pipelines automatically renew expired caches:</p> <pre><code>strategy = GeminiCachedStrategy(\n    cache_ttl_seconds=3600,  # 1 hour\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n    auto_renew=True,  # Default: automatic renewal\n)\n</code></pre> <p>How it works:</p> <ol> <li>Before each API call, checks if cache will expire soon</li> <li>If yes, creates new cache or finds existing one</li> <li>API call uses fresh cache</li> <li>No expiration errors!</li> </ol> <p>Benefits:</p> <ul> <li>Pipelines &gt; 1 hour don't fail with cache expiration errors</li> <li>Automatic renewal with configurable buffer</li> <li>Proactive renewal prevents downtime</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#4-cache-reuse-across-runs","title":"4. Cache Reuse Across Runs","text":"<p>Caches are now preserved between runs (within TTL window):</p> <pre><code># Run 1: Creates cache at 10:00, expires at 11:00\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process 1000 items ...\n    pass\n\n# Run 2: Starts at 10:30, reuses cache (30min left)\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process 1000 more items ...\n    # 70-90% cost savings!\n    pass\n</code></pre> <p>Best practices:</p> <ul> <li>Set TTL to slightly longer than expected run frequency</li> <li>Monitor cache reuse via <code>result.cache_hit_rate()</code></li> <li>For hourly jobs, use TTL of 3600s (1 hour)</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#api-compatibility","title":"API Compatibility","text":""},{"location":"archive/MIGRATION_V0_2/#google-genai-versions","title":"google-genai Versions","text":"<p>Both old and new google-genai APIs are supported:</p> <pre><code># Recommended: Install latest\npip install 'batch-llm[gemini]'  # Gets google-genai&gt;=1.46\n\n# Legacy support (will be removed in v0.3)\npip install 'batch-llm[gemini]' 'google-genai&lt;1.46'\n</code></pre> <p>The framework auto-detects which version you have and uses the appropriate API.</p>"},{"location":"archive/MIGRATION_V0_2/#recommended-actions","title":"Recommended Actions","text":""},{"location":"archive/MIGRATION_V0_2/#for-all-users","title":"For All Users","text":"<ol> <li>Update to v0.2.0:</li> </ol> <pre><code>pip install --upgrade batch-llm\n</code></pre> <ol> <li>Review cache cleanup logic:</li> <li>If you explicitly delete caches, add <code>await strategy.delete_cache()</code> calls</li> <li> <p>For most users, no changes needed (new behavior is better)</p> </li> <li> <p>Enable shared strategies for caching:</p> </li> </ol> <pre><code># Before: New strategy per item (creates multiple caches)\nfor item in items:\n    strategy = GeminiCachedStrategy(...)  # Don't do this\n    work_item = LLMWorkItem(strategy=strategy, ...)\n\n# After: Shared strategy (creates single cache)\nstrategy = GeminiCachedStrategy(...)  # Create once\nfor item in items:\n    work_item = LLMWorkItem(strategy=strategy, ...)  # Reuse\n</code></pre> <ol> <li>Monitor cache metrics:</li> </ol> <pre><code>result = await processor.process_all()\nprint(f\"Cache hit rate: {result.cache_hit_rate():.1f}%\")\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#for-gemini-users","title":"For Gemini Users","text":"<ol> <li>Update google-genai to v1.46+:</li> </ol> <pre><code>pip install --upgrade 'google-genai&gt;=1.46'\n</code></pre> <ol> <li>Enable auto-renewal for long pipelines:</li> </ol> <pre><code>strategy = GeminiCachedStrategy(\n    cache_ttl_seconds=3600,\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n    auto_renew=True,  # Enable automatic renewal\n)\n</code></pre> <ol> <li>Leverage cache reuse across runs:</li> <li>Don't call <code>delete_cache()</code> unless needed</li> <li>Set appropriate TTL for your run frequency</li> <li>Monitor cost savings via <code>effective_input_tokens()</code></li> </ol>"},{"location":"archive/MIGRATION_V0_2/#deprecations","title":"Deprecations","text":""},{"location":"archive/MIGRATION_V0_2/#deprecated-parameters","title":"Deprecated Parameters","text":"<ul> <li><code>GeminiCachedStrategy.cache_refresh_threshold</code> - Use <code>cache_renewal_buffer_seconds</code> instead</li> </ul> <p>Before (v0.1):</p> <pre><code>strategy = GeminiCachedStrategy(\n    cache_refresh_threshold=0.1,  # Deprecated: Refresh if &lt;10% TTL remaining\n)\n</code></pre> <p>After (v0.2):</p> <pre><code>strategy = GeminiCachedStrategy(\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n)\n</code></pre> <p>Why: Absolute time (seconds) is more predictable than percentage for long-running jobs.</p>"},{"location":"archive/MIGRATION_V0_2/#common-migration-patterns","title":"Common Migration Patterns","text":""},{"location":"archive/MIGRATION_V0_2/#pattern-1-test-cleanup","title":"Pattern 1: Test Cleanup","text":"<p>Before:</p> <pre><code>async def test_something():\n    strategy = GeminiCachedStrategy(...)\n    async with ParallelBatchProcessor(...) as processor:\n        # ... test code ...\n        pass\n    # Cache automatically deleted\n</code></pre> <p>After:</p> <pre><code>async def test_something():\n    strategy = GeminiCachedStrategy(...)\n    async with ParallelBatchProcessor(...) as processor:\n        # ... test code ...\n        pass\n    # Explicitly delete for tests\n    await strategy.delete_cache()\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#pattern-2-one-off-jobs","title":"Pattern 2: One-Off Jobs","text":"<p>Before:</p> <pre><code># One-off job - cache deleted automatically\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n</code></pre> <p>After:</p> <pre><code># One-off job - explicitly delete if cache won't be reused\nstrategy = GeminiCachedStrategy(...)\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n\nawait strategy.delete_cache()  # Clean up\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#pattern-3-recurring-jobs","title":"Pattern 3: Recurring Jobs","text":"<p>Before:</p> <pre><code># Recurring job - paid full cost every time\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n# Cache deleted, next run pays full cost\n</code></pre> <p>After:</p> <pre><code># Recurring job - reuses cache between runs\nasync with ParallelBatchProcessor(...) as processor:\n    # ... process items ...\n    pass\n# Cache preserved, next run (within TTL) gets 70-90% discount!\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#troubleshooting","title":"Troubleshooting","text":""},{"location":"archive/MIGRATION_V0_2/#cache-not-reused","title":"Cache Not Reused","text":"<p>Symptom: <code>cache_hit_rate()</code> is 0% on subsequent runs</p> <p>Solutions:</p> <ol> <li>Check that cache hasn't expired between runs</li> <li>Verify you're using the same model name</li> <li>Check logs for \"Reusing existing Gemini cache\" message</li> <li>Ensure you're not calling <code>delete_cache()</code> between runs</li> </ol>"},{"location":"archive/MIGRATION_V0_2/#prepare-called-multiple-times","title":"prepare() Called Multiple Times","text":"<p>Symptom: Multiple caches created when you expect one</p> <p>Solution: Share the same strategy instance:</p> <pre><code># Wrong: Creates new strategy (and cache) per item\nfor item in items:\n    strategy = GeminiCachedStrategy(...)  # New instance each time\n    work_item = LLMWorkItem(strategy=strategy, ...)\n\n# Right: Reuse same strategy instance\nstrategy = GeminiCachedStrategy(...)  # Create once\nfor item in items:\n    work_item = LLMWorkItem(strategy=strategy, ...)  # Reuse\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#cache-expiration-errors","title":"Cache Expiration Errors","text":"<p>Symptom: \"Cache content XXX is expired\" errors in long pipelines</p> <p>Solution: Enable auto-renewal:</p> <pre><code>strategy = GeminiCachedStrategy(\n    auto_renew=True,  # Enable automatic renewal\n    cache_renewal_buffer_seconds=300,  # Renew 5min before expiration\n)\n</code></pre>"},{"location":"archive/MIGRATION_V0_2/#support","title":"Support","text":""},{"location":"archive/MIGRATION_V0_2/#documentation","title":"Documentation","text":"<ul> <li>README.md - Updated examples and features</li> <li>docs/GEMINI_INTEGRATION.md - Gemini-specific guide</li> <li>docs/API.md - Full API reference</li> <li>IMPLEMENTATION_PLAN_V0_2.md - Technical details</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#examples","title":"Examples","text":"<ul> <li><code>examples/example_gemini_direct.py</code> - Direct Gemini API usage</li> <li><code>examples/example_gemini_smart_retry.py</code> - Smart retry patterns</li> <li><code>examples/example_model_escalation.py</code> - Cost optimization</li> </ul>"},{"location":"archive/MIGRATION_V0_2/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: https://github.com/yourusername/batch-llm/issues</li> <li>Check logs for debug information</li> <li>Enable debug logging: <code>logging.basicConfig(level=logging.DEBUG)</code></li> </ul>"},{"location":"archive/MIGRATION_V0_2/#version-history","title":"Version History","text":"<ul> <li>v0.2.0 (Current)</li> <li>Shared strategy optimization</li> <li>Cached token tracking</li> <li>Automatic cache renewal</li> <li>google-genai v1.46+ support</li> <li> <p>Cache lifecycle improvements</p> </li> <li> <p>v0.1.0</p> </li> <li>Strategy pattern refactor</li> <li>PydanticAI, Gemini, custom strategies</li> <li> <p>Framework-level timeout enforcement</p> </li> <li> <p>v0.0.2.x</p> </li> <li>Direct API call support</li> <li> <p>Race condition fixes</p> </li> <li> <p>v0.0.1.x</p> </li> <li>Initial release</li> <li>PydanticAI agent support</li> </ul>"},{"location":"archive/MIGRATION_V0_3/","title":"Migration Guide: v0.2.x \u2192 v0.3.0","text":""},{"location":"archive/MIGRATION_V0_3/#overview","title":"Overview","text":"<p>Version 0.3.0 adds advanced retry strategies, safety ratings access, and cache tagging based on production feedback. All changes are backward compatible - existing code will continue to work without modification.</p> <p>Key improvements:</p> <ul> <li>Per-work-item retry state for sophisticated multi-stage strategies</li> <li>Access to Gemini safety ratings and response metadata</li> <li>Cache tagging for precise cache matching</li> <li>Enhanced error classifier extensibility</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#breaking-changes","title":"Breaking Changes","text":"<p>None. Version 0.3.0 is fully backward compatible with v0.2.x.</p> <p>All new features are opt-in:</p> <ul> <li>Retry state is optional (strategies without <code>state</code> parameter work unchanged)</li> <li>Metadata access requires <code>include_metadata=True</code></li> <li>Cache tags are optional</li> <li>Existing code requires no changes</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#new-features","title":"New Features","text":""},{"location":"archive/MIGRATION_V0_3/#1-per-work-item-retry-state","title":"1. Per-Work-Item Retry State","text":"<p>Enable sophisticated multi-stage retry strategies that maintain state across retry attempts.</p> <p>Basic Usage:</p> <pre><code>from async_batch_llm import LLMCallStrategy, RetryState\n\nclass MultiStageStrategy(LLMCallStrategy[Output]):\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,  # NEW: Optional state parameter\n    ):\n        # Access state across retries\n        if state:\n            stage = state.get(\"stage\", 1)\n            total_attempts = state.get(\"total_attempts\", 0)\n\n            # Store state for next retry\n            state.set(\"total_attempts\", total_attempts + 1)\n\n        # ... rest of execute logic ...\n</code></pre>"},{"location":"archive/MIGRATION_V0_3/#advanced-example-multi-stage-validation-recovery","title":"Advanced Example: Multi-Stage Validation Recovery","text":"<p>Implement cost-optimized retry strategy with different stages:</p> <pre><code>from async_batch_llm import LLMCallStrategy, RetryState\nfrom pydantic import ValidationError\n\nclass SmartRetryStrategy(LLMCallStrategy[BookMetadata]):\n    \"\"\"Multi-stage retry with partial recovery for 81% cost savings.\"\"\"\n\n    STAGES = {\n        1: {\"type\": \"full\", \"temperature\": 0.0},\n        2: {\"type\": \"partial\", \"temperature\": 0.0},  # 81% cheaper\n        3: {\"type\": \"full\", \"temperature\": 0.25},\n        4: {\"type\": \"partial\", \"temperature\": 0.25},  # 81% cheaper\n    }\n\n    async def execute(\n        self,\n        prompt: str,\n        attempt: int,\n        timeout: float,\n        state: RetryState | None = None,\n    ):\n        # Initialize state on first attempt\n        if state is None:\n            state = RetryState()\n\n        if attempt == 1:\n            state.set(\"stage\", 1)\n            state.set(\"total_prompts\", 0)\n\n        stage = state.get(\"stage\", 1)\n        total_prompts = state.get(\"total_prompts\", 0)\n\n        # Enforce total prompt limit (prevent runaway costs)\n        if total_prompts &gt;= 5:\n            raise ValueError(f\"Exceeded max prompts: {total_prompts}/5\")\n\n        stage_config = self.STAGES.get(stage, self.STAGES[4])\n\n        try:\n            if stage_config[\"type\"] == \"full\":\n                # Full prompt with all instructions\n                result = await self._call_llm(prompt, stage_config[\"temperature\"])\n            else:\n                # Partial recovery - only fix failed fields (81% cheaper)\n                last_error = state.get(\"last_validation_error\")\n                partial_data = state.get(\"partial_data\")\n                result = await self._partial_recovery(\n                    last_error, partial_data, stage_config[\"temperature\"]\n                )\n\n            state.set(\"total_prompts\", total_prompts + 1)\n            return result, tokens\n\n        except ValidationError as e:\n            # Validation failed - advance to next stage\n            state.set(\"stage\", min(stage + 1, 4))\n            state.set(\"last_validation_error\", str(e))\n            state.set(\"partial_data\", e.partial_data if hasattr(e, 'partial_data') else None)\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Framework will retry with new stage\n\n        except (ConnectionError, TimeoutError) as e:\n            # Network error - retry same stage (don't advance)\n            state.set(\"total_prompts\", total_prompts + 1)\n            raise  # Framework will retry same stage\n\n    async def on_error(\n        self,\n        exception: Exception,\n        attempt: int,\n        state: RetryState | None = None,\n    ):\n        \"\"\"Log state for debugging.\"\"\"\n        if state:\n            logger.info(\n                f\"Retry {attempt}: Stage {state.get('stage')}, \"\n                f\"Total prompts: {state.get('total_prompts')}\"\n            )\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 Partial recovery stages are 81% cheaper than full retries</li> <li>\u2705 Network errors don't waste progress (retry same stage)</li> <li>\u2705 Total prompt limit prevents runaway costs</li> <li>\u2705 Progressive temperature escalation only when needed</li> <li>\u2705 State is isolated per work item (no collisions)</li> </ul> <p>RetryState API:</p> <pre><code>from async_batch_llm import RetryState\n\nstate = RetryState()\n\n# Store values\nstate.set(\"key\", value)\n\n# Retrieve values with optional default\nvalue = state.get(\"key\", default=None)\n\n# Clear all state\nstate.clear()\n\n# Serialize for debugging\nstate_dict = state.to_dict()\nstate = RetryState.from_dict(state_dict)\n</code></pre> <p>See Also:</p> <ul> <li><code>examples/example_multi_stage_retry.py</code> - Complete multi-stage example</li> <li><code>examples/example_partial_recovery.py</code> - Partial recovery pattern</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#2-gemini-safety-ratings-and-metadata","title":"2. Gemini Safety Ratings and Metadata","text":"<p>Access safety ratings, finish reasons, and raw response metadata from Gemini.</p> <p>Before (v0.2.x):</p> <pre><code>strategy = GeminiCachedStrategy(\n    response_parser=lambda r: parse_json(r.text),\n)\n\n# No access to safety ratings or finish_reason\n</code></pre> <p>After (v0.3.0):</p> <pre><code>from async_batch_llm import GeminiCachedStrategy, GeminiResponse\n\nstrategy = GeminiCachedStrategy(\n    response_parser=lambda r: parse_json(r.text),\n    include_metadata=True,  # NEW: Enable metadata access\n)\n\n# In post-processor or after processing:\nresult = work_item_result.output  # GeminiResponse object\n\n# Access parsed output\nmetadata = result.output  # Your parsed BookMetadata\n\n# Access safety ratings\nif result.safety_ratings:\n    hate_speech = result.safety_ratings.get(\"HARM_CATEGORY_HATE_SPEECH\")\n    harassment = result.safety_ratings.get(\"HARM_CATEGORY_HARASSMENT\")\n\n    if hate_speech == \"HIGH\" or harassment == \"HIGH\":\n        logger.warning(f\"High-risk content detected: {work_item.item_id}\")\n        # Filter or flag content\n\n# Access finish reason\nif result.finish_reason == \"SAFETY\":\n    logger.warning(\"Response blocked by safety filters\")\n\n# Access full response object\nraw = result.raw_response\n</code></pre> <p>GeminiResponse API:</p> <pre><code>@dataclass\nclass GeminiResponse(Generic[TOutput]):\n    output: TOutput                          # Your parsed output\n    safety_ratings: dict[str, str] | None    # Safety ratings by category\n    finish_reason: str | None                # Why generation stopped\n    token_usage: dict[str, int]              # Token counts\n    raw_response: Any                        # Full Gemini response object\n</code></pre> <p>Safety Rating Categories:</p> <p>Common Gemini safety categories:</p> <ul> <li><code>HARM_CATEGORY_HATE_SPEECH</code></li> <li><code>HARM_CATEGORY_HARASSMENT</code></li> <li><code>HARM_CATEGORY_SEXUALLY_EXPLICIT</code></li> <li><code>HARM_CATEGORY_DANGEROUS_CONTENT</code></li> </ul> <p>Probability levels: <code>\"NEGLIGIBLE\"</code>, <code>\"LOW\"</code>, <code>\"MEDIUM\"</code>, <code>\"HIGH\"</code></p> <p>Content Filtering Example:</p> <pre><code>async def filter_unsafe_content(result: WorkItemResult):\n    \"\"\"Filter content based on safety ratings.\"\"\"\n    if not isinstance(result.output, GeminiResponse):\n        return\n\n    safety = result.output.safety_ratings or {}\n\n    # Check for high-risk content\n    high_risk_categories = [\n        cat for cat, prob in safety.items()\n        if prob in [\"HIGH\", \"MEDIUM\"]\n    ]\n\n    if high_risk_categories:\n        logger.warning(\n            f\"Flagged {result.item_id}: {high_risk_categories}\"\n        )\n        # Mark for manual review\n        await db.flag_for_review(result.item_id, high_risk_categories)\n\nprocessor = ParallelBatchProcessor(\n    config=config,\n    post_processor=filter_unsafe_content,\n)\n</code></pre> <p>Backward Compatibility:</p> <pre><code># include_metadata=False (default) - returns parsed output directly\nstrategy = GeminiCachedStrategy(response_parser=parser)\nresult.output  # Your parsed type (e.g., BookMetadata)\n\n# include_metadata=True - returns GeminiResponse wrapper\nstrategy = GeminiCachedStrategy(response_parser=parser, include_metadata=True)\nresult.output  # GeminiResponse[BookMetadata]\nresult.output.output  # Your parsed type\n</code></pre> <p>See Also:</p> <ul> <li><code>examples/example_gemini_safety_ratings.py</code> - Complete safety ratings example</li> <li><code>docs/GEMINI_INTEGRATION.md</code> - Gemini-specific features</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#3-cache-tagging-for-precise-matching","title":"3. Cache Tagging for Precise Matching","text":"<p>Tag caches with metadata to prevent accidental reuse when prompts change.</p> <p>Problem (v0.2.x):</p> <pre><code># Day 1: Create cache with prompt v1\nstrategy = GeminiCachedStrategy(\n    cached_content=[{\"role\": \"user\", \"parts\": [{\"text\": \"Prompt version 1\"}]}],\n)\n\n# Day 2: Change prompt but reuse old cache (BUG!)\nstrategy = GeminiCachedStrategy(\n    cached_content=[{\"role\": \"user\", \"parts\": [{\"text\": \"Prompt version 2\"}]}],\n)\n# May reuse v1 cache \u2192 LLM gets inconsistent instructions\n</code></pre> <p>Solution (v0.3.0):</p> <pre><code># Day 1: Create cache with tags\nstrategy = GeminiCachedStrategy(\n    cached_content=[{\"role\": \"user\", \"parts\": [{\"text\": \"Prompt version 1\"}]}],\n    cache_tags={\n        \"prompt_version\": \"v1\",\n        \"purpose\": \"enrichment\",\n        \"dataset\": \"openlibrary\",\n    },\n)\n\n# Day 2: Different tags \u2192 creates new cache\nstrategy = GeminiCachedStrategy(\n    cached_content=[{\"role\": \"user\", \"parts\": [{\"text\": \"Prompt version 2\"}]}],\n    cache_tags={\n        \"prompt_version\": \"v2\",  # Different tag\n        \"purpose\": \"enrichment\",\n        \"dataset\": \"openlibrary\",\n    },\n)\n# Won't reuse v1 cache - tags don't match\n</code></pre> <p>Tag Matching Logic:</p> <pre><code># Cache is reused only if:\n# 1. Model name matches AND\n# 2. All tags match (if tags provided)\n\n# Example: This will NOT reuse cache from above\nstrategy = GeminiCachedStrategy(\n    cache_tags={\n        \"prompt_version\": \"v1\",  # Matches\n        \"purpose\": \"enrichment\",  # Matches\n        \"dataset\": \"books\",       # Different! Won't match\n    },\n)\n</code></pre> <p>Best Practices:</p> <pre><code># Version your prompts\ncache_tags = {\n    \"prompt_version\": \"v3\",\n    \"schema_version\": \"2024-01\",\n}\n\n# Separate by purpose/dataset\ncache_tags = {\n    \"purpose\": \"enrichment\",  # vs \"classification\", \"summarization\"\n    \"dataset\": \"openlibrary\",  # vs \"gutenberg\", \"archive\"\n}\n\n# Include model parameters that affect output\ncache_tags = {\n    \"temperature\": \"0.0\",\n    \"output_format\": \"json\",\n}\n</code></pre> <p>Graceful Degradation:</p> <p>If google-genai doesn't support cache metadata:</p> <ul> <li>Framework logs warning: <code>\"Cache tags requested but not supported by API\"</code></li> <li>Falls back to model-name-only matching</li> <li>No errors, continues working</li> </ul> <p>See Also:</p> <ul> <li><code>examples/example_gemini_cache_tags.py</code> - Complete cache tagging example</li> <li><code>docs/GEMINI_INTEGRATION.md</code> - Cache management strategies</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#optional-enhancements","title":"Optional Enhancements","text":""},{"location":"archive/MIGRATION_V0_3/#prepareoncemixin-new","title":"PrepareOnceMixin (New)","text":"<p>Helper for custom strategies that need idempotency (though v0.2.0's framework tracking makes this less critical).</p> <p>Usage:</p> <pre><code>from async_batch_llm.mixins import PrepareOnceMixin\nfrom async_batch_llm import LLMCallStrategy\n\nclass MyCustomStrategy(PrepareOnceMixin, LLMCallStrategy[Output]):\n    async def _prepare_once(self) -&gt; None:\n        \"\"\"Expensive initialization - called exactly once.\"\"\"\n        self.connection = await create_expensive_connection()\n        self.cache = await initialize_cache()\n        logger.info(\"Strategy prepared once\")\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use self.connection and self.cache\n        response = await self.connection.generate(prompt)\n        return response, tokens\n</code></pre> <p>When to Use:</p> <ul> <li>\u2705 Extra safety for shared strategies</li> <li>\u2705 Clear intent documentation</li> <li>\u2705 Use outside ParallelBatchProcessor</li> <li>\u274c Not needed with v0.2.0+ framework tracking (prepare() called once by framework)</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#migration-checklist","title":"Migration Checklist","text":""},{"location":"archive/MIGRATION_V0_3/#from-v02x-to-v030","title":"From v0.2.x to v0.3.0","text":"<p>No code changes required! But you may want to adopt new features:</p>"},{"location":"archive/MIGRATION_V0_3/#consider-retry-state-if","title":"Consider Retry State If","text":"<ul> <li>[ ] You have multi-stage retry logic</li> <li>[ ] You want partial recovery to reduce costs</li> <li>[ ] You need different handling for ValidationError vs NetworkError</li> <li>[ ] You want to enforce total prompt limits</li> </ul> <p>Action: Review <code>examples/example_multi_stage_retry.py</code> and implement if beneficial.</p>"},{"location":"archive/MIGRATION_V0_3/#consider-safety-ratings-if","title":"Consider Safety Ratings If","text":"<ul> <li>[ ] You process user-generated content</li> <li>[ ] You need content moderation</li> <li>[ ] You want to filter harmful outputs</li> <li>[ ] You need audit trails for safety</li> </ul> <p>Action: Set <code>include_metadata=True</code> and add post-processor for filtering.</p>"},{"location":"archive/MIGRATION_V0_3/#consider-cache-tagging-if","title":"Consider Cache Tagging If","text":"<ul> <li>[ ] You frequently change prompts</li> <li>[ ] You run multiple different pipelines</li> <li>[ ] You've had cache reuse bugs</li> <li>[ ] You want explicit cache versioning</li> </ul> <p>Action: Add <code>cache_tags</code> to your <code>GeminiCachedStrategy</code> initialization.</p>"},{"location":"archive/MIGRATION_V0_3/#examples","title":"Examples","text":"<p>All new features have complete working examples:</p>"},{"location":"archive/MIGRATION_V0_3/#retry-state-examples","title":"Retry State Examples","text":"<ul> <li><code>examples/example_multi_stage_retry.py</code> - Multi-stage validation recovery</li> <li><code>examples/example_partial_recovery.py</code> - Partial recovery pattern (81% cost savings)</li> <li><code>examples/example_retry_state_basics.py</code> - Basic retry state usage</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#safety-ratings-examples","title":"Safety Ratings Examples","text":"<ul> <li><code>examples/example_gemini_safety_ratings.py</code> - Content filtering with safety ratings</li> <li><code>examples/example_safety_audit.py</code> - Audit logging for compliance</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#cache-tagging-examples","title":"Cache Tagging Examples","text":"<ul> <li><code>examples/example_gemini_cache_tags.py</code> - Cache versioning and tagging</li> <li><code>examples/example_multi_pipeline_caching.py</code> - Multiple pipelines with separate caches</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#configuration-reference","title":"Configuration Reference","text":""},{"location":"archive/MIGRATION_V0_3/#new-parameters","title":"New Parameters","text":"<p>GeminiCachedStrategy:</p> <pre><code>strategy = GeminiCachedStrategy(\n    # ... existing params ...\n\n    # NEW in v0.3.0:\n    include_metadata: bool = False,         # Enable safety ratings access\n    cache_tags: dict[str, str] | None = None,  # Tags for cache matching\n)\n</code></pre> <p>LLMCallStrategy.execute():</p> <pre><code>async def execute(\n    prompt: str,\n    attempt: int,\n    timeout: float,\n    state: RetryState | None = None,  # NEW: Optional retry state\n) -&gt; tuple[TOutput, dict[str, int]]:\n    ...\n</code></pre> <p>LLMCallStrategy.on_error():</p> <pre><code>async def on_error(\n    exception: Exception,\n    attempt: int,\n    state: RetryState | None = None,  # NEW: Optional retry state\n) -&gt; None:\n    ...\n</code></pre>"},{"location":"archive/MIGRATION_V0_3/#troubleshooting","title":"Troubleshooting","text":""},{"location":"archive/MIGRATION_V0_3/#retry-state-not-persisting","title":"Retry State Not Persisting","text":"<p>Symptom: State resets between retries</p> <p>Solution: Ensure you're using the <code>state</code> parameter correctly:</p> <pre><code># \u274c WRONG - Creating new state each time\nasync def execute(self, prompt, attempt, timeout, state=None):\n    state = RetryState()  # Don't create new state!\n    state.set(\"count\", state.get(\"count\", 0) + 1)\n\n# \u2705 CORRECT - Use provided state\nasync def execute(self, prompt, attempt, timeout, state=None):\n    if state is None:\n        state = RetryState()  # Only if framework didn't provide one\n    state.set(\"count\", state.get(\"count\", 0) + 1)\n</code></pre>"},{"location":"archive/MIGRATION_V0_3/#safety-ratings-always-none","title":"Safety Ratings Always None","text":"<p>Symptom: <code>result.output.safety_ratings</code> is always <code>None</code></p> <p>Solutions:</p> <ol> <li>Ensure <code>include_metadata=True</code>:</li> </ol> <pre><code>strategy = GeminiCachedStrategy(\n    response_parser=parser,\n    include_metadata=True,  # Required!\n)\n</code></pre> <ol> <li>Check that response is actually <code>GeminiResponse</code>:</li> </ol> <pre><code>if isinstance(result.output, GeminiResponse):\n    ratings = result.output.safety_ratings\n</code></pre> <ol> <li>Some Gemini models/prompts may not include safety ratings</li> </ol>"},{"location":"archive/MIGRATION_V0_3/#cache-tags-not-matching","title":"Cache Tags Not Matching","text":"<p>Symptom: New cache created instead of reusing tagged cache</p> <p>Solutions:</p> <ol> <li>Verify tags are identical:</li> </ol> <pre><code># All tags must match exactly\ncache_tags = {\"version\": \"v1\", \"dataset\": \"books\"}\n# Won't match: {\"version\": \"v1\"}  (missing dataset)\n# Won't match: {\"version\": \"v1\", \"dataset\": \"Books\"}  (case-sensitive)\n</code></pre> <ol> <li>Check for API support:</li> </ol> <pre><code># Look for warning in logs:\n# \"Cache tags requested but not supported by API\"\n</code></pre> <ol> <li>Use model name matching as fallback:</li> </ol> <pre><code># If tags not supported, ensure model names match exactly\nmodel = \"gemini-2.5-flash\"  # Use consistent model names\n</code></pre>"},{"location":"archive/MIGRATION_V0_3/#best-practices","title":"Best Practices","text":""},{"location":"archive/MIGRATION_V0_3/#retry-state","title":"Retry State","text":"<p>DO:</p> <ul> <li>\u2705 Use for multi-stage strategies</li> <li>\u2705 Enforce total prompt limits to prevent runaway costs</li> <li>\u2705 Log state before clearing for debugging</li> <li>\u2705 Store minimal state (only what's needed)</li> </ul> <p>DON'T:</p> <ul> <li>\u274c Store large objects in state (keep it lightweight)</li> <li>\u274c Assume state is always provided (check for None)</li> <li>\u274c Modify state in <code>on_error()</code> unless needed</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#safety-ratings","title":"Safety Ratings","text":"<p>DO:</p> <ul> <li>\u2705 Use for user-generated content</li> <li>\u2705 Log safety events for audit trails</li> <li>\u2705 Have fallback behavior if ratings unavailable</li> <li>\u2705 Consider cultural/regional differences in thresholds</li> </ul> <p>DON'T:</p> <ul> <li>\u274c Rely solely on safety ratings (defense in depth)</li> <li>\u274c Block all content with MEDIUM ratings (may be overly strict)</li> <li>\u274c Assume ratings are always provided</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#cache-tagging","title":"Cache Tagging","text":"<p>DO:</p> <ul> <li>\u2705 Version your prompts explicitly</li> <li>\u2705 Use semantic tags (purpose, dataset, version)</li> <li>\u2705 Handle graceful degradation (API may not support tags)</li> <li>\u2705 Document your tagging schema</li> </ul> <p>DON'T:</p> <ul> <li>\u274c Use too many tags (harder to match)</li> <li>\u274c Include dynamic values (timestamps, UUIDs)</li> <li>\u274c Rely on tags for security (they're for matching only)</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#performance-impact","title":"Performance Impact","text":""},{"location":"archive/MIGRATION_V0_3/#retry-state_1","title":"Retry State","text":"<ul> <li>Memory: ~1KB per work item (negligible)</li> <li>CPU: Minimal overhead for dict operations</li> <li>Concurrency: No impact (state is per work item)</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#safety-ratings_1","title":"Safety Ratings","text":"<ul> <li>API: No additional API calls (included in response)</li> <li>Memory: ~1KB per response (negligible)</li> <li>CPU: Minimal overhead for extraction</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#cache-tagging_1","title":"Cache Tagging","text":"<ul> <li>API: No additional API calls</li> <li>Memory: Negligible (tags stored in cache metadata)</li> <li>CPU: Minimal overhead for tag comparison</li> </ul> <p>Overall: v0.3.0 features have &lt;1% performance impact.</p>"},{"location":"archive/MIGRATION_V0_3/#support","title":"Support","text":""},{"location":"archive/MIGRATION_V0_3/#documentation","title":"Documentation","text":"<ul> <li>README.md - Updated with v0.3.0 features</li> <li>docs/API.md - Full API reference</li> <li>docs/GEMINI_INTEGRATION.md - Gemini-specific guide</li> <li>docs/IMPLEMENTATION_PLAN_V0_3.md - Technical details</li> </ul>"},{"location":"archive/MIGRATION_V0_3/#examples_1","title":"Examples","text":"<p>All features have complete working examples in <code>examples/</code> directory.</p>"},{"location":"archive/MIGRATION_V0_3/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: https://github.com/geoff-davis/async-batch-llm/issues</li> <li>Check logs for debug information</li> <li>Enable debug logging: <code>logging.basicConfig(level=logging.DEBUG)</code></li> </ul>"},{"location":"archive/MIGRATION_V0_3/#version-history","title":"Version History","text":"<ul> <li>v0.3.0 (Current)</li> <li>Per-work-item retry state for advanced strategies</li> <li>Gemini safety ratings and metadata access</li> <li>Cache tagging for precise matching</li> <li> <p>PrepareOnceMixin helper</p> </li> <li> <p>v0.2.0</p> </li> <li>Shared strategy optimization</li> <li>Cached token tracking</li> <li>Automatic cache renewal</li> <li>google-genai v1.46+ support</li> <li> <p>Cache lifecycle improvements</p> </li> <li> <p>v0.1.0</p> </li> <li>Strategy pattern refactor</li> <li>PydanticAI, Gemini, custom strategies</li> <li> <p>Framework-level timeout enforcement</p> </li> <li> <p>v0.0.2.x</p> </li> <li>Direct API call support</li> <li> <p>Race condition fixes</p> </li> <li> <p>v0.0.1.x</p> </li> <li>Initial release</li> <li>PydanticAI agent support</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Patterns","text":""},{"location":"examples/advanced/#smart-model-escalation","title":"Smart Model Escalation","text":"<p>Save costs by starting with cheap models and escalating only on validation errors:</p> <pre><code>from pydantic import ValidationError\nfrom async_batch_llm import LLMCallStrategy\n\nclass SmartModelEscalation(LLMCallStrategy[dict]):\n    MODELS = [\n        \"gemini-2.5-flash-lite\",  # Cheapest\n        \"gemini-2.5-flash\",       # Medium\n        \"gemini-2.5-pro\",         # Most capable\n    ]\n\n    def __init__(self, client):\n        self.client = client\n        self.validation_failures = 0\n\n    async def on_error(self, exception: Exception, attempt: int):\n        \"\"\"Only escalate on validation errors, not network/rate limit errors.\"\"\"\n        if isinstance(exception, ValidationError):\n            self.validation_failures += 1\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Network error on attempt 2? Retry with same cheap model\n        # Validation error on attempt 2? Escalate to better model\n        model_index = min(self.validation_failures, len(self.MODELS) - 1)\n        model = self.MODELS[model_index]\n\n        response = await self.client.generate(prompt, model=model)\n        return response.output, response.tokens\n</code></pre> <p>Cost savings: 60-80% vs. always using the best model.</p>"},{"location":"examples/advanced/#smart-retry-with-validation-feedback","title":"Smart Retry with Validation Feedback","text":"<p>Tell the LLM exactly what failed on retry:</p> <pre><code>class SmartRetryStrategy(LLMCallStrategy[PersonData]):\n    def __init__(self, client):\n        self.client = client\n        self.last_error = None\n        self.last_response = None\n\n    async def on_error(self, exception: Exception, attempt: int):\n        if isinstance(exception, ValidationError):\n            self.last_error = exception\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        if attempt == 1:\n            final_prompt = prompt\n        else:\n            # Create retry prompt with field-level feedback\n            final_prompt = self._create_retry_prompt(prompt)\n\n        try:\n            response = await self.client.generate(final_prompt)\n            output = PersonData.model_validate_json(response.text)\n            return output, tokens\n        except ValidationError as e:\n            self.last_response = response.text\n            raise\n\n    def _create_retry_prompt(self, original_prompt: str) -&gt; str:\n        # Parse self.last_error to identify which fields failed\n        # Build prompt like: \"These fields succeeded: [age]. Fix these: [name, email]\"\n        return retry_prompt\n</code></pre>"},{"location":"examples/advanced/#shared-context-caching","title":"Shared Context Caching","text":"<p>Dramatically reduce costs for RAG and repeated context:</p> <pre><code>from async_batch_llm.llm_strategies.gemini import GeminiCachedStrategy\nfrom google import genai\n\nasync def process_with_caching():\n    client = genai.Client(api_key=\"your-key\")\n\n    # Load large RAG context once\n    with open(\"knowledge_base.txt\") as f:\n        rag_context = f.read()  # Could be 100K+ tokens\n\n    # Strategy creates cache in prepare(), deletes in cleanup()\n    strategy = GeminiCachedStrategy(\n        client=client,\n        model=\"gemini-2.5-flash\",\n        system_instruction=rag_context,  # Cached!\n        output_type=str\n    )\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor(config=config) as processor:\n        # All 100 queries share the same cached context\n        for i in range(100):\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=f\"query_{i}\",\n                    strategy=strategy,\n                    prompt=f\"Answer based on context: {questions[i]}\"\n                )\n            )\n\n        result = await processor.process_all()\n        # Cache automatically cleaned up on exit\n</code></pre> <p>Cost savings: ~90% for input tokens on cached content.</p>"},{"location":"examples/advanced/#middleware-for-custom-logic","title":"Middleware for Custom Logic","text":"<p>Inject custom behavior into the processing pipeline:</p> <pre><code>from async_batch_llm.middleware import Middleware\nfrom async_batch_llm import LLMWorkItem, WorkItemResult\n\nclass LoggingMiddleware(Middleware):\n    async def before_process(self, work_item: LLMWorkItem):\n        print(f\"Starting {work_item.item_id}\")\n\n    async def after_process(self, result: WorkItemResult):\n        if result.success:\n            print(f\"Success: {result.item_id}\")\n        else:\n            print(f\"Failed: {result.item_id} - {result.error}\")\n\n    async def on_retry(self, work_item: LLMWorkItem, attempt: int, error: Exception):\n        print(f\"Retry {attempt} for {work_item.item_id}: {error}\")\n\nasync def main():\n    logging_middleware = LoggingMiddleware()\n\n    async with ParallelBatchProcessor(\n        config=config,\n        middlewares=[logging_middleware]\n    ) as processor:\n        # Add work items...\n        result = await processor.process_all()\n</code></pre>"},{"location":"examples/advanced/#custom-observers","title":"Custom Observers","text":"<p>Track custom metrics:</p> <pre><code>from async_batch_llm.observers import BaseObserver, ProcessingEvent\nfrom async_batch_llm import LLMWorkItem, WorkItemResult\nfrom typing import Any\n\nclass CostTracker(BaseObserver):\n    def __init__(self):\n        self.total_cost = 0.0\n        self.total_tokens = 0\n\n    async def on_event(self, event: ProcessingEvent, data: dict[str, Any]) -&gt; None:\n        if event == ProcessingEvent.ITEM_COMPLETED:\n            # Calculate cost based on tokens\n            tokens = data.get(\"tokens\", {})\n            total = tokens.get(\"total_tokens\", 0)\n            self.total_tokens += total\n            self.total_cost += total * 0.00001  # Example rate\n\nasync def main():\n    cost_tracker = CostTracker()\n\n    async with ParallelBatchProcessor(\n        config=config,\n        observers=[cost_tracker]\n    ) as processor:\n        # Add work items...\n        result = await processor.process_all()\n\n        print(f\"Total tokens: {cost_tracker.total_tokens}\")\n        print(f\"Estimated cost: ${cost_tracker.total_cost:.4f}\")\n</code></pre>"},{"location":"examples/advanced/#dynamic-worker-scaling","title":"Dynamic Worker Scaling","text":"<p>Adjust workers based on rate limits:</p> <pre><code>async def adaptive_processing():\n    config = ProcessorConfig(\n        max_workers=10,  # Start optimistic\n        timeout_per_item=30.0\n    )\n\n    async with ParallelBatchProcessor(config=config) as processor:\n        # Add work...\n        result = await processor.process_all()\n\n        # Check if rate limited\n        stats = await processor.get_stats()\n        if stats[\"rate_limit_count\"] &gt; 5:\n            # Too many rate limits, reduce workers for next batch\n            processor.config.max_workers = 3\n</code></pre>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":""},{"location":"examples/basic/#simple-batch-processing","title":"Simple Batch Processing","text":"<p>Process multiple prompts in parallel:</p> <pre><code>import asyncio\nfrom async_batch_llm import (\n    ParallelBatchProcessor,\n    LLMWorkItem,\n    ProcessorConfig,\n    PydanticAIStrategy,\n)\nfrom pydantic_ai import Agent\n\nasync def main():\n    agent = Agent(\"gemini-2.5-flash\", result_type=str)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor(config=config) as processor:\n        prompts = [\n            \"What is Python?\",\n            \"What is async/await?\",\n            \"What is asyncio?\"\n        ]\n\n        for i, prompt in enumerate(prompts):\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=f\"item_{i}\",\n                    strategy=strategy,\n                    prompt=prompt\n                )\n            )\n\n        result = await processor.process_all()\n\n        for work_result in result.results:\n            if work_result.success:\n                print(f\"{work_result.item_id}: {work_result.output}\")\n            else:\n                print(f\"{work_result.item_id}: Failed - {work_result.error}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic/#structured-output","title":"Structured Output","text":"<p>Use Pydantic models for validated output:</p> <pre><code>from pydantic import BaseModel\n\nclass CodeReview(BaseModel):\n    issues: list[str]\n    suggestions: list[str]\n    rating: int\n\nasync def review_code():\n    agent = Agent(\"gemini-2.5-flash\", result_type=CodeReview)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    config = ProcessorConfig(max_workers=3)\n\n    async with ParallelBatchProcessor(config=config) as processor:\n        code_snippets = [\"def foo(): pass\", \"def bar(): return 42\"]\n\n        for snippet in code_snippets:\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=snippet[:20],\n                    strategy=strategy,\n                    prompt=f\"Review this code:\\n{snippet}\"\n                )\n            )\n\n        result = await processor.process_all()\n\n        for work_result in result.results:\n            if work_result.success:\n                review = work_result.output\n                print(f\"Rating: {review.rating}/10\")\n                print(f\"Issues: {review.issues}\")\n</code></pre>"},{"location":"examples/basic/#context-passing","title":"Context Passing","text":"<p>Pass context through the processing pipeline:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass FileContext:\n    filepath: str\n    original_content: str\n\nasync def process_with_context():\n    agent = Agent(\"gemini-2.5-flash\", result_type=str)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor[str, str, FileContext](config=config) as processor:\n        files = [\n            (\"file1.py\", \"content1\"),\n            (\"file2.py\", \"content2\"),\n        ]\n\n        for filepath, content in files:\n            await processor.add_work(\n                LLMWorkItem(\n                    item_id=filepath,\n                    strategy=strategy,\n                    prompt=f\"Summarize: {content}\",\n                    context=FileContext(filepath=filepath, original_content=content)\n                )\n            )\n\n        result = await processor.process_all()\n\n        for work_result in result.results:\n            if work_result.success and work_result.context:\n                print(f\"File: {work_result.context.filepath}\")\n                print(f\"Summary: {work_result.output}\")\n</code></pre>"},{"location":"examples/basic/#post-processing","title":"Post-Processing","text":"<p>Use post-processors to handle results as they complete:</p> <pre><code>async def save_result(result):\n    \"\"\"Called for each completed work item.\"\"\"\n    if result.success:\n        # Save to database, file, etc.\n        await save_to_db(result.item_id, result.output)\n        print(f\"Saved {result.item_id}\")\n\nasync def process_with_post_processor():\n    agent = Agent(\"gemini-2.5-flash\", result_type=str)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor(\n        config=config,\n        post_processor=save_result  # Called for each result\n    ) as processor:\n        # Add work items...\n        result = await processor.process_all()\n</code></pre>"},{"location":"examples/basic/#metrics-collection","title":"Metrics Collection","text":"<p>Track metrics using observers:</p> <pre><code>from async_batch_llm.observers import MetricsObserver\n\nasync def process_with_metrics():\n    metrics = MetricsObserver()\n\n    agent = Agent(\"gemini-2.5-flash\", result_type=str)\n    strategy = PydanticAIStrategy(agent=agent)\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor(\n        config=config,\n        observers=[metrics]\n    ) as processor:\n        # Add work items...\n        result = await processor.process_all()\n\n        # Get collected metrics\n        collected_metrics = await metrics.get_metrics()\n        print(f\"Items processed: {collected_metrics['items_processed']}\")\n        print(f\"Succeeded: {collected_metrics['items_succeeded']}\")\n        print(f\"Failed: {collected_metrics['items_failed']}\")\n</code></pre>"},{"location":"examples/custom-strategies/","title":"Custom Strategies","text":"<p>Learn how to create custom strategies for any LLM provider.</p>"},{"location":"examples/custom-strategies/#basic-custom-strategy","title":"Basic Custom Strategy","text":"<pre><code>from async_batch_llm import LLMCallStrategy\n\nclass OpenAIStrategy(LLMCallStrategy[str]):\n    def __init__(self, client, model: str):\n        self.client = client\n        self.model = model\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        output = response.choices[0].message.content\n        tokens = {\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n            \"total_tokens\": response.usage.total_tokens\n        }\n\n        return output, tokens\n</code></pre>"},{"location":"examples/custom-strategies/#resource-management","title":"Resource Management","text":"<p>Use <code>prepare()</code> and <code>cleanup()</code> for resource lifecycle:</p> <pre><code>class CachedStrategy(LLMCallStrategy[str]):\n    def __init__(self, client, system_instruction: str):\n        self.client = client\n        self.system_instruction = system_instruction\n        self.cache_name = None\n\n    async def prepare(self):\n        \"\"\"Create cache before processing.\"\"\"\n        self.cache_name = await self.client.create_cache(\n            content=self.system_instruction\n        )\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use the cached content\n        response = await self.client.generate(\n            prompt=prompt,\n            cache_name=self.cache_name\n        )\n        return response.text, response.usage\n\n    async def cleanup(self):\n        \"\"\"Delete cache after processing.\"\"\"\n        if self.cache_name:\n            await self.client.delete_cache(self.cache_name)\n</code></pre>"},{"location":"examples/custom-strategies/#error-handling","title":"Error Handling","text":"<p>Use <code>on_error()</code> to track failures and adjust behavior:</p> <pre><code>from pydantic import ValidationError\n\nclass SmartRetryStrategy(LLMCallStrategy[dict]):\n    def __init__(self, client):\n        self.client = client\n        self.validation_failures = 0\n\n    async def on_error(self, exception: Exception, attempt: int):\n        \"\"\"Track validation errors for smart escalation.\"\"\"\n        if isinstance(exception, ValidationError):\n            self.validation_failures += 1\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use cheaper model initially, escalate only on validation errors\n        if self.validation_failures == 0:\n            model = \"cheap-model\"\n        elif self.validation_failures == 1:\n            model = \"medium-model\"\n        else:\n            model = \"expensive-model\"\n\n        response = await self.client.generate(prompt, model=model)\n        return response.output, response.tokens\n</code></pre>"},{"location":"examples/custom-strategies/#progressive-temperature","title":"Progressive Temperature","text":"<p>Increase temperature on retry for better success rates:</p> <pre><code>class ProgressiveTempStrategy(LLMCallStrategy[str]):\n    def __init__(self, client, temperatures=None):\n        self.client = client\n        self.temperatures = temperatures or [0.0, 0.5, 1.0]\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        # Use progressively higher temperature on retries\n        temp_index = min(attempt - 1, len(self.temperatures) - 1)\n        temperature = self.temperatures[temp_index]\n\n        response = await self.client.generate(\n            prompt=prompt,\n            temperature=temperature\n        )\n\n        return response.text, response.usage\n</code></pre>"},{"location":"examples/custom-strategies/#anthropic-example","title":"Anthropic Example","text":"<pre><code>from anthropic import AsyncAnthropic\n\nclass AnthropicStrategy(LLMCallStrategy[str]):\n    def __init__(self, client: AsyncAnthropic, model: str = \"claude-3-5-sonnet-20241022\"):\n        self.client = client\n        self.model = model\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        response = await self.client.messages.create(\n            model=self.model,\n            max_tokens=1024,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        output = response.content[0].text\n        tokens = {\n            \"input_tokens\": response.usage.input_tokens,\n            \"output_tokens\": response.usage.output_tokens,\n            \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens\n        }\n\n        return output, tokens\n</code></pre>"},{"location":"examples/custom-strategies/#usage","title":"Usage","text":"<pre><code>from async_batch_llm import ParallelBatchProcessor, LLMWorkItem, ProcessorConfig\n\nasync def main():\n    # Use your custom strategy\n    strategy = OpenAIStrategy(client=openai_client, model=\"gpt-4\")\n\n    config = ProcessorConfig(max_workers=5)\n\n    async with ParallelBatchProcessor(config=config) as processor:\n        await processor.add_work(\n            LLMWorkItem(\n                item_id=\"test\",\n                strategy=strategy,\n                prompt=\"Hello!\"\n            )\n        )\n\n        result = await processor.process_all()\n</code></pre>"},{"location":"migration/v0.1/","title":"Migration Guide: v0.1","text":"<p>This guide covers migrating from v0.0.x to v0.1+, which introduced the strategy pattern.</p> <p>See the full migration document for complete details.</p>"},{"location":"migration/v0.1/#key-changes-in-v01","title":"Key Changes in v0.1","text":""},{"location":"migration/v0.1/#1-strategy-pattern-introduction","title":"1. Strategy Pattern Introduction","text":"<p>v0.1 replaced direct <code>agent=</code> parameter with the <code>strategy=</code> parameter.</p> <p>Before (v0.0.x):</p> <pre><code>from async_batch_llm import LLMWorkItem\nfrom pydantic_ai import Agent\n\nagent = Agent(\"gemini-2.0-flash\", result_type=Output)\n\nwork_item = LLMWorkItem(\n    item_id=\"1\",\n    agent=agent,  # Direct agent\n    prompt=\"...\"\n)\n</code></pre> <p>After (v0.1+):</p> <pre><code>from async_batch_llm import LLMWorkItem, PydanticAIStrategy\nfrom pydantic_ai import Agent\n\nagent = Agent(\"gemini-2.0-flash\", result_type=Output)\nstrategy = PydanticAIStrategy(agent=agent)  # Wrap in strategy\n\nwork_item = LLMWorkItem(\n    item_id=\"1\",\n    strategy=strategy,  # Use strategy\n    prompt=\"...\"\n)\n</code></pre>"},{"location":"migration/v0.1/#2-why-the-change","title":"2. Why the Change?","text":"<p>The strategy pattern decouples the framework from specific LLM providers:</p> <ul> <li>\u2705 Support any LLM provider (OpenAI, Anthropic, Google, LangChain, custom)</li> <li>\u2705 Each strategy encapsulates provider-specific logic</li> <li>\u2705 Framework handles retry, timeout, rate limiting uniformly</li> <li>\u2705 Easy to test with mock strategies</li> <li>\u2705 Resource lifecycle management (prepare/cleanup)</li> </ul>"},{"location":"migration/v0.1/#3-built-in-strategies","title":"3. Built-in Strategies","text":"<p>v0.1 introduced several built-in strategies:</p> <ul> <li>PydanticAIStrategy - Wraps PydanticAI agents</li> <li>GeminiStrategy - Direct Gemini API calls</li> <li>GeminiCachedStrategy - Gemini with context caching</li> </ul>"},{"location":"migration/v0.1/#4-custom-strategies","title":"4. Custom Strategies","text":"<p>You can now create custom strategies for any provider:</p> <pre><code>from async_batch_llm import LLMCallStrategy\n\nclass OpenAIStrategy(LLMCallStrategy[str]):\n    def __init__(self, client, model: str):\n        self.client = client\n        self.model = model\n\n    async def execute(self, prompt: str, attempt: int, timeout: float):\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        output = response.choices[0].message.content\n        tokens = {\n            \"input_tokens\": response.usage.prompt_tokens,\n            \"output_tokens\": response.usage.completion_tokens,\n            \"total_tokens\": response.usage.total_tokens\n        }\n        return output, tokens\n</code></pre>"},{"location":"migration/v0.1/#see-also","title":"See Also","text":"<ul> <li>Full v0.1 Migration Guide</li> <li>Custom Strategies</li> </ul>"},{"location":"migration/v0.4/","title":"Migration Guide: v0.4","text":"<p>This guide covers migrating to v0.4 from earlier versions.</p> <p>See the full migration document for complete details.</p>"},{"location":"migration/v0.4/#key-changes-in-v04","title":"Key Changes in v0.4","text":""},{"location":"migration/v0.4/#1-gemini-client-update","title":"1. Gemini Client Update","text":"<p>The Google Gemini client library changed from <code>google.generativeai</code> to <code>google.genai</code>.</p> <p>Before (v0.3):</p> <pre><code>import google.generativeai as genai\n\ngenai.configure(api_key=\"your-key\")\nclient = genai  # Module as client\n</code></pre> <p>After (v0.4):</p> <pre><code>from google import genai\n\nclient = genai.Client(api_key=\"your-key\")\n</code></pre>"},{"location":"migration/v0.4/#2-updated-dependencies","title":"2. Updated Dependencies","text":"<p>Update your dependencies:</p> <pre><code># Old\npip install 'async-batch-llm[gemini]'  # Would install google-generativeai\n\n# New\npip install 'async-batch-llm[gemini]'  # Now installs google-genai&gt;=1.46.0\n</code></pre>"},{"location":"migration/v0.4/#3-geministrategy-changes","title":"3. GeminiStrategy Changes","text":"<p>If using <code>GeminiStrategy</code> or <code>GeminiCachedStrategy</code>, update client creation:</p> <pre><code>from google import genai\nfrom async_batch_llm.llm_strategies.gemini import GeminiStrategy\n\n# Create new-style client\nclient = genai.Client(api_key=\"your-key\")\n\nstrategy = GeminiStrategy(\n    client=client,\n    model=\"gemini-2.5-flash\",\n    output_type=str\n)\n</code></pre>"},{"location":"migration/v0.4/#see-also","title":"See Also","text":"<ul> <li>v0.1 Migration - Strategy pattern introduction</li> <li>Full v0.4 Migration Guide</li> </ul>"}]}